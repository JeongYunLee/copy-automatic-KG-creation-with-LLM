The provided context mentions that deep learning models are "completely trainable from data, usually in a supervised fashion, such that the learning algorithm (usually stochastic gradient descent), is told what the desired output is for each input data sample. " Therefore, stochastic gradient descent (SGD) is specifically referenced as an optimization technique applied in the deep learning pipeline. Other optimization techniques such as Adam are not mentioned in the provided context. 