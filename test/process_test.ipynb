{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aba7d064",
   "metadata": {},
   "source": [
    "# KG creation with LLM\n",
    "\n",
    "* GitHub: https://github.com/fusion-jena/automatic-KG-creation-with-LLM\n",
    "* 깃헙에 공개된 데이터, 코드를 기반으로 실험을 재현하는 코드\n",
    "* 모델은 GPT-4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a693647e",
   "metadata": {},
   "source": [
    "## (1) Data collection, (2) CQ genertaion\n",
    "\n",
    "* 이 과정은 수작업으로 진행됨\n",
    "\n",
    "1. 해당 도메인 관련 논문 문서 수집\n",
    "2. (논문을 기반으로) 해당 도메인의 온톨로지를 구축하기 위한 Ontology Requirement Specification을 구축하기\n",
    "3. ORSD를 기반으로 온톨로지 구축에 필요한 질문(Competency Question) 생성하기\n",
    "    - e.g. What methods are utilized for collecting raw data in the deep learning pipeline (e.g., surveys, sensors, public datasets)? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b3dc92",
   "metadata": {},
   "source": [
    "## (3) Ontology creation\n",
    "\n",
    "* LLM_loader.py, helper_functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d25127a",
   "metadata": {},
   "source": [
    "#### 🚫 Mixtral-8x22B-Instruct-v0.1/sentence-transformers/all-MiniLM-L12-v2 -> 로컬에서 실행하기 어려움(보류)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b239f36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Using cached ipywidgets-8.1.6-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from ipywidgets) (9.1.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Using cached widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.14 (from ipywidgets)\n",
      "  Using cached jupyterlab_widgets-3.0.14-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: decorator in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Using cached ipywidgets-8.1.6-py3-none-any.whl (139 kB)\n",
      "Using cached jupyterlab_widgets-3.0.14-py3-none-any.whl (213 kB)\n",
      "Using cached widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.6 jupyterlab_widgets-3.0.14 widgetsnbextension-4.0.14\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7002a9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting tqdm (from sentence-transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Using cached torch-2.6.0-cp312-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Using cached scikit_learn-1.6.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Using cached scipy-1.15.2-cp312-cp312-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Using cached huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting Pillow (from sentence-transformers)\n",
      "  Using cached pillow-11.2.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.9 kB)\n",
      "Collecting typing_extensions>=4.5.0 (from sentence-transformers)\n",
      "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Collecting pyyaml>=5.1 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting requests (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting setuptools (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached setuptools-78.1.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy==1.13.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting numpy>=1.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached numpy-2.2.4-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached regex-2024.11.6-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached charset_normalizer-3.4.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Using cached sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Using cached huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Using cached torch-2.6.0-cp312-none-macosx_11_0_arm64.whl (66.5 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "Using cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Using cached pillow-11.2.1-cp312-cp312-macosx_11_0_arm64.whl (3.0 MB)\n",
      "Using cached scikit_learn-1.6.1-cp312-cp312-macosx_12_0_arm64.whl (11.2 MB)\n",
      "Using cached scipy-1.15.2-cp312-cp312-macosx_14_0_arm64.whl (22.4 MB)\n",
      "Using cached fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached numpy-2.2.4-cp312-cp312-macosx_14_0_arm64.whl (5.2 MB)\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
      "Using cached regex-2024.11.6-cp312-cp312-macosx_11_0_arm64.whl (284 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached setuptools-78.1.0-py3-none-any.whl (1.3 MB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp312-cp312-macosx_10_13_universal2.whl (196 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Installing collected packages: mpmath, urllib3, typing_extensions, tqdm, threadpoolctl, sympy, setuptools, safetensors, regex, pyyaml, Pillow, numpy, networkx, MarkupSafe, joblib, idna, fsspec, filelock, charset-normalizer, certifi, scipy, requests, jinja2, torch, scikit-learn, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "Successfully installed MarkupSafe-3.0.2 Pillow-11.2.1 certifi-2025.1.31 charset-normalizer-3.4.1 filelock-3.18.0 fsspec-2025.3.2 huggingface-hub-0.30.2 idna-3.10 jinja2-3.1.6 joblib-1.4.2 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.4 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.5.3 scikit-learn-1.6.1 scipy-1.15.2 sentence-transformers-4.1.0 setuptools-78.1.0 sympy-1.13.1 threadpoolctl-3.6.0 tokenizers-0.21.1 torch-2.6.0 tqdm-4.67.1 transformers-4.51.3 typing_extensions-4.13.2 urllib3-2.4.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4a478d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Using cached bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: scipy in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from bitsandbytes) (1.15.2)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from scipy->bitsandbytes) (2.2.4)\n",
      "Using cached bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.42.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c327c0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Using cached accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from accelerate) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from accelerate) (0.30.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.2)\n",
      "Requirement already satisfied: requests in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
      "Requirement already satisfied: networkx in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (78.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n",
      "Using cached accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d85dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import transformers\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "access_token_read = os.getenv('HUGGINGFACE_API_KEY')\n",
    "\n",
    "login(token=access_token_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7f04c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm(model_id,embedding_model_id):\n",
    "    bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    # embeddings = HuggingFaceEmbeddings(model_name=embedding_model_id,model_kwargs={'device': 'cuda'})\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=embedding_model_id)\n",
    "    model_config = transformers.AutoConfig.from_pretrained(model_id)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "        trust_remote_code=True,\n",
    "        config=model_config,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map='auto')\n",
    "    model.eval()\n",
    "    pipe = pipeline(model=model, tokenizer=tokenizer,\n",
    "        #return_full_text=True,  # langchain expects the full text\n",
    "        task='text-generation',\n",
    "        temperature=0.00001,\n",
    "        max_new_tokens=25000,  # max number of tokens to generate in the output\n",
    "        #repetition_penalty=1.1,  # without this output begins repeating\n",
    "        device_map = \"auto\"\n",
    "    )\n",
    "    llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0.00001})\n",
    "    return llm\n",
    "\n",
    "def get_embeddings(embedding_model_id):\n",
    "    return HuggingFaceEmbeddings(model_name=embedding_model_id,model_kwargs={'device': 'cuda'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06552e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from helper_functions import load_llm\n",
    "from configparser import ConfigParser\n",
    "\n",
    "# config = ConfigParser()\n",
    "# config.read('config.ini')\n",
    "# model_id = config.get('Models', 'model_id')\n",
    "# embedding_model_id = config.get('Models', 'embedding_model_id')\n",
    "\n",
    "model_id = 'mistralai/Mixtral-8x22B-Instruct-v0.1'\n",
    "embedding_model_id = 'sentence-transformers/all-MiniLM-L12-v2'\n",
    "\n",
    "llm = load_llm(model_id, embedding_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f6a916",
   "metadata": {},
   "source": [
    "#### OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97bad1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1\", temperature=0)\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-3-large')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417bd321",
   "metadata": {},
   "source": [
    "##### 생성(Concepts_relations_generate.py)\n",
    "\n",
    "* CQs를 기반으로 Concepts, Relations, DataProperties, InverseProperties 추출하는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fb33358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper_functions.py\n",
    "def read_txt(txt_path):\n",
    "    with open(txt_path,'r') as f:\n",
    "        content = f.read()\n",
    "    return content\n",
    "\n",
    "def load_cqs(CQs_path):\n",
    "    with open(CQs_path) as f:\n",
    "        lines = f.readlines()\n",
    "    CQs = [l[:-1] for l in lines]\n",
    "    return CQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59222029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from helper_functions import load_llm, read_txt, load_cqs\n",
    "from langchain.prompts import PromptTemplate\n",
    "# from LLM_loader import llm\n",
    "\n",
    "def Concepts_relations_generate():\n",
    "    # template = read_txt(config.get('Paths', 'Concepts_and_relationships_prompt_path'))\n",
    "    template = read_txt('../Ontology/Mixtral_8_22b/Prompt/Concepts_relationships_dataproperties_extraction.txt')\n",
    "    prompt_template = PromptTemplate(input_variables=[\"CQs\"], template=template)\n",
    "    # prompt = prompt_template.format(CQs=load_cqs(config.get('Paths', 'CQs_path')))\n",
    "    prompt =  prompt_template.format(CQs=load_cqs('../CQs/CQs.txt')) # CQ 넣어주기\n",
    "\n",
    "    with open('concept_relation_data_inverse.txt',\"w\") as f:\n",
    "        f.write(llm.invoke(prompt).content)\n",
    "        \n",
    "Concepts_relations_generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477bf329",
   "metadata": {},
   "source": [
    "##### 변환 (Ontology_creation.py)\n",
    "\n",
    "* 추출한 Concepts, Relations, DataProperties, InverseProperties를 OWL로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb89d9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rdflib\n",
      "  Using cached rdflib-7.1.4-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pyparsing<4,>=2.1.0 (from rdflib)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Using cached rdflib-7.1.4-py3-none-any.whl (565 kB)\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, rdflib\n",
      "Successfully installed pyparsing-3.2.3 rdflib-7.1.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rdflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffd112f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper_functions.py\n",
    "from rdflib import Graph, Namespace\n",
    "\n",
    "def get_base_onto_class(owl_file):\n",
    "    g = Graph()\n",
    "    g.parse(owl_file)\n",
    "\n",
    "    # Define commonly used namespaces\n",
    "    RDF = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "    OWL = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "\n",
    "    classes = []\n",
    "    relations = []\n",
    "    # Find classes\n",
    "    for class_uri in g.subjects(RDF.type, OWL.Class):\n",
    "        class_label = class_uri.split('#')[-1]\n",
    "        classes.append(class_label)\n",
    "\n",
    "    # Find properties\n",
    "    for prop_uri in g.subjects(RDF.type, OWL.ObjectProperty):\n",
    "        prop_label = prop_uri.split('#')[-1]\n",
    "        relations.append(prop_label)\n",
    "    return classes, relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bd271236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%INSTRUCTIONS:\n",
      "Use the concepts and relations (properties) and build an ontology in RDF format for describing the provenance of Deep Learning Pipeline.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "Don't provide anything other than the ontology in RDF format.\n",
      "\n",
      "Use the IRI for the base ontology: https://w3id.org/dlprovenance/\n",
      "Use the ontology classes and relations as the base ontology.\n",
      "Incorporate data properties, Inverse Properties, and ensure the hierarchy is well-structured. \n",
      "Below are the examples and follow the same format for all the questions:\n",
      "\n",
      "Concepts: Hyperparameter, Model\n",
      "Relations: hasHyperparameter, hasModel\n",
      "\n",
      "<?xml version=\"1.0\"?>\n",
      "<rdf:RDF xmlns=\"https://w3id.org/dlprovenance#\"\n",
      "     xml:base=\"https://w3id.org/dlprovenance\"\n",
      "     xmlns:owl=\"http://www.w3.org/2002/07/owl#\"\n",
      "     xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n",
      "     xmlns:xml=\"http://www.w3.org/XML/1998/namespace\"\n",
      "     xmlns:xsd=\"http://www.w3.org/2001/XMLSchema#\"\n",
      "     xmlns:rdfs=\"http://www.w3.org/2000/01/rdf-schema#\">\n",
      "    <owl:Ontology/>\n",
      "    <owl:Class rdf:about=\"https://w3id.org/dlprovenance#Hyperparameter\">\n",
      "        <rdfs:subClassOf rdf:resource=\"http://www.w3.org/ns/prov#Entity\"/>\n",
      "        <rdfs:label xml:lang=\"en\">Hyperparameter</rdfs:label>\n",
      "        <rdfs:comment xml:lang=\"en\">Hyperparameter is a prior parameter of an implementation, i.e., a parameter which is set before its execution (e.g. C, the complexity parameter, in weka.SMO).</rdfs:comment>\n",
      "    </owl:Class>\n",
      "    <owl:Class rdf:about=\"https://w3id.org/dlprovenance#Model\">\n",
      "        <rdfs:subClassOf rdf:resource=\"http://www.w3.org/ns/prov#Entity\"/>\n",
      "        <rdfs:label xml:lang=\"en\">Model</rdfs:label>\n",
      "        <rdfs:comment xml:lang=\"en\">Model is a generalization of a set of training data able to predict values for unseen instances. It is an output from an execution of a data mining algorithm implementation.</rdfs:comment>\n",
      "    </owl:Class>\n",
      "    <owl:ObjectProperty rdf:about=\"https://w3id.org/dlprovenance#hasHyperparameter\">\n",
      "        <rdfs:domain rdf:resource=\"https://w3id.org/dlprovenance##Model\"/>\n",
      "        <rdfs:range rdf:resource=\"https://w3id.org/dlprovenance#Hyperparameter\"/>\n",
      "        <rdfs:label>hasHyperparameter</rdfs:label>\n",
      "        <rdfs:comment>A relation between an implementation of a machine learning algorithm and its hyperparameter.</rdfs:comment>\n",
      "    </owl:ObjectProperty>\n",
      "</rdf:RDF>\n",
      "\n",
      "%QUERY:\n",
      "concepts : Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform\n",
      "relations: hasMethod, hasRawData, hasDataFormat, hasDataAnnotationTechnique, hasDataAugmentationTechnique, hasDataset, hasPreprocessingStep, hasDataSplitCriteria, hasCodeRepository, hasDataRepository, hasCodeRepositoryLink, hasDataRepositoryLink, hasDeepLearningModel, hasHyperparameter, hasHyperparameterOptimization, hasOptimizationTechnique, hasTrainingCompletionCriteria, hasRegularizationMethod, hasModelPerformanceMonitoringStrategy, hasFramework, hasHardwareResource, hasPostprocessingStep, hasPerformanceMetric, hasGeneralizabilityMeasure, hasRandomnessStrategy, hasModelPurpose, hasDataBiasTechnique, hasModelDeploymentProcess, hasDeploymentPlatform\n",
      "data_properties: methodName, dataFormat, dataAnnotationTechniqueName, dataAugmentationTechniqueName, datasetName, preprocessingStepName, dataSplitCriteriaName, codeRepositoryName, dataRepositoryName, codeRepositoryLink, dataRepositoryLink, deepLearningModelName, hyperparameterName, hyperparameterOptimizationName, optimizationTechniqueName, trainingCompletionCriteriaName, regularizationMethodName, modelPerformanceMonitoringStrategyName, frameworkName, hardwareResourceName, postprocessingStepName, performanceMetricName, generalizabilityMeasureName, randomnessStrategyName, modelPurposeName, dataBiasTechniqueName, modelDeploymentProcessName, deploymentPlatformName\n",
      "InverseProperties: isMethodOf, isRawDataOf, isDataFormatOf, isDataAnnotationTechniqueOf, isDataAugmentationTechniqueOf, isDatasetOf, isPreprocessingStepOf, isDataSplitCriteriaOf, isCodeRepositoryOf, isDataRepositoryOf, isCodeRepositoryLinkOf, isDataRepositoryLinkOf, isDeepLearningModelOf, isHyperparameterOf, isHyperparameterOptimizationOf, isOptimizationTechniqueOf, isTrainingCompletionCriteriaOf, isRegularizationMethodOf, isModelPerformanceMonitoringStrategyOf, isFrameworkOf, isHardwareResourceOf, isPostprocessingStepOf, isPerformanceMetricOf, isGeneralizabilityMeasureOf, isRandomnessStrategyOf, isModelPurposeOf, isDataBiasTechniqueOf, isModelDeploymentProcessOf, isDeploymentPlatformOf\n",
      "base_onto_class: ['Nb3e2822944854153a7c8a6b8ba7d7ad1', 'Ncd3895e0a3e04f389ea2f6a2fc794679', 'N638c8c72109147d1a86239ca7d330718', 'Nb213022eb3ae42c4af50812ed0db3d99', 'N9c14d2406c6c4197b2dd59073a392462', 'Nf5a4f1558fde4275a6d4e389b951445c', 'N8b2bc6ce759a472d91d425c71ebff15b', 'N712252efcfe04872850f18c69828711f', 'Thing', 'Activity', 'ActivityInfluence', 'Agent', 'AgentInfluence', 'Association', 'Attribution', 'Bundle', 'Collection', 'Communication', 'Delegation', 'Derivation', 'EmptyCollection', 'End', 'Entity', 'EntityInfluence', 'Generation', 'Influence', 'InstantaneousEvent', 'Invalidation', 'Location', 'Organization', 'Person', 'Plan', 'PrimarySource', 'Quotation', 'Revision', 'Role', 'SoftwareAgent', 'Start', 'Usage']\n",
      "base_onto_property: ['actedOnBehalfOf', 'activity', 'agent', 'alternateOf', 'atLocation', 'entity', 'generated', 'hadActivity', 'hadGeneration', 'hadMember', 'hadPlan', 'hadPrimarySource', 'hadRole', 'hadUsage', 'influenced', 'influencer', 'invalidated', 'qualifiedAssociation', 'qualifiedAttribution', 'qualifiedCommunication', 'qualifiedDelegation', 'qualifiedDerivation', 'qualifiedEnd', 'qualifiedGeneration', 'qualifiedInfluence', 'qualifiedInvalidation', 'qualifiedPrimarySource', 'qualifiedQuotation', 'qualifiedRevision', 'qualifiedStart', 'qualifiedUsage', 'specializationOf', 'used', 'wasAssociatedWith', 'wasAttributedTo', 'wasDerivedFrom', 'wasEndedBy', 'wasGeneratedBy', 'wasInfluencedBy', 'wasInformedBy', 'wasInvalidatedBy', 'wasQuotedFrom', 'wasRevisionOf', 'wasStartedBy']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from helper_functions import load_llm, read_txt, get_base_onto_class\n",
    "from langchain.prompts import PromptTemplate\n",
    "from rdflib import Graph, Namespace\n",
    "# from LLM_loader import llm\n",
    "\n",
    "def Ontology_creation():\n",
    "    # template = read_txt(config.get('Paths', 'Ontology_creation_prompt'))\n",
    "    template = read_txt('../Ontology/Mixtral_8_22b/Prompt/Ontology_creation_with_data_props.txt')\n",
    "    prompt_template = PromptTemplate(input_variables=[\"concepts\", \"relations\",\"data_properties\",\"InverseProperties\",\"base_onto_class\",\"base_onto_property\"], template=template)\n",
    "    # content = read_txt(config.get('Paths', 'Concepts_and_relationships_save_path'))\n",
    "    content = read_txt('../Ontology/Mixtral_8_22b/Concepts_relations/concept_relation_data_inverse.txt')\n",
    "    concepts_s_ind = content.find('Concepts:') + len('Concepts:')\n",
    "    concepts_e_ind = content.find('Relationships:')\n",
    "    concepts = content[concepts_s_ind:concepts_e_ind].strip()\n",
    "    \n",
    "    relationships_s_ind = content.find('Relationships:') + len('Relationships:')\n",
    "    relationships_e_ind = content.find('DataProperties:')\n",
    "    relationships = content[relationships_s_ind:relationships_e_ind].strip()\n",
    "    \n",
    "    data_properties_s_ind = content.find('DataProperties:') + len('DataProperties:')\n",
    "    data_properties_e_ind = content.find('InverseProperties:')\n",
    "    data_properties = content[data_properties_s_ind:data_properties_e_ind].strip()\n",
    "    \n",
    "    inverse_properties_s_ind = content.find('InverseProperties:') + len('InverseProperties:')\n",
    "    inverse_properties = content[inverse_properties_s_ind:].strip()\n",
    "    # base_onto_class, base_onto_property = get_base_onto_class(config.get('Paths', 'SOTAOntology_path'))\n",
    "    base_onto_class, base_onto_property = get_base_onto_class('../SOTAOntologies/prov-o.owl') #PROV-O ontology 참고하는 파일\n",
    "\n",
    "    prompt = prompt_template.format(concepts=concepts,relations=relationships,data_properties=data_properties,InverseProperties=inverse_properties,base_onto_class=base_onto_class,base_onto_property=base_onto_property)\n",
    "    print(prompt)\n",
    "\n",
    "    with open('orig_datapropsandinver_v1.owl',\"w\") as f:\n",
    "        f.write(llm.invoke(prompt).content)\n",
    "        \n",
    "Ontology_creation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38bad2d",
   "metadata": {},
   "source": [
    "##### 변환된 파일 열기 (create_ontology.py)\n",
    "\n",
    "* 추출한 concept_relation_data_inverse.txt 파일(각모델별로 다른 결과가 나왔을 것)을 기준으로 온톨로지를 구축하는 코드라고 하는데, 사실 잘 모르겠음\n",
    "* 실제 개념 간의 관계는 설정하지 않고 그냥 정의만 한 것으로 보임\n",
    "    1. 개념(Concepts)을 OWL 클래스로 정의\n",
    "    2. 관계(Relationships)를 OWL 객체 속성으로 정의\n",
    "    3. 데이터 속성(DataProperties)을 OWL 데이터타입 속성으로 정의\n",
    "    4. 역관계(InverseProperties)를 OWL 객체 속성으로 정의\n",
    "* 주석에보면 이건 데모용이고 실제 데이터의 관계를 표현해야 한다는 내용이 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1462bcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name:GPT4\n",
      "concepts:['Method', 'RawData', 'DataFormat', 'DataAnnotationTechnique', 'DataAugmentationTechnique', 'Dataset', 'PreprocessingStep', 'DataSplitCriteria', 'CodeRepository', 'DataRepository', 'CodeRepositoryLink', 'DataRepositoryLink', 'DeepLearningModel', 'Hyperparameter', 'HyperparameterOptimization', 'OptimizationTechnique', 'TrainingCompletionCriteria', 'RegularizationMethod', 'ModelPerformanceMonitoringStrategy', 'Framework', 'HardwareResource', 'PostprocessingStep', 'PerformanceMetric', 'GeneralizabilityMeasure', 'RandomnessStrategy', 'ModelPurpose', 'DataBiasTechnique', 'ModelDeploymentProcess', 'DeploymentPlatform']\n",
      "relationships:['hasMethod', 'hasRawData', 'hasDataFormat', 'hasDataAnnotationTechnique', 'hasDataAugmentationTechnique', 'hasDataset', 'hasPreprocessingStep', 'hasDataSplitCriteria', 'hasCodeRepository', 'hasDataRepository', 'hasCodeRepositoryLink', 'hasDataRepositoryLink', 'hasDeepLearningModel', 'hasHyperparameter', 'hasHyperparameterOptimization', 'hasOptimizationTechnique', 'hasTrainingCompletionCriteria', 'hasRegularizationMethod', 'hasModelPerformanceMonitoringStrategy', 'hasFramework', 'hasHardwareResource', 'hasPostprocessingStep', 'hasPerformanceMetric', 'hasGeneralizabilityMeasure', 'hasRandomnessStrategy', 'hasModelPurpose', 'hasDataBiasTechnique', 'hasModelDeploymentProcess', 'hasDeploymentPlatform']\n",
      "concept_uri:https://w3id.org/dlprov#Method\n",
      "concept_uri:https://w3id.org/dlprov#RawData\n",
      "concept_uri:https://w3id.org/dlprov#DataFormat\n",
      "concept_uri:https://w3id.org/dlprov#DataAnnotationTechnique\n",
      "concept_uri:https://w3id.org/dlprov#DataAugmentationTechnique\n",
      "concept_uri:https://w3id.org/dlprov#Dataset\n",
      "concept_uri:https://w3id.org/dlprov#PreprocessingStep\n",
      "concept_uri:https://w3id.org/dlprov#DataSplitCriteria\n",
      "concept_uri:https://w3id.org/dlprov#CodeRepository\n",
      "concept_uri:https://w3id.org/dlprov#DataRepository\n",
      "concept_uri:https://w3id.org/dlprov#CodeRepositoryLink\n",
      "concept_uri:https://w3id.org/dlprov#DataRepositoryLink\n",
      "concept_uri:https://w3id.org/dlprov#DeepLearningModel\n",
      "concept_uri:https://w3id.org/dlprov#Hyperparameter\n",
      "concept_uri:https://w3id.org/dlprov#HyperparameterOptimization\n",
      "concept_uri:https://w3id.org/dlprov#OptimizationTechnique\n",
      "concept_uri:https://w3id.org/dlprov#TrainingCompletionCriteria\n",
      "concept_uri:https://w3id.org/dlprov#RegularizationMethod\n",
      "concept_uri:https://w3id.org/dlprov#ModelPerformanceMonitoringStrategy\n",
      "concept_uri:https://w3id.org/dlprov#Framework\n",
      "concept_uri:https://w3id.org/dlprov#HardwareResource\n",
      "concept_uri:https://w3id.org/dlprov#PostprocessingStep\n",
      "concept_uri:https://w3id.org/dlprov#PerformanceMetric\n",
      "concept_uri:https://w3id.org/dlprov#GeneralizabilityMeasure\n",
      "concept_uri:https://w3id.org/dlprov#RandomnessStrategy\n",
      "concept_uri:https://w3id.org/dlprov#ModelPurpose\n",
      "concept_uri:https://w3id.org/dlprov#DataBiasTechnique\n",
      "concept_uri:https://w3id.org/dlprov#ModelDeploymentProcess\n",
      "concept_uri:https://w3id.org/dlprov#DeploymentPlatform\n",
      "rel_uri:https://w3id.org/dlprov#hasMethod\n",
      "rel_uri:https://w3id.org/dlprov#hasRawData\n",
      "rel_uri:https://w3id.org/dlprov#hasDataFormat\n",
      "rel_uri:https://w3id.org/dlprov#hasDataAnnotationTechnique\n",
      "rel_uri:https://w3id.org/dlprov#hasDataAugmentationTechnique\n",
      "rel_uri:https://w3id.org/dlprov#hasDataset\n",
      "rel_uri:https://w3id.org/dlprov#hasPreprocessingStep\n",
      "rel_uri:https://w3id.org/dlprov#hasDataSplitCriteria\n",
      "rel_uri:https://w3id.org/dlprov#hasCodeRepository\n",
      "rel_uri:https://w3id.org/dlprov#hasDataRepository\n",
      "rel_uri:https://w3id.org/dlprov#hasCodeRepositoryLink\n",
      "rel_uri:https://w3id.org/dlprov#hasDataRepositoryLink\n",
      "rel_uri:https://w3id.org/dlprov#hasDeepLearningModel\n",
      "rel_uri:https://w3id.org/dlprov#hasHyperparameter\n",
      "rel_uri:https://w3id.org/dlprov#hasHyperparameterOptimization\n",
      "rel_uri:https://w3id.org/dlprov#hasOptimizationTechnique\n",
      "rel_uri:https://w3id.org/dlprov#hasTrainingCompletionCriteria\n",
      "rel_uri:https://w3id.org/dlprov#hasRegularizationMethod\n",
      "rel_uri:https://w3id.org/dlprov#hasModelPerformanceMonitoringStrategy\n",
      "rel_uri:https://w3id.org/dlprov#hasFramework\n",
      "rel_uri:https://w3id.org/dlprov#hasHardwareResource\n",
      "rel_uri:https://w3id.org/dlprov#hasPostprocessingStep\n",
      "rel_uri:https://w3id.org/dlprov#hasPerformanceMetric\n",
      "rel_uri:https://w3id.org/dlprov#hasGeneralizabilityMeasure\n",
      "rel_uri:https://w3id.org/dlprov#hasRandomnessStrategy\n",
      "rel_uri:https://w3id.org/dlprov#hasModelPurpose\n",
      "rel_uri:https://w3id.org/dlprov#hasDataBiasTechnique\n",
      "rel_uri:https://w3id.org/dlprov#hasModelDeploymentProcess\n",
      "rel_uri:https://w3id.org/dlprov#hasDeploymentPlatform\n",
      "prop_uri:https://w3id.org/dlprov#methodName\n",
      "prop_uri:https://w3id.org/dlprov#dataFormat\n",
      "prop_uri:https://w3id.org/dlprov#dataAnnotationTechniqueName\n",
      "prop_uri:https://w3id.org/dlprov#dataAugmentationTechniqueName\n",
      "prop_uri:https://w3id.org/dlprov#datasetName\n",
      "prop_uri:https://w3id.org/dlprov#preprocessingStepName\n",
      "prop_uri:https://w3id.org/dlprov#dataSplitCriteriaName\n",
      "prop_uri:https://w3id.org/dlprov#codeRepositoryName\n",
      "prop_uri:https://w3id.org/dlprov#dataRepositoryName\n",
      "prop_uri:https://w3id.org/dlprov#codeRepositoryLink\n",
      "prop_uri:https://w3id.org/dlprov#dataRepositoryLink\n",
      "prop_uri:https://w3id.org/dlprov#deepLearningModelName\n",
      "prop_uri:https://w3id.org/dlprov#hyperparameterName\n",
      "prop_uri:https://w3id.org/dlprov#hyperparameterOptimizationName\n",
      "prop_uri:https://w3id.org/dlprov#optimizationTechniqueName\n",
      "prop_uri:https://w3id.org/dlprov#trainingCompletionCriteriaName\n",
      "prop_uri:https://w3id.org/dlprov#regularizationMethodName\n",
      "prop_uri:https://w3id.org/dlprov#modelPerformanceMonitoringStrategyName\n",
      "prop_uri:https://w3id.org/dlprov#frameworkName\n",
      "prop_uri:https://w3id.org/dlprov#hardwareResourceName\n",
      "prop_uri:https://w3id.org/dlprov#postprocessingStepName\n",
      "prop_uri:https://w3id.org/dlprov#performanceMetricName\n",
      "prop_uri:https://w3id.org/dlprov#generalizabilityMeasureName\n",
      "prop_uri:https://w3id.org/dlprov#randomnessStrategyName\n",
      "prop_uri:https://w3id.org/dlprov#modelPurposeName\n",
      "prop_uri:https://w3id.org/dlprov#dataBiasTechniqueName\n",
      "prop_uri:https://w3id.org/dlprov#modelDeploymentProcessName\n",
      "prop_uri:https://w3id.org/dlprov#deploymentPlatformName\n",
      "inv_prop_uri:https://w3id.org/dlprov#isMethodOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isRawDataOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isDataFormatOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isDataAnnotationTechniqueOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isDataAugmentationTechniqueOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isDatasetOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isPreprocessingStepOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isDataSplitCriteriaOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isCodeRepositoryOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isDataRepositoryOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isCodeRepositoryLinkOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isDataRepositoryLinkOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isDeepLearningModelOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isHyperparameterOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isHyperparameterOptimizationOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isOptimizationTechniqueOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isTrainingCompletionCriteriaOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isRegularizationMethodOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isModelPerformanceMonitoringStrategyOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isFrameworkOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isHardwareResourceOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isPostprocessingStepOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isPerformanceMetricOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isGeneralizabilityMeasureOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isRandomnessStrategyOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isModelPurposeOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isDataBiasTechniqueOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isModelDeploymentProcessOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isDeploymentPlatformOf\n",
      "../Ontology/GPT4/Ontology/dlprov.ttl\n",
      "@prefix dlprov: <https://w3id.org/dlprov#> .\n",
      "@prefix owl: <http://www.w3.org/2002/07/owl#> .\n",
      "@prefix prov: <http://www.w3.org/ns/prov#> .\n",
      "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
      "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
      "\n",
      "dlprov:CodeRepository a owl:Class ;\n",
      "    rdfs:label \"CodeRepository\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:CodeRepositoryLink a owl:Class ;\n",
      "    rdfs:label \"CodeRepositoryLink\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:DataAnnotationTechnique a owl:Class ;\n",
      "    rdfs:label \"DataAnnotationTechnique\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:DataAugmentationTechnique a owl:Class ;\n",
      "    rdfs:label \"DataAugmentationTechnique\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:DataBiasTechnique a owl:Class ;\n",
      "    rdfs:label \"DataBiasTechnique\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:DataFormat a owl:Class ;\n",
      "    rdfs:label \"DataFormat\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:DataRepository a owl:Class ;\n",
      "    rdfs:label \"DataRepository\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:DataRepositoryLink a owl:Class ;\n",
      "    rdfs:label \"DataRepositoryLink\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:DataSplitCriteria a owl:Class ;\n",
      "    rdfs:label \"DataSplitCriteria\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:Dataset a owl:Class ;\n",
      "    rdfs:label \"Dataset\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:DeepLearningModel a owl:Class ;\n",
      "    rdfs:label \"DeepLearningModel\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:DeploymentPlatform a owl:Class ;\n",
      "    rdfs:label \"DeploymentPlatform\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:Framework a owl:Class ;\n",
      "    rdfs:label \"Framework\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:GeneralizabilityMeasure a owl:Class ;\n",
      "    rdfs:label \"GeneralizabilityMeasure\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:HardwareResource a owl:Class ;\n",
      "    rdfs:label \"HardwareResource\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:Hyperparameter a owl:Class ;\n",
      "    rdfs:label \"Hyperparameter\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:HyperparameterOptimization a owl:Class ;\n",
      "    rdfs:label \"HyperparameterOptimization\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:Method a owl:Class ;\n",
      "    rdfs:label \"Method\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:ModelDeploymentProcess a owl:Class ;\n",
      "    rdfs:label \"ModelDeploymentProcess\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:ModelPerformanceMonitoringStrategy a owl:Class ;\n",
      "    rdfs:label \"ModelPerformanceMonitoringStrategy\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:ModelPurpose a owl:Class ;\n",
      "    rdfs:label \"ModelPurpose\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:OptimizationTechnique a owl:Class ;\n",
      "    rdfs:label \"OptimizationTechnique\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:PerformanceMetric a owl:Class ;\n",
      "    rdfs:label \"PerformanceMetric\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:PostprocessingStep a owl:Class ;\n",
      "    rdfs:label \"PostprocessingStep\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:PreprocessingStep a owl:Class ;\n",
      "    rdfs:label \"PreprocessingStep\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:RandomnessStrategy a owl:Class ;\n",
      "    rdfs:label \"RandomnessStrategy\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:RawData a owl:Class ;\n",
      "    rdfs:label \"RawData\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:RegularizationMethod a owl:Class ;\n",
      "    rdfs:label \"RegularizationMethod\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:TrainingCompletionCriteria a owl:Class ;\n",
      "    rdfs:label \"TrainingCompletionCriteria\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:codeRepositoryLink a owl:DatatypeProperty ;\n",
      "    rdfs:label \"codeRepositoryLink\"^^xsd:string .\n",
      "\n",
      "dlprov:codeRepositoryName a owl:DatatypeProperty ;\n",
      "    rdfs:label \"codeRepositoryName\"^^xsd:string .\n",
      "\n",
      "dlprov:dataAnnotationTechniqueName a owl:DatatypeProperty ;\n",
      "    rdfs:label \"dataAnnotationTechniqueName\"^^xsd:string .\n",
      "\n",
      "dlprov:dataAugmentationTechniqueName a owl:DatatypeProperty ;\n",
      "    rdfs:label \"dataAugmentationTechniqueName\"^^xsd:string .\n",
      "\n",
      "dlprov:dataBiasTechniqueName a owl:DatatypeProperty ;\n",
      "    rdfs:label \"dataBiasTechniqueName\"^^xsd:string .\n",
      "\n",
      "dlprov:dataFormat a owl:DatatypeProperty ;\n",
      "    rdfs:label \"dataFormat\"^^xsd:string .\n",
      "\n",
      "dlprov:dataRepositoryLink a owl:DatatypeProperty ;\n",
      "    rdfs:label \"dataRepositoryLink\"^^xsd:string .\n",
      "\n",
      "dlprov:dataRepositoryName a owl:DatatypeProperty ;\n",
      "    rdfs:label \"dataRepositoryName\"^^xsd:string .\n",
      "\n",
      "dlprov:dataSplitCriteriaName a owl:DatatypeProperty ;\n",
      "    rdfs:label \"dataSplitCriteriaName\"^^xsd:string .\n",
      "\n",
      "dlprov:datasetName a owl:DatatypeProperty ;\n",
      "    rdfs:label \"datasetName\"^^xsd:string .\n",
      "\n",
      "dlprov:deepLearningModelName a owl:DatatypeProperty ;\n",
      "    rdfs:label \"deepLearningModelName\"^^xsd:string .\n",
      "\n",
      "dlprov:deploymentPlatformName a owl:DatatypeProperty ;\n",
      "    rdfs:label \"deploymentPlatformName\"^^xsd:string .\n",
      "\n",
      "dlprov:frameworkName a owl:DatatypeProperty ;\n",
      "    rdfs:label \"frameworkName\"^^xsd:string .\n",
      "\n",
      "dlprov:generalizabilityMeasureName a owl:DatatypeProperty ;\n",
      "    rdfs:label \"generalizabilityMeasureName\"^^xsd:string .\n",
      "\n",
      "dlprov:hardwareResourceName a owl:DatatypeProperty ;\n",
      "    rdfs:label \"hardwareResourceName\"^^xsd:string .\n",
      "\n",
      "dlprov:hasCodeRepository a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasCodeRepository\"^^xsd:string .\n",
      "\n",
      "dlprov:hasCodeRepositoryLink a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasCodeRepositoryLink\"^^xsd:string .\n",
      "\n",
      "dlprov:hasDataAnnotationTechnique a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasDataAnnotationTechnique\"^^xsd:string .\n",
      "\n",
      "dlprov:hasDataAugmentationTechnique a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasDataAugmentationTechnique\"^^xsd:string .\n",
      "\n",
      "dlprov:hasDataBiasTechnique a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasDataBiasTechnique\"^^xsd:string .\n",
      "\n",
      "dlprov:hasDataFormat a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasDataFormat\"^^xsd:string .\n",
      "\n",
      "dlprov:hasDataRepository a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasDataRepository\"^^xsd:string .\n",
      "\n",
      "dlprov:hasDataRepositoryLink a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasDataRepositoryLink\"^^xsd:string .\n",
      "\n",
      "dlprov:hasDataSplitCriteria a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasDataSplitCriteria\"^^xsd:string .\n",
      "\n",
      "dlprov:hasDataset a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasDataset\"^^xsd:string .\n",
      "\n",
      "dlprov:hasDeepLearningModel a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasDeepLearningModel\"^^xsd:string .\n",
      "\n",
      "dlprov:hasDeploymentPlatform a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasDeploymentPlatform\"^^xsd:string .\n",
      "\n",
      "dlprov:hasFramework a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasFramework\"^^xsd:string .\n",
      "\n",
      "dlprov:hasGeneralizabilityMeasure a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasGeneralizabilityMeasure\"^^xsd:string .\n",
      "\n",
      "dlprov:hasHardwareResource a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasHardwareResource\"^^xsd:string .\n",
      "\n",
      "dlprov:hasHyperparameter a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasHyperparameter\"^^xsd:string .\n",
      "\n",
      "dlprov:hasHyperparameterOptimization a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasHyperparameterOptimization\"^^xsd:string .\n",
      "\n",
      "dlprov:hasMethod a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasMethod\"^^xsd:string .\n",
      "\n",
      "dlprov:hasModelDeploymentProcess a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasModelDeploymentProcess\"^^xsd:string .\n",
      "\n",
      "dlprov:hasModelPerformanceMonitoringStrategy a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasModelPerformanceMonitoringStrategy\"^^xsd:string .\n",
      "\n",
      "dlprov:hasModelPurpose a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasModelPurpose\"^^xsd:string .\n",
      "\n",
      "dlprov:hasOptimizationTechnique a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasOptimizationTechnique\"^^xsd:string .\n",
      "\n",
      "dlprov:hasPerformanceMetric a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasPerformanceMetric\"^^xsd:string .\n",
      "\n",
      "dlprov:hasPostprocessingStep a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasPostprocessingStep\"^^xsd:string .\n",
      "\n",
      "dlprov:hasPreprocessingStep a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasPreprocessingStep\"^^xsd:string .\n",
      "\n",
      "dlprov:hasRandomnessStrategy a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasRandomnessStrategy\"^^xsd:string .\n",
      "\n",
      "dlprov:hasRawData a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasRawData\"^^xsd:string .\n",
      "\n",
      "dlprov:hasRegularizationMethod a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasRegularizationMethod\"^^xsd:string .\n",
      "\n",
      "dlprov:hasTrainingCompletionCriteria a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasTrainingCompletionCriteria\"^^xsd:string .\n",
      "\n",
      "dlprov:hyperparameterName a owl:DatatypeProperty ;\n",
      "    rdfs:label \"hyperparameterName\"^^xsd:string .\n",
      "\n",
      "dlprov:hyperparameterOptimizationName a owl:DatatypeProperty ;\n",
      "    rdfs:label \"hyperparameterOptimizationName\"^^xsd:string .\n",
      "\n",
      "dlprov:methodName a owl:DatatypeProperty ;\n",
      "    rdfs:label \"methodName\"^^xsd:string .\n",
      "\n",
      "dlprov:modelDeploymentProcessName a owl:DatatypeProperty ;\n",
      "    rdfs:label \"modelDeploymentProcessName\"^^xsd:string .\n",
      "\n",
      "dlprov:modelPerformanceMonitoringStrategyName a owl:DatatypeProperty ;\n",
      "    rdfs:label \"modelPerformanceMonitoringStrategyName\"^^xsd:string .\n",
      "\n",
      "dlprov:modelPurposeName a owl:DatatypeProperty ;\n",
      "    rdfs:label \"modelPurposeName\"^^xsd:string .\n",
      "\n",
      "dlprov:optimizationTechniqueName a owl:DatatypeProperty ;\n",
      "    rdfs:label \"optimizationTechniqueName\"^^xsd:string .\n",
      "\n",
      "dlprov:performanceMetricName a owl:DatatypeProperty ;\n",
      "    rdfs:label \"performanceMetricName\"^^xsd:string .\n",
      "\n",
      "dlprov:postprocessingStepName a owl:DatatypeProperty ;\n",
      "    rdfs:label \"postprocessingStepName\"^^xsd:string .\n",
      "\n",
      "dlprov:preprocessingStepName a owl:DatatypeProperty ;\n",
      "    rdfs:label \"preprocessingStepName\"^^xsd:string .\n",
      "\n",
      "dlprov:randomnessStrategyName a owl:DatatypeProperty ;\n",
      "    rdfs:label \"randomnessStrategyName\"^^xsd:string .\n",
      "\n",
      "dlprov:regularizationMethodName a owl:DatatypeProperty ;\n",
      "    rdfs:label \"regularizationMethodName\"^^xsd:string .\n",
      "\n",
      "dlprov:trainingCompletionCriteriaName a owl:DatatypeProperty ;\n",
      "    rdfs:label \"trainingCompletionCriteriaName\"^^xsd:string .\n",
      "\n",
      "dlprov:isCodeRepositoryLinkOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isCodeRepositoryLinkOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isCodeRepositoryLinkOf .\n",
      "\n",
      "dlprov:isCodeRepositoryOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isCodeRepositoryOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isCodeRepositoryOf .\n",
      "\n",
      "dlprov:isDataAnnotationTechniqueOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isDataAnnotationTechniqueOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isDataAnnotationTechniqueOf .\n",
      "\n",
      "dlprov:isDataAugmentationTechniqueOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isDataAugmentationTechniqueOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isDataAugmentationTechniqueOf .\n",
      "\n",
      "dlprov:isDataBiasTechniqueOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isDataBiasTechniqueOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isDataBiasTechniqueOf .\n",
      "\n",
      "dlprov:isDataFormatOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isDataFormatOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isDataFormatOf .\n",
      "\n",
      "dlprov:isDataRepositoryLinkOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isDataRepositoryLinkOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isDataRepositoryLinkOf .\n",
      "\n",
      "dlprov:isDataRepositoryOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isDataRepositoryOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isDataRepositoryOf .\n",
      "\n",
      "dlprov:isDataSplitCriteriaOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isDataSplitCriteriaOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isDataSplitCriteriaOf .\n",
      "\n",
      "dlprov:isDatasetOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isDatasetOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isDatasetOf .\n",
      "\n",
      "dlprov:isDeepLearningModelOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isDeepLearningModelOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isDeepLearningModelOf .\n",
      "\n",
      "dlprov:isDeploymentPlatformOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isDeploymentPlatformOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isDeploymentPlatformOf .\n",
      "\n",
      "dlprov:isFrameworkOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isFrameworkOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isFrameworkOf .\n",
      "\n",
      "dlprov:isGeneralizabilityMeasureOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isGeneralizabilityMeasureOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isGeneralizabilityMeasureOf .\n",
      "\n",
      "dlprov:isHardwareResourceOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isHardwareResourceOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isHardwareResourceOf .\n",
      "\n",
      "dlprov:isHyperparameterOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isHyperparameterOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isHyperparameterOf .\n",
      "\n",
      "dlprov:isHyperparameterOptimizationOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isHyperparameterOptimizationOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isHyperparameterOptimizationOf .\n",
      "\n",
      "dlprov:isMethodOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isMethodOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isMethodOf .\n",
      "\n",
      "dlprov:isModelDeploymentProcessOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isModelDeploymentProcessOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isModelDeploymentProcessOf .\n",
      "\n",
      "dlprov:isModelPerformanceMonitoringStrategyOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isModelPerformanceMonitoringStrategyOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isModelPerformanceMonitoringStrategyOf .\n",
      "\n",
      "dlprov:isModelPurposeOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isModelPurposeOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isModelPurposeOf .\n",
      "\n",
      "dlprov:isOptimizationTechniqueOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isOptimizationTechniqueOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isOptimizationTechniqueOf .\n",
      "\n",
      "dlprov:isPerformanceMetricOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isPerformanceMetricOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isPerformanceMetricOf .\n",
      "\n",
      "dlprov:isPostprocessingStepOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isPostprocessingStepOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isPostprocessingStepOf .\n",
      "\n",
      "dlprov:isPreprocessingStepOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isPreprocessingStepOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isPreprocessingStepOf .\n",
      "\n",
      "dlprov:isRandomnessStrategyOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isRandomnessStrategyOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isRandomnessStrategyOf .\n",
      "\n",
      "dlprov:isRawDataOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isRawDataOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isRawDataOf .\n",
      "\n",
      "dlprov:isRegularizationMethodOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isRegularizationMethodOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isRegularizationMethodOf .\n",
      "\n",
      "dlprov:isTrainingCompletionCriteriaOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isTrainingCompletionCriteriaOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isTrainingCompletionCriteriaOf .\n",
      "\n",
      "\n",
      "Ontology for GPT4 created and saved at ../Ontology/GPT4/Ontology/dlprov.ttl\n",
      "model_name:GPT3.5\n",
      "concepts:['RawDataCollectionMethod', 'DataFormat', 'DataAnnotationTechnique', 'DataAugmentationTechnique', 'Dataset', 'PreprocessingStep', 'DataSplittingCriteria', 'CodeRepository', 'DataRepository', 'ModelType', 'Hyperparameter', 'HyperparameterOptimizationTechnique', 'OptimizationTechnique', 'TrainingCompletionCriteria', 'RegularizationMethod', 'ModelPerformanceMonitoringStrategy', 'DeepLearningFramework', 'HardwareResource', 'PostprocessingStep', 'EvaluationMetric', 'GeneralizabilityMeasure', 'RandomnessHandlingStrategy', 'ModelPurpose', 'DataBiasHandlingTechnique', 'ModelDeploymentProcess', 'DeploymentPlatform']\n",
      "relationships:['utilizesRawDataCollectionMethod', 'hasDataFormat', 'hasDataAnnotationTechnique', 'hasDataAugmentationTechnique', 'hasDataset', 'involvesPreprocessingStep', 'usesDataSplittingCriteria', 'hasCodeRepository', 'hasDataRepository', 'usesModelType', 'hasHyperparameter', 'optimizesHyperparameter', 'appliesOptimizationTechnique', 'determinesTrainingCompletion', 'usesRegularizationMethod', 'implementsModelPerformanceMonitoring', 'usesDeepLearningFramework', 'usesHardwareResource', 'involvesPostprocessingStep', 'evaluatesWithMetric', 'ensuresGeneralizabilityWithMeasure', 'employsRandomnessHandlingStrategy', 'servesModelPurpose', 'addressesDataBiasWithTechnique', 'followsModelDeploymentProcess', 'deploysOnPlatform']\n",
      "concept_uri:https://w3id.org/dlprov#RawDataCollectionMethod\n",
      "concept_uri:https://w3id.org/dlprov#DataFormat\n",
      "concept_uri:https://w3id.org/dlprov#DataAnnotationTechnique\n",
      "concept_uri:https://w3id.org/dlprov#DataAugmentationTechnique\n",
      "concept_uri:https://w3id.org/dlprov#Dataset\n",
      "concept_uri:https://w3id.org/dlprov#PreprocessingStep\n",
      "concept_uri:https://w3id.org/dlprov#DataSplittingCriteria\n",
      "concept_uri:https://w3id.org/dlprov#CodeRepository\n",
      "concept_uri:https://w3id.org/dlprov#DataRepository\n",
      "concept_uri:https://w3id.org/dlprov#ModelType\n",
      "concept_uri:https://w3id.org/dlprov#Hyperparameter\n",
      "concept_uri:https://w3id.org/dlprov#HyperparameterOptimizationTechnique\n",
      "concept_uri:https://w3id.org/dlprov#OptimizationTechnique\n",
      "concept_uri:https://w3id.org/dlprov#TrainingCompletionCriteria\n",
      "concept_uri:https://w3id.org/dlprov#RegularizationMethod\n",
      "concept_uri:https://w3id.org/dlprov#ModelPerformanceMonitoringStrategy\n",
      "concept_uri:https://w3id.org/dlprov#DeepLearningFramework\n",
      "concept_uri:https://w3id.org/dlprov#HardwareResource\n",
      "concept_uri:https://w3id.org/dlprov#PostprocessingStep\n",
      "concept_uri:https://w3id.org/dlprov#EvaluationMetric\n",
      "concept_uri:https://w3id.org/dlprov#GeneralizabilityMeasure\n",
      "concept_uri:https://w3id.org/dlprov#RandomnessHandlingStrategy\n",
      "concept_uri:https://w3id.org/dlprov#ModelPurpose\n",
      "concept_uri:https://w3id.org/dlprov#DataBiasHandlingTechnique\n",
      "concept_uri:https://w3id.org/dlprov#ModelDeploymentProcess\n",
      "concept_uri:https://w3id.org/dlprov#DeploymentPlatform\n",
      "rel_uri:https://w3id.org/dlprov#utilizesRawDataCollectionMethod\n",
      "rel_uri:https://w3id.org/dlprov#hasDataFormat\n",
      "rel_uri:https://w3id.org/dlprov#hasDataAnnotationTechnique\n",
      "rel_uri:https://w3id.org/dlprov#hasDataAugmentationTechnique\n",
      "rel_uri:https://w3id.org/dlprov#hasDataset\n",
      "rel_uri:https://w3id.org/dlprov#involvesPreprocessingStep\n",
      "rel_uri:https://w3id.org/dlprov#usesDataSplittingCriteria\n",
      "rel_uri:https://w3id.org/dlprov#hasCodeRepository\n",
      "rel_uri:https://w3id.org/dlprov#hasDataRepository\n",
      "rel_uri:https://w3id.org/dlprov#usesModelType\n",
      "rel_uri:https://w3id.org/dlprov#hasHyperparameter\n",
      "rel_uri:https://w3id.org/dlprov#optimizesHyperparameter\n",
      "rel_uri:https://w3id.org/dlprov#appliesOptimizationTechnique\n",
      "rel_uri:https://w3id.org/dlprov#determinesTrainingCompletion\n",
      "rel_uri:https://w3id.org/dlprov#usesRegularizationMethod\n",
      "rel_uri:https://w3id.org/dlprov#implementsModelPerformanceMonitoring\n",
      "rel_uri:https://w3id.org/dlprov#usesDeepLearningFramework\n",
      "rel_uri:https://w3id.org/dlprov#usesHardwareResource\n",
      "rel_uri:https://w3id.org/dlprov#involvesPostprocessingStep\n",
      "rel_uri:https://w3id.org/dlprov#evaluatesWithMetric\n",
      "rel_uri:https://w3id.org/dlprov#ensuresGeneralizabilityWithMeasure\n",
      "rel_uri:https://w3id.org/dlprov#employsRandomnessHandlingStrategy\n",
      "rel_uri:https://w3id.org/dlprov#servesModelPurpose\n",
      "rel_uri:https://w3id.org/dlprov#addressesDataBiasWithTechnique\n",
      "rel_uri:https://w3id.org/dlprov#followsModelDeploymentProcess\n",
      "rel_uri:https://w3id.org/dlprov#deploysOnPlatform\n",
      "prop_uri:https://w3id.org/dlprov#repositoryLink\n",
      "inv_prop_uri:https://w3id.org/dlprov#isUtilizedByRawDataCollectionMethod\n",
      "inv_prop_uri:https://w3id.org/dlprov#isDataFormatOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isDataAnnotationTechniqueOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isDataAugmentationTechniqueOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isDatasetOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isPreprocessingStepOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isDataSplittingCriteriaOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isCodeRepositoryOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isDataRepositoryOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isModelTypeOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isHyperparameterOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isHyperparameterOptimizationTechniqueOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isOptimizationTechniqueOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isTrainingCompletionCriteriaOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isRegularizationMethodOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isModelPerformanceMonitoringStrategyOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isDeepLearningFrameworkOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isHardwareResourceOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isPostprocessingStepOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isEvaluationMetricOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isGeneralizabilityMeasureOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isRandomnessHandlingStrategyOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isModelPurposeOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isDataBiasHandlingTechniqueOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isModelDeploymentProcessOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isDeploymentPlatformOf\n",
      "../Ontology/GPT3.5/Ontology/dlprov.ttl\n",
      "@prefix dlprov: <https://w3id.org/dlprov#> .\n",
      "@prefix owl: <http://www.w3.org/2002/07/owl#> .\n",
      "@prefix prov: <http://www.w3.org/ns/prov#> .\n",
      "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
      "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
      "\n",
      "dlprov:CodeRepository a owl:Class ;\n",
      "    rdfs:label \"CodeRepository\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:DataAnnotationTechnique a owl:Class ;\n",
      "    rdfs:label \"DataAnnotationTechnique\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:DataAugmentationTechnique a owl:Class ;\n",
      "    rdfs:label \"DataAugmentationTechnique\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:DataBiasHandlingTechnique a owl:Class ;\n",
      "    rdfs:label \"DataBiasHandlingTechnique\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:DataFormat a owl:Class ;\n",
      "    rdfs:label \"DataFormat\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:DataRepository a owl:Class ;\n",
      "    rdfs:label \"DataRepository\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:DataSplittingCriteria a owl:Class ;\n",
      "    rdfs:label \"DataSplittingCriteria\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:Dataset a owl:Class ;\n",
      "    rdfs:label \"Dataset\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:DeepLearningFramework a owl:Class ;\n",
      "    rdfs:label \"DeepLearningFramework\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:DeploymentPlatform a owl:Class ;\n",
      "    rdfs:label \"DeploymentPlatform\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:EvaluationMetric a owl:Class ;\n",
      "    rdfs:label \"EvaluationMetric\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:GeneralizabilityMeasure a owl:Class ;\n",
      "    rdfs:label \"GeneralizabilityMeasure\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:HardwareResource a owl:Class ;\n",
      "    rdfs:label \"HardwareResource\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:Hyperparameter a owl:Class ;\n",
      "    rdfs:label \"Hyperparameter\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:HyperparameterOptimizationTechnique a owl:Class ;\n",
      "    rdfs:label \"HyperparameterOptimizationTechnique\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:ModelDeploymentProcess a owl:Class ;\n",
      "    rdfs:label \"ModelDeploymentProcess\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:ModelPerformanceMonitoringStrategy a owl:Class ;\n",
      "    rdfs:label \"ModelPerformanceMonitoringStrategy\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:ModelPurpose a owl:Class ;\n",
      "    rdfs:label \"ModelPurpose\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:ModelType a owl:Class ;\n",
      "    rdfs:label \"ModelType\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:OptimizationTechnique a owl:Class ;\n",
      "    rdfs:label \"OptimizationTechnique\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:PostprocessingStep a owl:Class ;\n",
      "    rdfs:label \"PostprocessingStep\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:PreprocessingStep a owl:Class ;\n",
      "    rdfs:label \"PreprocessingStep\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:RandomnessHandlingStrategy a owl:Class ;\n",
      "    rdfs:label \"RandomnessHandlingStrategy\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:RawDataCollectionMethod a owl:Class ;\n",
      "    rdfs:label \"RawDataCollectionMethod\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:RegularizationMethod a owl:Class ;\n",
      "    rdfs:label \"RegularizationMethod\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:TrainingCompletionCriteria a owl:Class ;\n",
      "    rdfs:label \"TrainingCompletionCriteria\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:addressesDataBiasWithTechnique a owl:ObjectProperty ;\n",
      "    rdfs:label \"addressesDataBiasWithTechnique\"^^xsd:string .\n",
      "\n",
      "dlprov:appliesOptimizationTechnique a owl:ObjectProperty ;\n",
      "    rdfs:label \"appliesOptimizationTechnique\"^^xsd:string .\n",
      "\n",
      "dlprov:deploysOnPlatform a owl:ObjectProperty ;\n",
      "    rdfs:label \"deploysOnPlatform\"^^xsd:string .\n",
      "\n",
      "dlprov:determinesTrainingCompletion a owl:ObjectProperty ;\n",
      "    rdfs:label \"determinesTrainingCompletion\"^^xsd:string .\n",
      "\n",
      "dlprov:employsRandomnessHandlingStrategy a owl:ObjectProperty ;\n",
      "    rdfs:label \"employsRandomnessHandlingStrategy\"^^xsd:string .\n",
      "\n",
      "dlprov:ensuresGeneralizabilityWithMeasure a owl:ObjectProperty ;\n",
      "    rdfs:label \"ensuresGeneralizabilityWithMeasure\"^^xsd:string .\n",
      "\n",
      "dlprov:evaluatesWithMetric a owl:ObjectProperty ;\n",
      "    rdfs:label \"evaluatesWithMetric\"^^xsd:string .\n",
      "\n",
      "dlprov:followsModelDeploymentProcess a owl:ObjectProperty ;\n",
      "    rdfs:label \"followsModelDeploymentProcess\"^^xsd:string .\n",
      "\n",
      "dlprov:hasCodeRepository a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasCodeRepository\"^^xsd:string .\n",
      "\n",
      "dlprov:hasDataAnnotationTechnique a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasDataAnnotationTechnique\"^^xsd:string .\n",
      "\n",
      "dlprov:hasDataAugmentationTechnique a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasDataAugmentationTechnique\"^^xsd:string .\n",
      "\n",
      "dlprov:hasDataFormat a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasDataFormat\"^^xsd:string .\n",
      "\n",
      "dlprov:hasDataRepository a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasDataRepository\"^^xsd:string .\n",
      "\n",
      "dlprov:hasDataset a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasDataset\"^^xsd:string .\n",
      "\n",
      "dlprov:hasHyperparameter a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasHyperparameter\"^^xsd:string .\n",
      "\n",
      "dlprov:implementsModelPerformanceMonitoring a owl:ObjectProperty ;\n",
      "    rdfs:label \"implementsModelPerformanceMonitoring\"^^xsd:string .\n",
      "\n",
      "dlprov:involvesPostprocessingStep a owl:ObjectProperty ;\n",
      "    rdfs:label \"involvesPostprocessingStep\"^^xsd:string .\n",
      "\n",
      "dlprov:involvesPreprocessingStep a owl:ObjectProperty ;\n",
      "    rdfs:label \"involvesPreprocessingStep\"^^xsd:string .\n",
      "\n",
      "dlprov:optimizesHyperparameter a owl:ObjectProperty ;\n",
      "    rdfs:label \"optimizesHyperparameter\"^^xsd:string .\n",
      "\n",
      "dlprov:repositoryLink a owl:DatatypeProperty ;\n",
      "    rdfs:label \"repositoryLink\"^^xsd:string .\n",
      "\n",
      "dlprov:servesModelPurpose a owl:ObjectProperty ;\n",
      "    rdfs:label \"servesModelPurpose\"^^xsd:string .\n",
      "\n",
      "dlprov:usesDataSplittingCriteria a owl:ObjectProperty ;\n",
      "    rdfs:label \"usesDataSplittingCriteria\"^^xsd:string .\n",
      "\n",
      "dlprov:usesDeepLearningFramework a owl:ObjectProperty ;\n",
      "    rdfs:label \"usesDeepLearningFramework\"^^xsd:string .\n",
      "\n",
      "dlprov:usesHardwareResource a owl:ObjectProperty ;\n",
      "    rdfs:label \"usesHardwareResource\"^^xsd:string .\n",
      "\n",
      "dlprov:usesModelType a owl:ObjectProperty ;\n",
      "    rdfs:label \"usesModelType\"^^xsd:string .\n",
      "\n",
      "dlprov:usesRegularizationMethod a owl:ObjectProperty ;\n",
      "    rdfs:label \"usesRegularizationMethod\"^^xsd:string .\n",
      "\n",
      "dlprov:utilizesRawDataCollectionMethod a owl:ObjectProperty ;\n",
      "    rdfs:label \"utilizesRawDataCollectionMethod\"^^xsd:string .\n",
      "\n",
      "dlprov:isCodeRepositoryOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isCodeRepositoryOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isCodeRepositoryOf .\n",
      "\n",
      "dlprov:isDataAnnotationTechniqueOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isDataAnnotationTechniqueOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isDataAnnotationTechniqueOf .\n",
      "\n",
      "dlprov:isDataAugmentationTechniqueOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isDataAugmentationTechniqueOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isDataAugmentationTechniqueOf .\n",
      "\n",
      "dlprov:isDataBiasHandlingTechniqueOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isDataBiasHandlingTechniqueOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isDataBiasHandlingTechniqueOf .\n",
      "\n",
      "dlprov:isDataFormatOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isDataFormatOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isDataFormatOf .\n",
      "\n",
      "dlprov:isDataRepositoryOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isDataRepositoryOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isDataRepositoryOf .\n",
      "\n",
      "dlprov:isDataSplittingCriteriaOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isDataSplittingCriteriaOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isDataSplittingCriteriaOf .\n",
      "\n",
      "dlprov:isDatasetOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isDatasetOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isDatasetOf .\n",
      "\n",
      "dlprov:isDeepLearningFrameworkOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isDeepLearningFrameworkOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isDeepLearningFrameworkOf .\n",
      "\n",
      "dlprov:isDeploymentPlatformOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isDeploymentPlatformOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isDeploymentPlatformOf .\n",
      "\n",
      "dlprov:isEvaluationMetricOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isEvaluationMetricOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isEvaluationMetricOf .\n",
      "\n",
      "dlprov:isGeneralizabilityMeasureOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isGeneralizabilityMeasureOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isGeneralizabilityMeasureOf .\n",
      "\n",
      "dlprov:isHardwareResourceOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isHardwareResourceOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isHardwareResourceOf .\n",
      "\n",
      "dlprov:isHyperparameterOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isHyperparameterOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isHyperparameterOf .\n",
      "\n",
      "dlprov:isHyperparameterOptimizationTechniqueOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isHyperparameterOptimizationTechniqueOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isHyperparameterOptimizationTechniqueOf .\n",
      "\n",
      "dlprov:isModelDeploymentProcessOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isModelDeploymentProcessOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isModelDeploymentProcessOf .\n",
      "\n",
      "dlprov:isModelPerformanceMonitoringStrategyOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isModelPerformanceMonitoringStrategyOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isModelPerformanceMonitoringStrategyOf .\n",
      "\n",
      "dlprov:isModelPurposeOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isModelPurposeOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isModelPurposeOf .\n",
      "\n",
      "dlprov:isModelTypeOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isModelTypeOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isModelTypeOf .\n",
      "\n",
      "dlprov:isOptimizationTechniqueOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isOptimizationTechniqueOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isOptimizationTechniqueOf .\n",
      "\n",
      "dlprov:isPostprocessingStepOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isPostprocessingStepOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isPostprocessingStepOf .\n",
      "\n",
      "dlprov:isPreprocessingStepOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isPreprocessingStepOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isPreprocessingStepOf .\n",
      "\n",
      "dlprov:isRandomnessHandlingStrategyOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isRandomnessHandlingStrategyOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isRandomnessHandlingStrategyOf .\n",
      "\n",
      "dlprov:isRegularizationMethodOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isRegularizationMethodOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isRegularizationMethodOf .\n",
      "\n",
      "dlprov:isTrainingCompletionCriteriaOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isTrainingCompletionCriteriaOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isTrainingCompletionCriteriaOf .\n",
      "\n",
      "dlprov:isUtilizedByRawDataCollectionMethod a owl:ObjectProperty ;\n",
      "    rdfs:label \"isUtilizedByRawDataCollectionMethod\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isUtilizedByRawDataCollectionMethod .\n",
      "\n",
      "\n",
      "Ontology for GPT3.5 created and saved at ../Ontology/GPT3.5/Ontology/dlprov.ttl\n",
      "model_name:Gemini\n",
      "concepts:['DeepLearningPipeline', 'RawDataCollectionMethod', 'DataFormat', 'DataAnnotationTechnique', 'DataAugmentationTechnique', 'Dataset', 'PreprocessingStep', 'DataSplitCriteria', 'CodeRepository', 'DataRepository', 'DeepLearningModelType', 'Hyperparameter', 'HyperparameterOptimizationTechnique', 'OptimizationTechnique', 'TrainingCompletionCriteria', 'RegularizationMethod', 'MonitoringStrategy', 'DeepLearningFramework', 'HardwareResource', 'PostprocessingStep', 'EvaluationMetric', 'GeneralizabilityMeasure', 'RandomnessHandlingStrategy', 'ModelPurpose']\n",
      "relationships:['hasRawDataCollectionMethod', 'hasDataFormat', 'hasDataAnnotationTechnique', 'hasDataAugmentationTechnique', 'hasDataset', 'involvesPreprocessingStep', 'hasDataSplitCriteria', 'hasCodeRepository', 'hasDataRepository', 'hasLink', 'usesDeepLearningModelType', 'hasHyperparameter', 'usesHyperparameterOptimizationTechnique', 'usesOptimizationTechnique', 'hasTrainingCompletionCriteria', 'usesRegularizationMethod', 'hasMonitoringStrategy', 'usesDeepLearningFramework', 'usesHardwareResource', 'involvesPostprocessingStep', 'usesEvaluationMetric', 'ensuresGeneralizability', 'usesRandomnessHandlingStrategy', 'hasModelPurpose', 'addressesDataBias']\n",
      "concept_uri:https://w3id.org/dlprov#DeepLearningPipeline\n",
      "concept_uri:https://w3id.org/dlprov#RawDataCollectionMethod\n",
      "concept_uri:https://w3id.org/dlprov#DataFormat\n",
      "concept_uri:https://w3id.org/dlprov#DataAnnotationTechnique\n",
      "concept_uri:https://w3id.org/dlprov#DataAugmentationTechnique\n",
      "concept_uri:https://w3id.org/dlprov#Dataset\n",
      "concept_uri:https://w3id.org/dlprov#PreprocessingStep\n",
      "concept_uri:https://w3id.org/dlprov#DataSplitCriteria\n",
      "concept_uri:https://w3id.org/dlprov#CodeRepository\n",
      "concept_uri:https://w3id.org/dlprov#DataRepository\n",
      "concept_uri:https://w3id.org/dlprov#DeepLearningModelType\n",
      "concept_uri:https://w3id.org/dlprov#Hyperparameter\n",
      "concept_uri:https://w3id.org/dlprov#HyperparameterOptimizationTechnique\n",
      "concept_uri:https://w3id.org/dlprov#OptimizationTechnique\n",
      "concept_uri:https://w3id.org/dlprov#TrainingCompletionCriteria\n",
      "concept_uri:https://w3id.org/dlprov#RegularizationMethod\n",
      "concept_uri:https://w3id.org/dlprov#MonitoringStrategy\n",
      "concept_uri:https://w3id.org/dlprov#DeepLearningFramework\n",
      "concept_uri:https://w3id.org/dlprov#HardwareResource\n",
      "concept_uri:https://w3id.org/dlprov#PostprocessingStep\n",
      "concept_uri:https://w3id.org/dlprov#EvaluationMetric\n",
      "concept_uri:https://w3id.org/dlprov#GeneralizabilityMeasure\n",
      "concept_uri:https://w3id.org/dlprov#RandomnessHandlingStrategy\n",
      "concept_uri:https://w3id.org/dlprov#ModelPurpose\n",
      "rel_uri:https://w3id.org/dlprov#hasRawDataCollectionMethod\n",
      "rel_uri:https://w3id.org/dlprov#hasDataFormat\n",
      "rel_uri:https://w3id.org/dlprov#hasDataAnnotationTechnique\n",
      "rel_uri:https://w3id.org/dlprov#hasDataAugmentationTechnique\n",
      "rel_uri:https://w3id.org/dlprov#hasDataset\n",
      "rel_uri:https://w3id.org/dlprov#involvesPreprocessingStep\n",
      "rel_uri:https://w3id.org/dlprov#hasDataSplitCriteria\n",
      "rel_uri:https://w3id.org/dlprov#hasCodeRepository\n",
      "rel_uri:https://w3id.org/dlprov#hasDataRepository\n",
      "rel_uri:https://w3id.org/dlprov#hasLink\n",
      "rel_uri:https://w3id.org/dlprov#usesDeepLearningModelType\n",
      "rel_uri:https://w3id.org/dlprov#hasHyperparameter\n",
      "rel_uri:https://w3id.org/dlprov#usesHyperparameterOptimizationTechnique\n",
      "rel_uri:https://w3id.org/dlprov#usesOptimizationTechnique\n",
      "rel_uri:https://w3id.org/dlprov#hasTrainingCompletionCriteria\n",
      "rel_uri:https://w3id.org/dlprov#usesRegularizationMethod\n",
      "rel_uri:https://w3id.org/dlprov#hasMonitoringStrategy\n",
      "rel_uri:https://w3id.org/dlprov#usesDeepLearningFramework\n",
      "rel_uri:https://w3id.org/dlprov#usesHardwareResource\n",
      "rel_uri:https://w3id.org/dlprov#involvesPostprocessingStep\n",
      "rel_uri:https://w3id.org/dlprov#usesEvaluationMetric\n",
      "rel_uri:https://w3id.org/dlprov#ensuresGeneralizability\n",
      "rel_uri:https://w3id.org/dlprov#usesRandomnessHandlingStrategy\n",
      "rel_uri:https://w3id.org/dlprov#hasModelPurpose\n",
      "rel_uri:https://w3id.org/dlprov#addressesDataBias\n",
      "prop_uri:https://w3id.org/dlprov#codeRepositoryLink\n",
      "prop_uri:https://w3id.org/dlprov#dataRepositoryLink\n",
      "inv_prop_uri:https://w3id.org/dlprov#isRawDataCollectionMethodOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isDataFormatOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isDataAnnotationTechniqueOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isDataAugmentationTechniqueOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isDatasetOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isPreprocessingStepOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isDataSplitCriteriaOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isCodeRepositoryOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isDataRepositoryOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isModelOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isHyperparameterOf\n",
      "inv_prop_uri:https://w3id.org/dlprov#isOptimizedBy\n",
      "inv_prop_uri:https://w3id.org/dlprov#isTrainingCompleteBy\n",
      "inv_prop_uri:https://w3id.org/dlprov#isRegularizedBy\n",
      "inv_prop_uri:https://w3id.org/dlprov#isMonitoredBy\n",
      "inv_prop_uri:https://w3id.org/dlprov#isBuiltBy\n",
      "inv_prop_uri:https://w3id.org/dlprov#isTrainedOn\n",
      "inv_prop_uri:https://w3id.org/dlprov#isPostprocessedBy\n",
      "inv_prop_uri:https://w3id.org/dlprov#isEvaluatedBy\n",
      "inv_prop_uri:https://w3id.org/dlprov#isGeneralizableBy\n",
      "inv_prop_uri:https://w3id.org/dlprov#isHandledBy\n",
      "../Ontology/Gemini/Ontology/dlprov.ttl\n",
      "@prefix dlprov: <https://w3id.org/dlprov#> .\n",
      "@prefix owl: <http://www.w3.org/2002/07/owl#> .\n",
      "@prefix prov: <http://www.w3.org/ns/prov#> .\n",
      "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
      "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
      "\n",
      "dlprov:CodeRepository a owl:Class ;\n",
      "    rdfs:label \"CodeRepository\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:DataAnnotationTechnique a owl:Class ;\n",
      "    rdfs:label \"DataAnnotationTechnique\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:DataAugmentationTechnique a owl:Class ;\n",
      "    rdfs:label \"DataAugmentationTechnique\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:DataFormat a owl:Class ;\n",
      "    rdfs:label \"DataFormat\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:DataRepository a owl:Class ;\n",
      "    rdfs:label \"DataRepository\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:DataSplitCriteria a owl:Class ;\n",
      "    rdfs:label \"DataSplitCriteria\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:Dataset a owl:Class ;\n",
      "    rdfs:label \"Dataset\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:DeepLearningFramework a owl:Class ;\n",
      "    rdfs:label \"DeepLearningFramework\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:DeepLearningModelType a owl:Class ;\n",
      "    rdfs:label \"DeepLearningModelType\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:DeepLearningPipeline a owl:Class ;\n",
      "    rdfs:label \"DeepLearningPipeline\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:EvaluationMetric a owl:Class ;\n",
      "    rdfs:label \"EvaluationMetric\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:GeneralizabilityMeasure a owl:Class ;\n",
      "    rdfs:label \"GeneralizabilityMeasure\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:HardwareResource a owl:Class ;\n",
      "    rdfs:label \"HardwareResource\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:Hyperparameter a owl:Class ;\n",
      "    rdfs:label \"Hyperparameter\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:HyperparameterOptimizationTechnique a owl:Class ;\n",
      "    rdfs:label \"HyperparameterOptimizationTechnique\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:ModelPurpose a owl:Class ;\n",
      "    rdfs:label \"ModelPurpose\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:MonitoringStrategy a owl:Class ;\n",
      "    rdfs:label \"MonitoringStrategy\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:OptimizationTechnique a owl:Class ;\n",
      "    rdfs:label \"OptimizationTechnique\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:PostprocessingStep a owl:Class ;\n",
      "    rdfs:label \"PostprocessingStep\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:PreprocessingStep a owl:Class ;\n",
      "    rdfs:label \"PreprocessingStep\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:RandomnessHandlingStrategy a owl:Class ;\n",
      "    rdfs:label \"RandomnessHandlingStrategy\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:RawDataCollectionMethod a owl:Class ;\n",
      "    rdfs:label \"RawDataCollectionMethod\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:RegularizationMethod a owl:Class ;\n",
      "    rdfs:label \"RegularizationMethod\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:TrainingCompletionCriteria a owl:Class ;\n",
      "    rdfs:label \"TrainingCompletionCriteria\"^^xsd:string ;\n",
      "    rdfs:subClassOf prov:Entity .\n",
      "\n",
      "dlprov:addressesDataBias a owl:ObjectProperty ;\n",
      "    rdfs:label \"addressesDataBias\"^^xsd:string .\n",
      "\n",
      "dlprov:codeRepositoryLink a owl:DatatypeProperty ;\n",
      "    rdfs:label \"codeRepositoryLink\"^^xsd:string .\n",
      "\n",
      "dlprov:dataRepositoryLink a owl:DatatypeProperty ;\n",
      "    rdfs:label \"dataRepositoryLink\"^^xsd:string .\n",
      "\n",
      "dlprov:ensuresGeneralizability a owl:ObjectProperty ;\n",
      "    rdfs:label \"ensuresGeneralizability\"^^xsd:string .\n",
      "\n",
      "dlprov:hasCodeRepository a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasCodeRepository\"^^xsd:string .\n",
      "\n",
      "dlprov:hasDataAnnotationTechnique a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasDataAnnotationTechnique\"^^xsd:string .\n",
      "\n",
      "dlprov:hasDataAugmentationTechnique a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasDataAugmentationTechnique\"^^xsd:string .\n",
      "\n",
      "dlprov:hasDataFormat a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasDataFormat\"^^xsd:string .\n",
      "\n",
      "dlprov:hasDataRepository a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasDataRepository\"^^xsd:string .\n",
      "\n",
      "dlprov:hasDataSplitCriteria a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasDataSplitCriteria\"^^xsd:string .\n",
      "\n",
      "dlprov:hasDataset a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasDataset\"^^xsd:string .\n",
      "\n",
      "dlprov:hasHyperparameter a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasHyperparameter\"^^xsd:string .\n",
      "\n",
      "dlprov:hasLink a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasLink\"^^xsd:string .\n",
      "\n",
      "dlprov:hasModelPurpose a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasModelPurpose\"^^xsd:string .\n",
      "\n",
      "dlprov:hasMonitoringStrategy a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasMonitoringStrategy\"^^xsd:string .\n",
      "\n",
      "dlprov:hasRawDataCollectionMethod a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasRawDataCollectionMethod\"^^xsd:string .\n",
      "\n",
      "dlprov:hasTrainingCompletionCriteria a owl:ObjectProperty ;\n",
      "    rdfs:label \"hasTrainingCompletionCriteria\"^^xsd:string .\n",
      "\n",
      "dlprov:involvesPostprocessingStep a owl:ObjectProperty ;\n",
      "    rdfs:label \"involvesPostprocessingStep\"^^xsd:string .\n",
      "\n",
      "dlprov:involvesPreprocessingStep a owl:ObjectProperty ;\n",
      "    rdfs:label \"involvesPreprocessingStep\"^^xsd:string .\n",
      "\n",
      "dlprov:usesDeepLearningFramework a owl:ObjectProperty ;\n",
      "    rdfs:label \"usesDeepLearningFramework\"^^xsd:string .\n",
      "\n",
      "dlprov:usesDeepLearningModelType a owl:ObjectProperty ;\n",
      "    rdfs:label \"usesDeepLearningModelType\"^^xsd:string .\n",
      "\n",
      "dlprov:usesEvaluationMetric a owl:ObjectProperty ;\n",
      "    rdfs:label \"usesEvaluationMetric\"^^xsd:string .\n",
      "\n",
      "dlprov:usesHardwareResource a owl:ObjectProperty ;\n",
      "    rdfs:label \"usesHardwareResource\"^^xsd:string .\n",
      "\n",
      "dlprov:usesHyperparameterOptimizationTechnique a owl:ObjectProperty ;\n",
      "    rdfs:label \"usesHyperparameterOptimizationTechnique\"^^xsd:string .\n",
      "\n",
      "dlprov:usesOptimizationTechnique a owl:ObjectProperty ;\n",
      "    rdfs:label \"usesOptimizationTechnique\"^^xsd:string .\n",
      "\n",
      "dlprov:usesRandomnessHandlingStrategy a owl:ObjectProperty ;\n",
      "    rdfs:label \"usesRandomnessHandlingStrategy\"^^xsd:string .\n",
      "\n",
      "dlprov:usesRegularizationMethod a owl:ObjectProperty ;\n",
      "    rdfs:label \"usesRegularizationMethod\"^^xsd:string .\n",
      "\n",
      "dlprov:isBuiltBy a owl:ObjectProperty ;\n",
      "    rdfs:label \"isBuiltBy\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isBuiltBy .\n",
      "\n",
      "dlprov:isCodeRepositoryOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isCodeRepositoryOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isCodeRepositoryOf .\n",
      "\n",
      "dlprov:isDataAnnotationTechniqueOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isDataAnnotationTechniqueOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isDataAnnotationTechniqueOf .\n",
      "\n",
      "dlprov:isDataAugmentationTechniqueOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isDataAugmentationTechniqueOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isDataAugmentationTechniqueOf .\n",
      "\n",
      "dlprov:isDataFormatOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isDataFormatOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isDataFormatOf .\n",
      "\n",
      "dlprov:isDataRepositoryOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isDataRepositoryOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isDataRepositoryOf .\n",
      "\n",
      "dlprov:isDataSplitCriteriaOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isDataSplitCriteriaOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isDataSplitCriteriaOf .\n",
      "\n",
      "dlprov:isDatasetOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isDatasetOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isDatasetOf .\n",
      "\n",
      "dlprov:isEvaluatedBy a owl:ObjectProperty ;\n",
      "    rdfs:label \"isEvaluatedBy\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isEvaluatedBy .\n",
      "\n",
      "dlprov:isGeneralizableBy a owl:ObjectProperty ;\n",
      "    rdfs:label \"isGeneralizableBy\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isGeneralizableBy .\n",
      "\n",
      "dlprov:isHandledBy a owl:ObjectProperty ;\n",
      "    rdfs:label \"isHandledBy\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isHandledBy .\n",
      "\n",
      "dlprov:isHyperparameterOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isHyperparameterOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isHyperparameterOf .\n",
      "\n",
      "dlprov:isModelOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isModelOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isModelOf .\n",
      "\n",
      "dlprov:isMonitoredBy a owl:ObjectProperty ;\n",
      "    rdfs:label \"isMonitoredBy\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isMonitoredBy .\n",
      "\n",
      "dlprov:isOptimizedBy a owl:ObjectProperty ;\n",
      "    rdfs:label \"isOptimizedBy\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isOptimizedBy .\n",
      "\n",
      "dlprov:isPostprocessedBy a owl:ObjectProperty ;\n",
      "    rdfs:label \"isPostprocessedBy\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isPostprocessedBy .\n",
      "\n",
      "dlprov:isPreprocessingStepOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isPreprocessingStepOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isPreprocessingStepOf .\n",
      "\n",
      "dlprov:isRawDataCollectionMethodOf a owl:ObjectProperty ;\n",
      "    rdfs:label \"isRawDataCollectionMethodOf\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isRawDataCollectionMethodOf .\n",
      "\n",
      "dlprov:isRegularizedBy a owl:ObjectProperty ;\n",
      "    rdfs:label \"isRegularizedBy\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isRegularizedBy .\n",
      "\n",
      "dlprov:isTrainedOn a owl:ObjectProperty ;\n",
      "    rdfs:label \"isTrainedOn\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isTrainedOn .\n",
      "\n",
      "dlprov:isTrainingCompleteBy a owl:ObjectProperty ;\n",
      "    rdfs:label \"isTrainingCompleteBy\"^^xsd:string ;\n",
      "    owl:inverseOf dlprov:isTrainingCompleteBy .\n",
      "\n",
      "\n",
      "Ontology for Gemini created and saved at ../Ontology/Gemini/Ontology/dlprov.ttl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from rdflib import Graph, Namespace, RDF, RDFS, OWL, Literal, URIRef, XSD\n",
    "\n",
    "models = ['GPT4', 'GPT3.5', 'Gemini']\n",
    "ontology_folder = \"../Ontology/\"\n",
    "concept_relations_folder = \"Concepts_relations\"\n",
    "\n",
    "# Define namespaces\n",
    "DLPROV = Namespace(\"https://w3id.org/dlprov#\")\n",
    "PROV = Namespace(\"http://www.w3.org/ns/prov#\")\n",
    "\n",
    "# Function to load concept relations from concept_relations.txt\n",
    "def load_concept_relations(file_path):\n",
    "    concepts = []\n",
    "    relationships = []\n",
    "    data_properties = []\n",
    "    inverse_properties = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "        current_section = None\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith('Concepts:'):\n",
    "                current_section = 'Concepts'\n",
    "                concepts.extend(line.split(':')[1].strip().split(', '))\n",
    "            elif line.startswith('Relationships:'):\n",
    "                current_section = 'Relationships'\n",
    "                relationships.extend(line.split(':')[1].strip().split(', '))\n",
    "            elif line.startswith('DataProperties:'):\n",
    "                current_section = 'DataProperties'\n",
    "                data_properties.extend(line.split(':')[1].strip().split(', '))\n",
    "            elif line.startswith('InverseProperties:'):\n",
    "                current_section = 'InverseProperties'\n",
    "                inverse_properties.extend(line.split(':')[1].strip().split(', '))\n",
    "            elif current_section:\n",
    "                # If the line doesn't start with one of the section headers, it belongs to the current section\n",
    "                if line:\n",
    "                    if current_section == 'Concepts':\n",
    "                        concepts.extend(line.split(', '))\n",
    "                    elif current_section == 'Relationships':\n",
    "                        relationships.extend(line.split(', '))\n",
    "                    elif current_section == 'DataProperties':\n",
    "                        data_properties.extend(line.split(', '))\n",
    "                    elif current_section == 'InverseProperties':\n",
    "                        inverse_properties.extend(line.split(', '))\n",
    "\n",
    "    return concepts, relationships, data_properties, inverse_properties\n",
    "\n",
    "\n",
    "# Function to create RDF triples for concepts, relationships, properties\n",
    "def create_ontology(model_name, concepts, relationships, data_properties, inverse_properties):\n",
    "    g = Graph()    \n",
    "    g.bind(\"dlprov\", DLPROV)\n",
    "    print(f'model_name:{model_name}')\n",
    "    print(f'concepts:{concepts}')\n",
    "    print(f'relationships:{relationships}')\n",
    "\n",
    "    # Define classes (concepts) as subclasses of DLPROV:Entity\n",
    "    for concept in concepts:\n",
    "        concept_uri = DLPROV[concept]\n",
    "        print(f'concept_uri:{concept_uri}')\n",
    "        g.add((concept_uri, RDF.type, OWL.Class))\n",
    "        g.add((concept_uri, RDFS.subClassOf, PROV.Entity))\n",
    "        g.add((concept_uri, RDFS.label, Literal(concept, datatype=XSD.string)))\n",
    "\n",
    "    # Define relationships (object properties)\n",
    "    for rel in relationships:\n",
    "        rel_uri = DLPROV[rel]\n",
    "        print(f'rel_uri:{rel_uri}')\n",
    "        g.add((rel_uri, RDF.type, OWL.ObjectProperty))\n",
    "        g.add((rel_uri, RDFS.label, Literal(rel, datatype=XSD.string)))\n",
    "\n",
    "    # Define data properties\n",
    "    for prop in data_properties:\n",
    "        prop_uri = DLPROV[prop]\n",
    "        print(f'prop_uri:{prop_uri}')\n",
    "        g.add((prop_uri, RDF.type, OWL.DatatypeProperty))\n",
    "        g.add((prop_uri, RDFS.label, Literal(prop, datatype=XSD.string)))\n",
    "\n",
    "    # Define inverse properties\n",
    "    for inv_prop in inverse_properties:\n",
    "        inv_prop_uri = DLPROV[inv_prop]\n",
    "        print(f'inv_prop_uri:{inv_prop_uri}')\n",
    "        g.add((inv_prop_uri, RDF.type, OWL.ObjectProperty))\n",
    "        g.add((inv_prop_uri, OWL.inverseOf, inv_prop_uri))\n",
    "        g.add((inv_prop_uri, RDFS.label, Literal(inv_prop, datatype=XSD.string)))\n",
    "\n",
    "    # # Define relationships between concepts and properties (for demonstration)\n",
    "    # # These should be replaced with actual relationships from your data\n",
    "    # for concept in concepts:\n",
    "    #     for rel in relationships:\n",
    "    #         g.add((DLPROV[concept], DLPROV[rel], DLPROV[concept + '_' + rel]))\n",
    "\n",
    "    # Save the ontology to a Turtle file\n",
    "    ontology_folder_path = os.path.join(ontology_folder, model_name, 'Ontology')\n",
    "    os.makedirs(ontology_folder_path, exist_ok=True)\n",
    "    ontology_file = os.path.join(ontology_folder_path, f'dlprov.ttl')\n",
    "    print(ontology_file)\n",
    "    g.serialize(destination=ontology_file, format='turtle')\n",
    "    print(g.serialize(format='turtle'))\n",
    "\n",
    "    print(f'Ontology for {model_name} created and saved at {ontology_file}')\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # List of models with their concept_relations.txt paths\n",
    "    for model in models:\n",
    "        model_folder = os.path.join(ontology_folder, model, concept_relations_folder)\n",
    "        if not os.path.isdir(model_folder):\n",
    "            print(f\"Warning: {concept_relations_folder} folder not found for {model}. Skipping.\")\n",
    "            continue     \n",
    "    \n",
    "    \n",
    "        concept_relations_file = os.path.join(model_folder, 'concept_relation_data_inverse.txt')\n",
    "        \n",
    "        concepts, relationships, data_properties, inverse_properties = load_concept_relations(concept_relations_file)\n",
    "        create_ontology(model, concepts, relationships, data_properties, inverse_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc685f8",
   "metadata": {},
   "source": [
    "## (4) CQ Answering\n",
    "\n",
    "* 모든 pdf 파일을 임베딩해서 벡터 DB에 넣어놓고, 각 질문별 답변을 문서별로 확인 -> 문서 전체를 하나의 벡터에 넣는게 아니라 문서별로 저장, 검색하는 방식인 것 같음\n",
    "* 논문에서는 Mixtral 모델로만 답변을 생성했다고 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6a5ce1",
   "metadata": {},
   "source": [
    "### 답변 생성(RAG_CQ_answering_with_LLMs.py)\n",
    "\n",
    "* 각 문서별로 28개의 질문을 해서 30*28개의 답변 문서를 생성하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60b9871a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unstructured[pdf] in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (0.17.2)\n",
      "Requirement already satisfied: chardet in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured[pdf]) (5.2.0)\n",
      "Requirement already satisfied: filetype in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured[pdf]) (1.2.0)\n",
      "Requirement already satisfied: python-magic in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured[pdf]) (0.4.27)\n",
      "Requirement already satisfied: lxml in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured[pdf]) (5.3.2)\n",
      "Requirement already satisfied: nltk in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured[pdf]) (3.9.1)\n",
      "Requirement already satisfied: requests in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured[pdf]) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured[pdf]) (4.13.4)\n",
      "Requirement already satisfied: emoji in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured[pdf]) (2.14.1)\n",
      "Requirement already satisfied: dataclasses-json in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured[pdf]) (0.6.7)\n",
      "Requirement already satisfied: python-iso639 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured[pdf]) (2025.2.18)\n",
      "Requirement already satisfied: langdetect in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured[pdf]) (1.0.9)\n",
      "Requirement already satisfied: numpy in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured[pdf]) (2.2.4)\n",
      "Requirement already satisfied: rapidfuzz in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured[pdf]) (3.13.0)\n",
      "Requirement already satisfied: backoff in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured[pdf]) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured[pdf]) (4.13.2)\n",
      "Requirement already satisfied: unstructured-client in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured[pdf]) (0.32.3)\n",
      "Requirement already satisfied: wrapt in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured[pdf]) (1.17.2)\n",
      "Requirement already satisfied: tqdm in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured[pdf]) (4.67.1)\n",
      "Requirement already satisfied: psutil in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured[pdf]) (7.0.0)\n",
      "Requirement already satisfied: python-oxmsg in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured[pdf]) (0.0.2)\n",
      "Requirement already satisfied: html5lib in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured[pdf]) (1.1)\n",
      "Requirement already satisfied: onnx>=1.17.0 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured[pdf]) (1.17.0)\n",
      "Collecting onnxruntime>=1.19.0 (from unstructured[pdf])\n",
      "  Using cached onnxruntime-1.21.0-cp312-cp312-macosx_13_0_universal2.whl.metadata (4.5 kB)\n",
      "Collecting pdf2image (from unstructured[pdf])\n",
      "  Using cached pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting pdfminer.six (from unstructured[pdf])\n",
      "  Downloading pdfminer_six-20250327-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting pikepdf (from unstructured[pdf])\n",
      "  Downloading pikepdf-9.7.0-cp312-cp312-macosx_14_0_arm64.whl.metadata (8.1 kB)\n",
      "Collecting pi-heif (from unstructured[pdf])\n",
      "  Downloading pi_heif-0.22.0-cp312-cp312-macosx_14_0_arm64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: pypdf in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured[pdf]) (5.4.0)\n",
      "Collecting google-cloud-vision (from unstructured[pdf])\n",
      "  Downloading google_cloud_vision-3.10.1-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting effdet (from unstructured[pdf])\n",
      "  Using cached effdet-0.4.1-py3-none-any.whl.metadata (33 kB)\n",
      "Collecting unstructured-inference>=0.8.10 (from unstructured[pdf])\n",
      "  Using cached unstructured_inference-0.8.10-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting unstructured.pytesseract>=0.3.12 (from unstructured[pdf])\n",
      "  Downloading unstructured.pytesseract-0.3.15-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from onnx>=1.17.0->unstructured[pdf]) (6.30.2)\n",
      "Requirement already satisfied: coloredlogs in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from onnxruntime>=1.19.0->unstructured[pdf]) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from onnxruntime>=1.19.0->unstructured[pdf]) (25.2.10)\n",
      "Requirement already satisfied: packaging in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from onnxruntime>=1.19.0->unstructured[pdf]) (24.2)\n",
      "Requirement already satisfied: sympy in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from onnxruntime>=1.19.0->unstructured[pdf]) (1.13.1)\n",
      "Requirement already satisfied: python-multipart in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured-inference>=0.8.10->unstructured[pdf]) (0.0.20)\n",
      "Requirement already satisfied: huggingface-hub in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured-inference>=0.8.10->unstructured[pdf]) (0.30.2)\n",
      "Collecting opencv-python!=4.7.0.68 (from unstructured-inference>=0.8.10->unstructured[pdf])\n",
      "  Using cached opencv_python-4.11.0.86-cp37-abi3-macosx_13_0_arm64.whl.metadata (20 kB)\n",
      "Collecting matplotlib (from unstructured-inference>=0.8.10->unstructured[pdf])\n",
      "  Using cached matplotlib-3.10.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: torch in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured-inference>=0.8.10->unstructured[pdf]) (2.6.0)\n",
      "Collecting timm (from unstructured-inference>=0.8.10->unstructured[pdf])\n",
      "  Using cached timm-1.0.15-py3-none-any.whl.metadata (52 kB)\n",
      "Requirement already satisfied: transformers>=4.25.1 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured-inference>=0.8.10->unstructured[pdf]) (4.51.3)\n",
      "Requirement already satisfied: pandas in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured-inference>=0.8.10->unstructured[pdf]) (2.2.3)\n",
      "Requirement already satisfied: scipy in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured-inference>=0.8.10->unstructured[pdf]) (1.15.2)\n",
      "Collecting pypdfium2 (from unstructured-inference>=0.8.10->unstructured[pdf])\n",
      "  Using cached pypdfium2-4.30.1-py3-none-macosx_11_0_arm64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured.pytesseract>=0.3.12->unstructured[pdf]) (11.2.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from beautifulsoup4->unstructured[pdf]) (2.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from dataclasses-json->unstructured[pdf]) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from dataclasses-json->unstructured[pdf]) (0.9.0)\n",
      "Requirement already satisfied: torchvision in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from effdet->unstructured[pdf]) (0.21.0)\n",
      "Collecting pycocotools>=2.0.2 (from effdet->unstructured[pdf])\n",
      "  Downloading pycocotools-2.0.8-cp312-cp312-macosx_10_9_universal2.whl.metadata (1.1 kB)\n",
      "Collecting omegaconf>=2.0 (from effdet->unstructured[pdf])\n",
      "  Using cached omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[pdf])\n",
      "  Downloading google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 (from google-cloud-vision->unstructured[pdf])\n",
      "  Downloading google_auth-2.39.0-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-cloud-vision->unstructured[pdf])\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: six>=1.9 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from html5lib->unstructured[pdf]) (1.17.0)\n",
      "Requirement already satisfied: webencodings in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from html5lib->unstructured[pdf]) (0.5.1)\n",
      "Requirement already satisfied: click in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from nltk->unstructured[pdf]) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from nltk->unstructured[pdf]) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from nltk->unstructured[pdf]) (2024.11.6)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from pdfminer.six->unstructured[pdf]) (3.4.1)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from pdfminer.six->unstructured[pdf]) (44.0.2)\n",
      "Collecting Deprecated (from pikepdf->unstructured[pdf])\n",
      "  Using cached Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: olefile in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from python-oxmsg->unstructured[pdf]) (0.47)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from requests->unstructured[pdf]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from requests->unstructured[pdf]) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from requests->unstructured[pdf]) (2025.1.31)\n",
      "Requirement already satisfied: aiofiles>=24.1.0 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured-client->unstructured[pdf]) (24.1.0)\n",
      "Requirement already satisfied: eval-type-backport>=0.2.0 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured-client->unstructured[pdf]) (0.2.2)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured-client->unstructured[pdf]) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured-client->unstructured[pdf]) (1.6.0)\n",
      "Requirement already satisfied: pydantic>=2.10.3 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured-client->unstructured[pdf]) (2.11.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured-client->unstructured[pdf]) (2.9.0.post0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured-client->unstructured[pdf]) (1.0.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from unstructured-client->unstructured[pdf]) (0.4.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[pdf]) (1.17.1)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[pdf])\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[pdf])\n",
      "  Downloading grpcio-1.72.0rc1-cp312-cp312-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[pdf])\n",
      "  Downloading grpcio_status-1.72.0rc1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[pdf]) (5.5.2)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[pdf])\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[pdf])\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: anyio in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from httpx>=0.27.0->unstructured-client->unstructured[pdf]) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from httpx>=0.27.0->unstructured-client->unstructured[pdf]) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured[pdf]) (0.14.0)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0->effdet->unstructured[pdf])\n",
      "  Using cached antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.1.0 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from omegaconf>=2.0->effdet->unstructured[pdf]) (6.0.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->unstructured-inference>=0.8.10->unstructured[pdf])\n",
      "  Downloading contourpy-1.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->unstructured-inference>=0.8.10->unstructured[pdf])\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->unstructured-inference>=0.8.10->unstructured[pdf])\n",
      "  Using cached fonttools-4.57.0-cp312-cp312-macosx_10_13_universal2.whl.metadata (102 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->unstructured-inference>=0.8.10->unstructured[pdf])\n",
      "  Using cached kiwisolver-1.4.8-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from matplotlib->unstructured-inference>=0.8.10->unstructured[pdf]) (3.2.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from pydantic>=2.10.3->unstructured-client->unstructured[pdf]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from pydantic>=2.10.3->unstructured-client->unstructured[pdf]) (2.33.1)\n",
      "Requirement already satisfied: safetensors in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from timm->unstructured-inference>=0.8.10->unstructured[pdf]) (0.5.3)\n",
      "Requirement already satisfied: filelock in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from torch->unstructured-inference>=0.8.10->unstructured[pdf]) (3.18.0)\n",
      "Requirement already satisfied: networkx in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from torch->unstructured-inference>=0.8.10->unstructured[pdf]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from torch->unstructured-inference>=0.8.10->unstructured[pdf]) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from torch->unstructured-inference>=0.8.10->unstructured[pdf]) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from torch->unstructured-inference>=0.8.10->unstructured[pdf]) (78.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from sympy->onnxruntime>=1.19.0->unstructured[pdf]) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from transformers>=4.25.1->unstructured-inference>=0.8.10->unstructured[pdf]) (0.21.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured[pdf]) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from coloredlogs->onnxruntime>=1.19.0->unstructured[pdf]) (10.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from pandas->unstructured-inference>=0.8.10->unstructured[pdf]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from pandas->unstructured-inference>=0.8.10->unstructured[pdf]) (2025.2)\n",
      "Requirement already satisfied: pycparser in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[pdf]) (2.22)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[pdf])\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured[pdf]) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jeongyunl/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages (from jinja2->torch->unstructured-inference>=0.8.10->unstructured[pdf]) (3.0.2)\n",
      "Using cached onnxruntime-1.21.0-cp312-cp312-macosx_13_0_universal2.whl (33.7 MB)\n",
      "Downloading unstructured_inference-0.8.10-py3-none-any.whl (48 kB)\n",
      "Downloading unstructured.pytesseract-0.3.15-py3-none-any.whl (14 kB)\n",
      "Using cached effdet-0.4.1-py3-none-any.whl (112 kB)\n",
      "Downloading google_cloud_vision-3.10.1-py3-none-any.whl (526 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.1/526.1 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Downloading pdfminer_six-20250327-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pi_heif-0.22.0-cp312-cp312-macosx_14_0_arm64.whl (559 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m559.8/559.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pikepdf-9.7.0-cp312-cp312-macosx_14_0_arm64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.24.2-py3-none-any.whl (160 kB)\n",
      "Downloading google_auth-2.39.0-py2.py3-none-any.whl (212 kB)\n",
      "Using cached omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Using cached opencv_python-4.11.0.86-cp37-abi3-macosx_13_0_arm64.whl (37.3 MB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading pycocotools-2.0.8-cp312-cp312-macosx_10_9_universal2.whl (162 kB)\n",
      "Using cached matplotlib-3.10.1-cp312-cp312-macosx_11_0_arm64.whl (8.0 MB)\n",
      "Using cached timm-1.0.15-py3-none-any.whl (2.4 MB)\n",
      "Using cached Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Using cached pypdfium2-4.30.1-py3-none-macosx_11_0_arm64.whl (2.8 MB)\n",
      "Downloading contourpy-1.3.2-cp312-cp312-macosx_11_0_arm64.whl (255 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.57.0-cp312-cp312-macosx_10_13_universal2.whl (2.8 MB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading grpcio-1.72.0rc1-cp312-cp312-macosx_11_0_universal2.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio_status-1.72.0rc1-py3-none-any.whl (14 kB)\n",
      "Using cached kiwisolver-1.4.8-cp312-cp312-macosx_11_0_arm64.whl (65 kB)\n",
      "Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Building wheels for collected packages: antlr4-python3-runtime\n",
      "  Building wheel for antlr4-python3-runtime (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144592 sha256=b62c2fb444583d26a8c6a560e0b716546558448e22e4a43132c3dd136a125339\n",
      "  Stored in directory: /Users/jeongyunl/Library/Caches/pip/wheels/1f/be/48/13754633f1d08d1fbfc60d5e80ae1e5d7329500477685286cd\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: antlr4-python3-runtime, unstructured.pytesseract, pypdfium2, pyasn1, proto-plus, pi-heif, pdf2image, opencv-python, omegaconf, kiwisolver, grpcio, googleapis-common-protos, fonttools, Deprecated, cycler, contourpy, rsa, pyasn1-modules, pikepdf, onnxruntime, matplotlib, grpcio-status, pycocotools, pdfminer.six, google-auth, timm, google-api-core, unstructured-inference, effdet, google-cloud-vision\n",
      "Successfully installed Deprecated-1.2.18 antlr4-python3-runtime-4.9.3 contourpy-1.3.2 cycler-0.12.1 effdet-0.4.1 fonttools-4.57.0 google-api-core-2.24.2 google-auth-2.39.0 google-cloud-vision-3.10.1 googleapis-common-protos-1.70.0 grpcio-1.72.0rc1 grpcio-status-1.72.0rc1 kiwisolver-1.4.8 matplotlib-3.10.1 omegaconf-2.3.0 onnxruntime-1.21.0 opencv-python-4.11.0.86 pdf2image-1.17.0 pdfminer.six-20250327 pi-heif-0.22.0 pikepdf-9.7.0 proto-plus-1.26.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 pycocotools-2.0.8 pypdfium2-4.30.1 rsa-4.9 timm-1.0.15 unstructured-inference-0.8.10 unstructured.pytesseract-0.3.15\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"unstructured[pdf]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0464afa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]CropBox missing from /Page, defaulting to MediaBox\n",
      "  7%|▋         | 2/30 [03:03<42:57, 92.07s/it]CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "  7%|▋         | 2/30 [03:51<54:06, 115.95s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRAG_CQ_ans/Original_not_processed/Publication\u001b[39m\u001b[38;5;132;01m{\u001b[39;00md\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_CQ\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.txt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     33\u001b[39m                 f.write(wrapped_text)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43mRAG_CQ_answering\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mRAG_CQ_answering\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cq,p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(CQs,np.arange(\u001b[32m1\u001b[39m,\u001b[38;5;28mlen\u001b[39m(CQs)+\u001b[32m1\u001b[39m)):\n\u001b[32m     29\u001b[39m     prompt = prompt_template.format(query=cq)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     result = \u001b[43mchain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     wrapped_text = textwrap.fill(result[\u001b[33m'\u001b[39m\u001b[33mresult\u001b[39m\u001b[33m'\u001b[39m], width=\u001b[32m100\u001b[39m)\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRAG_CQ_ans/Original_not_processed/Publication\u001b[39m\u001b[38;5;132;01m{\u001b[39;00md\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_CQ\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.txt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:191\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    190\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/langchain/chains/base.py:389\u001b[39m, in \u001b[36mChain.__call__\u001b[39m\u001b[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[32m    358\u001b[39m \n\u001b[32m    359\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    380\u001b[39m \u001b[33;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[32m    381\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    382\u001b[39m config = {\n\u001b[32m    383\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks,\n\u001b[32m    384\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: tags,\n\u001b[32m    385\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    386\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: run_name,\n\u001b[32m    387\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/langchain/chains/base.py:170\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    169\u001b[39m     run_manager.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    171\u001b[39m run_manager.on_chain_end(outputs)\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/langchain/chains/base.py:160\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    158\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    159\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    162\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    163\u001b[39m     )\n\u001b[32m    165\u001b[39m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    166\u001b[39m         inputs, outputs, return_only_outputs\n\u001b[32m    167\u001b[39m     )\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/langchain/chains/retrieval_qa/base.py:154\u001b[39m, in \u001b[36mBaseRetrievalQA._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    153\u001b[39m     docs = \u001b[38;5;28mself\u001b[39m._get_docs(question)  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcombine_documents_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_documents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_run_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_source_documents:\n\u001b[32m    159\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;28mself\u001b[39m.output_key: answer, \u001b[33m\"\u001b[39m\u001b[33msource_documents\u001b[39m\u001b[33m\"\u001b[39m: docs}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:191\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    190\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/langchain/chains/base.py:611\u001b[39m, in \u001b[36mChain.run\u001b[39m\u001b[34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[39m\n\u001b[32m    606\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(args[\u001b[32m0\u001b[39m], callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[32m    607\u001b[39m         _output_key\n\u001b[32m    608\u001b[39m     ]\n\u001b[32m    610\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m--> \u001b[39m\u001b[32m611\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[32m    612\u001b[39m         _output_key\n\u001b[32m    613\u001b[39m     ]\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    617\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    618\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m but none were provided.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    619\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:191\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    190\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/langchain/chains/base.py:389\u001b[39m, in \u001b[36mChain.__call__\u001b[39m\u001b[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[32m    358\u001b[39m \n\u001b[32m    359\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    380\u001b[39m \u001b[33;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[32m    381\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    382\u001b[39m config = {\n\u001b[32m    383\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks,\n\u001b[32m    384\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: tags,\n\u001b[32m    385\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    386\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: run_name,\n\u001b[32m    387\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/langchain/chains/base.py:170\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    169\u001b[39m     run_manager.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    171\u001b[39m run_manager.on_chain_end(outputs)\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/langchain/chains/base.py:160\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    158\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    159\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    162\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    163\u001b[39m     )\n\u001b[32m    165\u001b[39m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    166\u001b[39m         inputs, outputs, return_only_outputs\n\u001b[32m    167\u001b[39m     )\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/langchain/chains/combine_documents/base.py:138\u001b[39m, in \u001b[36mBaseCombineDocumentsChain._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[32m    137\u001b[39m other_keys = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items() \u001b[38;5;28;01mif\u001b[39;00m k != \u001b[38;5;28mself\u001b[39m.input_key}\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m output, extra_return_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_run_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mother_keys\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m extra_return_dict[\u001b[38;5;28mself\u001b[39m.output_key] = output\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/langchain/chains/combine_documents/stuff.py:259\u001b[39m, in \u001b[36mStuffDocumentsChain.combine_docs\u001b[39m\u001b[34m(self, docs, callbacks, **kwargs)\u001b[39m\n\u001b[32m    257\u001b[39m inputs = \u001b[38;5;28mself\u001b[39m._get_inputs(docs, **kwargs)\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Call predict on the LLM.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m, {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/langchain/chains/llm.py:318\u001b[39m, in \u001b[36mLLMChain.predict\u001b[39m\u001b[34m(self, callbacks, **kwargs)\u001b[39m\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks = \u001b[38;5;28;01mNone\u001b[39;00m, **kwargs: Any) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    304\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[32m    305\u001b[39m \n\u001b[32m    306\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    316\u001b[39m \u001b[33;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[32m    317\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m.output_key]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:191\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    190\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/langchain/chains/base.py:389\u001b[39m, in \u001b[36mChain.__call__\u001b[39m\u001b[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[32m    358\u001b[39m \n\u001b[32m    359\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    380\u001b[39m \u001b[33;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[32m    381\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    382\u001b[39m config = {\n\u001b[32m    383\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks,\n\u001b[32m    384\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: tags,\n\u001b[32m    385\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    386\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: run_name,\n\u001b[32m    387\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/langchain/chains/base.py:170\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    169\u001b[39m     run_manager.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    171\u001b[39m run_manager.on_chain_end(outputs)\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/langchain/chains/base.py:160\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    158\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    159\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    162\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    163\u001b[39m     )\n\u001b[32m    165\u001b[39m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    166\u001b[39m         inputs, outputs, return_only_outputs\n\u001b[32m    167\u001b[39m     )\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/langchain/chains/llm.py:126\u001b[39m, in \u001b[36mLLMChain._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call\u001b[39m(\n\u001b[32m    122\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    123\u001b[39m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m    124\u001b[39m     run_manager: Optional[CallbackManagerForChainRun] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    125\u001b[39m ) -> Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.create_outputs(response)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/langchain/chains/llm.py:138\u001b[39m, in \u001b[36mLLMChain.generate\u001b[39m\u001b[34m(self, input_list, run_manager)\u001b[39m\n\u001b[32m    136\u001b[39m callbacks = run_manager.get_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm, BaseLanguageModel):\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    145\u001b[39m     results = \u001b[38;5;28mself\u001b[39m.llm.bind(stop=stop, **\u001b[38;5;28mself\u001b[39m.llm_kwargs).batch(\n\u001b[32m    146\u001b[39m         cast(List, prompts), {\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks}\n\u001b[32m    147\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:937\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    930\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    934\u001b[39m     **kwargs: Any,\n\u001b[32m    935\u001b[39m ) -> LLMResult:\n\u001b[32m    936\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m937\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:759\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    756\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[32m    757\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    758\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    761\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    762\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    765\u001b[39m         )\n\u001b[32m    766\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    767\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1002\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1000\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1001\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1002\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1006\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:978\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    976\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n\u001b[32m    977\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m978\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, generation_info)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/openai/_utils/_utils.py:279\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:914\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    871\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    872\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    873\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    911\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    912\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    913\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/openai/_base_client.py:1247\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1233\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1234\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1235\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1242\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1243\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1244\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1245\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1246\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1247\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/openai/_base_client.py:920\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[39m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    918\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m920\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/openai/_base_client.py:960\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m    957\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, request.method, request.url)\n\u001b[32m    959\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    966\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/automatic-KG-creation-with-LLM/env/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.7_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1232\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1230\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1231\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1232\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.7_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1105\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# from helper_functions import load_llm, load_cqs, read_txt, get_embeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "import textwrap\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "# from LLM_loader import llm\n",
    "\n",
    "def RAG_CQ_answering():\n",
    "    # CQs = load_cqs(config.get('Paths', 'CQs_path'))\n",
    "    CQs = load_cqs('../CQs/CQs.txt')\n",
    "    # embedding_model_id = config.get('Models', 'embedding_model_id')\n",
    "    # template = read_txt(config.get('Paths', 'RAG_question_answering_prompt'))\n",
    "    template = read_txt('../RAG_CQ_ans/RAG_question_answering.txt')\n",
    "    prompt_template = PromptTemplate(input_variables=[\"query\"], template=template)\n",
    "\n",
    "    # for d in tqdm([1,3,5,7,8,9,10,12,13,14,16,18,19,20,24,25,27,28,33,34,37,38,39,41,42,43,44,45,46,47]):\n",
    "    for d in tqdm([1,3,5]):\n",
    "        # loader = UnstructuredFileLoader(f\"{config.get('Paths', 'pdfs_path')}{d}.pdf\")\n",
    "        loader = UnstructuredFileLoader(f\"../Data/Pdfs/{d}.pdf\")\n",
    "        documents = loader.load()\n",
    "        text_splitter=RecursiveCharacterTextSplitter(chunk_size=2500, chunk_overlap=100)\n",
    "        text_chunks=text_splitter.split_documents(documents)\n",
    "        vectorstore=FAISS.from_documents(text_chunks, embedding)\n",
    "        chain = RetrievalQA.from_chain_type(llm=llm, chain_type = \"stuff\",return_source_documents=False, retriever=vectorstore.as_retriever())                                     \n",
    "        for cq,p in zip(CQs,np.arange(1,len(CQs)+1)):\n",
    "            prompt = prompt_template.format(query=cq)\n",
    "            result = chain({\"query\": prompt}, return_only_outputs=True)\n",
    "            wrapped_text = textwrap.fill(result['result'], width=100)\n",
    "            with open(f\"RAG_CQ_ans/Original_not_processed/Publication{d}_CQ{p}.txt\", 'w') as f:\n",
    "                f.write(wrapped_text)\n",
    "                \n",
    "RAG_CQ_answering()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0b6796",
   "metadata": {},
   "source": [
    "### 후처리 (Process_CQ_ans.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b54089b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_question_and_query(text):\n",
    "    if \"Question:\" in text:\n",
    "        return text[:text.index(\"Question:\")]\n",
    "    elif \"Query:\" in text:\n",
    "        return text[:text.index(\"Query:\")]\n",
    "    elif \"References\" in text:\n",
    "        return text[:text.index(\"References\")]\n",
    "    elif \"Reference(s):\" in text:\n",
    "        return text[:text.index(\"Reference(s):\")]\n",
    "    elif \"Answer:\" in text:\n",
    "        return text[:text.index(\"Answer:\")]\n",
    "    elif \"%Query Query:\" in text:\n",
    "        return text[:text.index(\"%Query Query:\")]\n",
    "    elif \"%Explanation Explanation:\" in text:\n",
    "        return text[:text.index(\"%Explanation Explanation:\")]\n",
    "    elif \"%Context Context:\" in text:\n",
    "        return text[:text.index(\"%Context Context:\")]\n",
    "    elif \"%Context\" in text:\n",
    "        return text[:text.index(\"%Context\")]\n",
    "    elif \"%Explanation\" in text:\n",
    "        return text[:text.index(\"%Explanation\")]\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4608cb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeated_sentences(text):\n",
    "    sentences = text.split('.')\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip().replace('\\n', ' ')\n",
    "        sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "        if sentence.strip() not in seen:\n",
    "            seen.add(sentence.strip())\n",
    "            result.append(sentence.strip())\n",
    "    return '. '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cdf1c9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "# from helper_functions import remove_question_and_query, remove_repeated_sentences\n",
    "\n",
    "def process_cq_ans():\n",
    "    # input_folder = config.get('Paths', 'Ans_to_cq_input_folder')   \n",
    "    # output_folder = config.get('Paths', 'Ans_to_cq_output_folder') \n",
    "    input_folder = 'RAG_CQ_ans/Original_not_processed/'\n",
    "    output_folder = 'RAG_CQ_ans/V1_processed/'\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.startswith(\"Pub\") and filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(input_folder, filename), 'r') as file:\n",
    "                text = file.read()\n",
    "                text = text.replace('\\n', ' ')\n",
    "            text = remove_repeated_sentences(text)\n",
    "            text = remove_question_and_query(text)\n",
    "            with open(os.path.join(output_folder, filename), 'w') as file:\n",
    "                file.write(text)\n",
    "                \n",
    "process_cq_ans()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5faa8c2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d42e0b26",
   "metadata": {},
   "source": [
    "## (5) KG construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4c99a9",
   "metadata": {},
   "source": [
    "### NER(NER.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c7309633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%INSTRUCTIONS: \n",
      "Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. \n",
      "Use provided concepts to understand which named entities to extract from competency answers. \n",
      "If there are no entities found for a specific concept, please write \"Not mentioned\" instead of leaving empty parentheses.\n",
      "Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         \n",
      "Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)\n",
      "Below are the competency questions and answers: \n",
      "Q: What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?\n",
      "A: Based on the provided context, the deep learning pipeline primarily uses the following data formats: - **Audio**: High sample rate audio data, such as 16-bit stereo audio at 22,000 samples per second (22 kHz), is used. Audio data is often represented as 2D spectrograms for analysis and visualization. - **Image**: High-resolution color images, for example, 2048-by-1536 8-bit 4:2:2 color images, are collected and processed. - **Other Environmental Sensor Data**: Data such as temperature or air quality, which typically have much lower data rates, are also mentioned. There is no explicit mention of video or CSV formats in the context provided. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "         \n",
      "==============\n",
      "\n",
      "%INSTRUCTIONS: \n",
      "Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. \n",
      "Use provided concepts to understand which named entities to extract from competency answers. \n",
      "If there are no entities found for a specific concept, please write \"Not mentioned\" instead of leaving empty parentheses.\n",
      "Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         \n",
      "Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)\n",
      "Below are the competency questions and answers: \n",
      "Q: What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?\n",
      "A: Based on the provided context, the deep learning pipeline primarily uses the following data formats: - **Audio**: High sample rate audio data, such as 16-bit stereo audio at 22,000 samples per second (22 kHz), is used. Audio data is often represented as 2D spectrograms for analysis and visualization. - **Image**: High-resolution color images, for example, 2048-by-1536 8-bit 4:2:2 color images, are collected and processed. - **Other Environmental Sensor Data**: Data such as temperature or air quality, which typically have much lower data rates, are also mentioned. There is no explicit mention of video or CSV formats in the context provided. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? \n",
      "A: The context does not specify the use of standard datasets like MNIST, CIFAR, or ImageNet in the deep learning pipeline for biodiversity monitoring. Instead, it describes the use of large amounts of labeled data produced by analysts using their own user interface, including audio spectrogram and image data collected from sensor networks for classifying species signals and event types. These datasets are specific to the biodiversity monitoring projects and are not standard public datasets like MNIST, CIFAR, or ImageNet. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "         \n",
      "==============\n",
      "\n",
      "%INSTRUCTIONS: \n",
      "Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. \n",
      "Use provided concepts to understand which named entities to extract from competency answers. \n",
      "If there are no entities found for a specific concept, please write \"Not mentioned\" instead of leaving empty parentheses.\n",
      "Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         \n",
      "Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)\n",
      "Below are the competency questions and answers: \n",
      "Q: What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?\n",
      "A: Based on the provided context, the deep learning pipeline primarily uses the following data formats: - **Audio**: High sample rate audio data, such as 16-bit stereo audio at 22,000 samples per second (22 kHz), is used. Audio data is often represented as 2D spectrograms for analysis and visualization. - **Image**: High-resolution color images, for example, 2048-by-1536 8-bit 4:2:2 color images, are collected and processed. - **Other Environmental Sensor Data**: Data such as temperature or air quality, which typically have much lower data rates, are also mentioned. There is no explicit mention of video or CSV formats in the context provided. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? \n",
      "A: The context does not specify the use of standard datasets like MNIST, CIFAR, or ImageNet in the deep learning pipeline for biodiversity monitoring. Instead, it describes the use of large amounts of labeled data produced by analysts using their own user interface, including audio spectrogram and image data collected from sensor networks for classifying species signals and event types. These datasets are specific to the biodiversity monitoring projects and are not standard public datasets like MNIST, CIFAR, or ImageNet. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?\n",
      "A: The provided context does not give a detailed, step-by-step list of specific preprocessing steps such as normalization, scaling, or cleaning before training a deep learning model. However, it does mention that analysts use tools for data exploration to sort and filter data according to various attributes (e. g. , date ranges, time of day, site location, frequency ranges for audio, object size and color for images). This suggests that some level of data organization and selection is performed before model training. Additionally, labeled datasets are created to train and refine deep learning models, which implies that data labeling and possibly cleaning (to ensure correct labels) are part of the process. In summary, while the context implies that data exploration, filtering, and labeling are important preprocessing steps, it does not explicitly mention standard preprocessing techniques like normalization or scaling. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "         \n",
      "==============\n",
      "\n",
      "%INSTRUCTIONS: \n",
      "Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. \n",
      "Use provided concepts to understand which named entities to extract from competency answers. \n",
      "If there are no entities found for a specific concept, please write \"Not mentioned\" instead of leaving empty parentheses.\n",
      "Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         \n",
      "Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)\n",
      "Below are the competency questions and answers: \n",
      "Q: What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?\n",
      "A: Based on the provided context, the deep learning pipeline primarily uses the following data formats: - **Audio**: High sample rate audio data, such as 16-bit stereo audio at 22,000 samples per second (22 kHz), is used. Audio data is often represented as 2D spectrograms for analysis and visualization. - **Image**: High-resolution color images, for example, 2048-by-1536 8-bit 4:2:2 color images, are collected and processed. - **Other Environmental Sensor Data**: Data such as temperature or air quality, which typically have much lower data rates, are also mentioned. There is no explicit mention of video or CSV formats in the context provided. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? \n",
      "A: The context does not specify the use of standard datasets like MNIST, CIFAR, or ImageNet in the deep learning pipeline for biodiversity monitoring. Instead, it describes the use of large amounts of labeled data produced by analysts using their own user interface, including audio spectrogram and image data collected from sensor networks for classifying species signals and event types. These datasets are specific to the biodiversity monitoring projects and are not standard public datasets like MNIST, CIFAR, or ImageNet. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?\n",
      "A: The provided context does not give a detailed, step-by-step list of specific preprocessing steps such as normalization, scaling, or cleaning before training a deep learning model. However, it does mention that analysts use tools for data exploration to sort and filter data according to various attributes (e. g. , date ranges, time of day, site location, frequency ranges for audio, object size and color for images). This suggests that some level of data organization and selection is performed before model training. Additionally, labeled datasets are created to train and refine deep learning models, which implies that data labeling and possibly cleaning (to ensure correct labels) are part of the process. In summary, while the context implies that data exploration, filtering, and labeling are important preprocessing steps, it does not explicitly mention standard preprocessing techniques like normalization or scaling. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the data augmentation techniques applied in the deep learning pipeline (e.g., Flipping, Rotating, Scaling)?\n",
      "A: The provided context does not specify which data augmentation techniques (such as flipping, rotating, or scaling) are applied in the deep learning pipeline. It describes the use of deep learning models and data exploration tools but does not mention specific data augmentation methods. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "         \n",
      "==============\n",
      "\n",
      "%INSTRUCTIONS: \n",
      "Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. \n",
      "Use provided concepts to understand which named entities to extract from competency answers. \n",
      "If there are no entities found for a specific concept, please write \"Not mentioned\" instead of leaving empty parentheses.\n",
      "Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         \n",
      "Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)\n",
      "Below are the competency questions and answers: \n",
      "Q: What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?\n",
      "A: Based on the provided context, the deep learning pipeline primarily uses the following data formats: - **Audio**: High sample rate audio data, such as 16-bit stereo audio at 22,000 samples per second (22 kHz), is used. Audio data is often represented as 2D spectrograms for analysis and visualization. - **Image**: High-resolution color images, for example, 2048-by-1536 8-bit 4:2:2 color images, are collected and processed. - **Other Environmental Sensor Data**: Data such as temperature or air quality, which typically have much lower data rates, are also mentioned. There is no explicit mention of video or CSV formats in the context provided. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? \n",
      "A: The context does not specify the use of standard datasets like MNIST, CIFAR, or ImageNet in the deep learning pipeline for biodiversity monitoring. Instead, it describes the use of large amounts of labeled data produced by analysts using their own user interface, including audio spectrogram and image data collected from sensor networks for classifying species signals and event types. These datasets are specific to the biodiversity monitoring projects and are not standard public datasets like MNIST, CIFAR, or ImageNet. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?\n",
      "A: The provided context does not give a detailed, step-by-step list of specific preprocessing steps such as normalization, scaling, or cleaning before training a deep learning model. However, it does mention that analysts use tools for data exploration to sort and filter data according to various attributes (e. g. , date ranges, time of day, site location, frequency ranges for audio, object size and color for images). This suggests that some level of data organization and selection is performed before model training. Additionally, labeled datasets are created to train and refine deep learning models, which implies that data labeling and possibly cleaning (to ensure correct labels) are part of the process. In summary, while the context implies that data exploration, filtering, and labeling are important preprocessing steps, it does not explicitly mention standard preprocessing techniques like normalization or scaling. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the data augmentation techniques applied in the deep learning pipeline (e.g., Flipping, Rotating, Scaling)?\n",
      "A: The provided context does not specify which data augmentation techniques (such as flipping, rotating, or scaling) are applied in the deep learning pipeline. It describes the use of deep learning models and data exploration tools but does not mention specific data augmentation methods. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?\n",
      "A: The pipeline employs deep convolutional neural networks (CNNs) and deep feed forward neural networks (DNNs) for processing audio spectrogram and image data. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "         \n",
      "==============\n",
      "\n",
      "%INSTRUCTIONS: \n",
      "Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. \n",
      "Use provided concepts to understand which named entities to extract from competency answers. \n",
      "If there are no entities found for a specific concept, please write \"Not mentioned\" instead of leaving empty parentheses.\n",
      "Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         \n",
      "Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)\n",
      "Below are the competency questions and answers: \n",
      "Q: What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?\n",
      "A: Based on the provided context, the deep learning pipeline primarily uses the following data formats: - **Audio**: High sample rate audio data, such as 16-bit stereo audio at 22,000 samples per second (22 kHz), is used. Audio data is often represented as 2D spectrograms for analysis and visualization. - **Image**: High-resolution color images, for example, 2048-by-1536 8-bit 4:2:2 color images, are collected and processed. - **Other Environmental Sensor Data**: Data such as temperature or air quality, which typically have much lower data rates, are also mentioned. There is no explicit mention of video or CSV formats in the context provided. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? \n",
      "A: The context does not specify the use of standard datasets like MNIST, CIFAR, or ImageNet in the deep learning pipeline for biodiversity monitoring. Instead, it describes the use of large amounts of labeled data produced by analysts using their own user interface, including audio spectrogram and image data collected from sensor networks for classifying species signals and event types. These datasets are specific to the biodiversity monitoring projects and are not standard public datasets like MNIST, CIFAR, or ImageNet. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?\n",
      "A: The provided context does not give a detailed, step-by-step list of specific preprocessing steps such as normalization, scaling, or cleaning before training a deep learning model. However, it does mention that analysts use tools for data exploration to sort and filter data according to various attributes (e. g. , date ranges, time of day, site location, frequency ranges for audio, object size and color for images). This suggests that some level of data organization and selection is performed before model training. Additionally, labeled datasets are created to train and refine deep learning models, which implies that data labeling and possibly cleaning (to ensure correct labels) are part of the process. In summary, while the context implies that data exploration, filtering, and labeling are important preprocessing steps, it does not explicitly mention standard preprocessing techniques like normalization or scaling. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the data augmentation techniques applied in the deep learning pipeline (e.g., Flipping, Rotating, Scaling)?\n",
      "A: The provided context does not specify which data augmentation techniques (such as flipping, rotating, or scaling) are applied in the deep learning pipeline. It describes the use of deep learning models and data exploration tools but does not mention specific data augmentation methods. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?\n",
      "A: The pipeline employs deep convolutional neural networks (CNNs) and deep feed forward neural networks (DNNs) for processing audio spectrogram and image data. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?\n",
      "A: The provided context mentions that deep learning models are \"completely trainable from data, usually in a supervised fashion, such that the learning algorithm (usually stochastic gradient descent), is told what the desired output is for each input data sample. \" Therefore, stochastic gradient descent (SGD) is specifically referenced as an optimization technique applied in the deep learning pipeline. Other optimization techniques such as Adam are not mentioned in the provided context. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "         \n",
      "==============\n",
      "\n",
      "%INSTRUCTIONS: \n",
      "Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. \n",
      "Use provided concepts to understand which named entities to extract from competency answers. \n",
      "If there are no entities found for a specific concept, please write \"Not mentioned\" instead of leaving empty parentheses.\n",
      "Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         \n",
      "Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)\n",
      "Below are the competency questions and answers: \n",
      "Q: What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?\n",
      "A: Based on the provided context, the deep learning pipeline primarily uses the following data formats: - **Audio**: High sample rate audio data, such as 16-bit stereo audio at 22,000 samples per second (22 kHz), is used. Audio data is often represented as 2D spectrograms for analysis and visualization. - **Image**: High-resolution color images, for example, 2048-by-1536 8-bit 4:2:2 color images, are collected and processed. - **Other Environmental Sensor Data**: Data such as temperature or air quality, which typically have much lower data rates, are also mentioned. There is no explicit mention of video or CSV formats in the context provided. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? \n",
      "A: The context does not specify the use of standard datasets like MNIST, CIFAR, or ImageNet in the deep learning pipeline for biodiversity monitoring. Instead, it describes the use of large amounts of labeled data produced by analysts using their own user interface, including audio spectrogram and image data collected from sensor networks for classifying species signals and event types. These datasets are specific to the biodiversity monitoring projects and are not standard public datasets like MNIST, CIFAR, or ImageNet. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?\n",
      "A: The provided context does not give a detailed, step-by-step list of specific preprocessing steps such as normalization, scaling, or cleaning before training a deep learning model. However, it does mention that analysts use tools for data exploration to sort and filter data according to various attributes (e. g. , date ranges, time of day, site location, frequency ranges for audio, object size and color for images). This suggests that some level of data organization and selection is performed before model training. Additionally, labeled datasets are created to train and refine deep learning models, which implies that data labeling and possibly cleaning (to ensure correct labels) are part of the process. In summary, while the context implies that data exploration, filtering, and labeling are important preprocessing steps, it does not explicitly mention standard preprocessing techniques like normalization or scaling. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the data augmentation techniques applied in the deep learning pipeline (e.g., Flipping, Rotating, Scaling)?\n",
      "A: The provided context does not specify which data augmentation techniques (such as flipping, rotating, or scaling) are applied in the deep learning pipeline. It describes the use of deep learning models and data exploration tools but does not mention specific data augmentation methods. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?\n",
      "A: The pipeline employs deep convolutional neural networks (CNNs) and deep feed forward neural networks (DNNs) for processing audio spectrogram and image data. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?\n",
      "A: The provided context mentions that deep learning models are \"completely trainable from data, usually in a supervised fashion, such that the learning algorithm (usually stochastic gradient descent), is told what the desired output is for each input data sample. \" Therefore, stochastic gradient descent (SGD) is specifically referenced as an optimization technique applied in the deep learning pipeline. Other optimization techniques such as Adam are not mentioned in the provided context. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?\n",
      "A: The provided context does not specify the exact hyperparameters (such as learning rate or optimizer) used in the deep learning models for biodiversity monitoring. It describes the use of deep convolutional neural networks (CNNs) and deep feed forward neural networks (DNNs), but does not detail the specific hyperparameter settings. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "         \n",
      "==============\n",
      "\n",
      "%INSTRUCTIONS: \n",
      "Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. \n",
      "Use provided concepts to understand which named entities to extract from competency answers. \n",
      "If there are no entities found for a specific concept, please write \"Not mentioned\" instead of leaving empty parentheses.\n",
      "Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         \n",
      "Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)\n",
      "Below are the competency questions and answers: \n",
      "Q: What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?\n",
      "A: Based on the provided context, the deep learning pipeline primarily uses the following data formats: - **Audio**: High sample rate audio data, such as 16-bit stereo audio at 22,000 samples per second (22 kHz), is used. Audio data is often represented as 2D spectrograms for analysis and visualization. - **Image**: High-resolution color images, for example, 2048-by-1536 8-bit 4:2:2 color images, are collected and processed. - **Other Environmental Sensor Data**: Data such as temperature or air quality, which typically have much lower data rates, are also mentioned. There is no explicit mention of video or CSV formats in the context provided. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? \n",
      "A: The context does not specify the use of standard datasets like MNIST, CIFAR, or ImageNet in the deep learning pipeline for biodiversity monitoring. Instead, it describes the use of large amounts of labeled data produced by analysts using their own user interface, including audio spectrogram and image data collected from sensor networks for classifying species signals and event types. These datasets are specific to the biodiversity monitoring projects and are not standard public datasets like MNIST, CIFAR, or ImageNet. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?\n",
      "A: The provided context does not give a detailed, step-by-step list of specific preprocessing steps such as normalization, scaling, or cleaning before training a deep learning model. However, it does mention that analysts use tools for data exploration to sort and filter data according to various attributes (e. g. , date ranges, time of day, site location, frequency ranges for audio, object size and color for images). This suggests that some level of data organization and selection is performed before model training. Additionally, labeled datasets are created to train and refine deep learning models, which implies that data labeling and possibly cleaning (to ensure correct labels) are part of the process. In summary, while the context implies that data exploration, filtering, and labeling are important preprocessing steps, it does not explicitly mention standard preprocessing techniques like normalization or scaling. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the data augmentation techniques applied in the deep learning pipeline (e.g., Flipping, Rotating, Scaling)?\n",
      "A: The provided context does not specify which data augmentation techniques (such as flipping, rotating, or scaling) are applied in the deep learning pipeline. It describes the use of deep learning models and data exploration tools but does not mention specific data augmentation methods. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?\n",
      "A: The pipeline employs deep convolutional neural networks (CNNs) and deep feed forward neural networks (DNNs) for processing audio spectrogram and image data. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?\n",
      "A: The provided context mentions that deep learning models are \"completely trainable from data, usually in a supervised fashion, such that the learning algorithm (usually stochastic gradient descent), is told what the desired output is for each input data sample. \" Therefore, stochastic gradient descent (SGD) is specifically referenced as an optimization technique applied in the deep learning pipeline. Other optimization techniques such as Adam are not mentioned in the provided context. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?\n",
      "A: The provided context does not specify the exact hyperparameters (such as learning rate or optimizer) used in the deep learning models for biodiversity monitoring. It describes the use of deep convolutional neural networks (CNNs) and deep feed forward neural networks (DNNs), but does not detail the specific hyperparameter settings. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?\n",
      "A: The provided context does not specify which metrics (such as accuracy, precision, or recall) are used to evaluate the performance of the deep learning model. It describes the use of deep learning for classification and detection, and mentions reductions in manual data review, but does not detail specific evaluation metrics. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "         \n",
      "==============\n",
      "\n",
      "%INSTRUCTIONS: \n",
      "Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. \n",
      "Use provided concepts to understand which named entities to extract from competency answers. \n",
      "If there are no entities found for a specific concept, please write \"Not mentioned\" instead of leaving empty parentheses.\n",
      "Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         \n",
      "Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)\n",
      "Below are the competency questions and answers: \n",
      "Q: What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?\n",
      "A: Based on the provided context, the deep learning pipeline primarily uses the following data formats: - **Audio**: High sample rate audio data, such as 16-bit stereo audio at 22,000 samples per second (22 kHz), is used. Audio data is often represented as 2D spectrograms for analysis and visualization. - **Image**: High-resolution color images, for example, 2048-by-1536 8-bit 4:2:2 color images, are collected and processed. - **Other Environmental Sensor Data**: Data such as temperature or air quality, which typically have much lower data rates, are also mentioned. There is no explicit mention of video or CSV formats in the context provided. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? \n",
      "A: The context does not specify the use of standard datasets like MNIST, CIFAR, or ImageNet in the deep learning pipeline for biodiversity monitoring. Instead, it describes the use of large amounts of labeled data produced by analysts using their own user interface, including audio spectrogram and image data collected from sensor networks for classifying species signals and event types. These datasets are specific to the biodiversity monitoring projects and are not standard public datasets like MNIST, CIFAR, or ImageNet. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?\n",
      "A: The provided context does not give a detailed, step-by-step list of specific preprocessing steps such as normalization, scaling, or cleaning before training a deep learning model. However, it does mention that analysts use tools for data exploration to sort and filter data according to various attributes (e. g. , date ranges, time of day, site location, frequency ranges for audio, object size and color for images). This suggests that some level of data organization and selection is performed before model training. Additionally, labeled datasets are created to train and refine deep learning models, which implies that data labeling and possibly cleaning (to ensure correct labels) are part of the process. In summary, while the context implies that data exploration, filtering, and labeling are important preprocessing steps, it does not explicitly mention standard preprocessing techniques like normalization or scaling. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the data augmentation techniques applied in the deep learning pipeline (e.g., Flipping, Rotating, Scaling)?\n",
      "A: The provided context does not specify which data augmentation techniques (such as flipping, rotating, or scaling) are applied in the deep learning pipeline. It describes the use of deep learning models and data exploration tools but does not mention specific data augmentation methods. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?\n",
      "A: The pipeline employs deep convolutional neural networks (CNNs) and deep feed forward neural networks (DNNs) for processing audio spectrogram and image data. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?\n",
      "A: The provided context mentions that deep learning models are \"completely trainable from data, usually in a supervised fashion, such that the learning algorithm (usually stochastic gradient descent), is told what the desired output is for each input data sample. \" Therefore, stochastic gradient descent (SGD) is specifically referenced as an optimization technique applied in the deep learning pipeline. Other optimization techniques such as Adam are not mentioned in the provided context. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?\n",
      "A: The provided context does not specify the exact hyperparameters (such as learning rate or optimizer) used in the deep learning models for biodiversity monitoring. It describes the use of deep convolutional neural networks (CNNs) and deep feed forward neural networks (DNNs), but does not detail the specific hyperparameter settings. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?\n",
      "A: The provided context does not specify which metrics (such as accuracy, precision, or recall) are used to evaluate the performance of the deep learning model. It describes the use of deep learning for classification and detection, and mentions reductions in manual data review, but does not detail specific evaluation metrics. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?\n",
      "A: The provided context does not specify which regularization methods (such as dropout or L2 regularization) are used to prevent overfitting in the described deep learning pipeline. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "         \n",
      "==============\n",
      "\n",
      "%INSTRUCTIONS: \n",
      "Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. \n",
      "Use provided concepts to understand which named entities to extract from competency answers. \n",
      "If there are no entities found for a specific concept, please write \"Not mentioned\" instead of leaving empty parentheses.\n",
      "Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         \n",
      "Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)\n",
      "Below are the competency questions and answers: \n",
      "Q: What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?\n",
      "A: Based on the provided context, the deep learning pipeline primarily uses the following data formats: - **Audio**: High sample rate audio data, such as 16-bit stereo audio at 22,000 samples per second (22 kHz), is used. Audio data is often represented as 2D spectrograms for analysis and visualization. - **Image**: High-resolution color images, for example, 2048-by-1536 8-bit 4:2:2 color images, are collected and processed. - **Other Environmental Sensor Data**: Data such as temperature or air quality, which typically have much lower data rates, are also mentioned. There is no explicit mention of video or CSV formats in the context provided. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? \n",
      "A: The context does not specify the use of standard datasets like MNIST, CIFAR, or ImageNet in the deep learning pipeline for biodiversity monitoring. Instead, it describes the use of large amounts of labeled data produced by analysts using their own user interface, including audio spectrogram and image data collected from sensor networks for classifying species signals and event types. These datasets are specific to the biodiversity monitoring projects and are not standard public datasets like MNIST, CIFAR, or ImageNet. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?\n",
      "A: The provided context does not give a detailed, step-by-step list of specific preprocessing steps such as normalization, scaling, or cleaning before training a deep learning model. However, it does mention that analysts use tools for data exploration to sort and filter data according to various attributes (e. g. , date ranges, time of day, site location, frequency ranges for audio, object size and color for images). This suggests that some level of data organization and selection is performed before model training. Additionally, labeled datasets are created to train and refine deep learning models, which implies that data labeling and possibly cleaning (to ensure correct labels) are part of the process. In summary, while the context implies that data exploration, filtering, and labeling are important preprocessing steps, it does not explicitly mention standard preprocessing techniques like normalization or scaling. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the data augmentation techniques applied in the deep learning pipeline (e.g., Flipping, Rotating, Scaling)?\n",
      "A: The provided context does not specify which data augmentation techniques (such as flipping, rotating, or scaling) are applied in the deep learning pipeline. It describes the use of deep learning models and data exploration tools but does not mention specific data augmentation methods. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?\n",
      "A: The pipeline employs deep convolutional neural networks (CNNs) and deep feed forward neural networks (DNNs) for processing audio spectrogram and image data. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?\n",
      "A: The provided context mentions that deep learning models are \"completely trainable from data, usually in a supervised fashion, such that the learning algorithm (usually stochastic gradient descent), is told what the desired output is for each input data sample. \" Therefore, stochastic gradient descent (SGD) is specifically referenced as an optimization technique applied in the deep learning pipeline. Other optimization techniques such as Adam are not mentioned in the provided context. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?\n",
      "A: The provided context does not specify the exact hyperparameters (such as learning rate or optimizer) used in the deep learning models for biodiversity monitoring. It describes the use of deep convolutional neural networks (CNNs) and deep feed forward neural networks (DNNs), but does not detail the specific hyperparameter settings. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?\n",
      "A: The provided context does not specify which metrics (such as accuracy, precision, or recall) are used to evaluate the performance of the deep learning model. It describes the use of deep learning for classification and detection, and mentions reductions in manual data review, but does not detail specific evaluation metrics. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?\n",
      "A: The provided context does not specify which regularization methods (such as dropout or L2 regularization) are used to prevent overfitting in the described deep learning pipeline. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: Which frameworks are used to build the deep learning model (e.g., TensorFlow, PyTorch)?\n",
      "A: The provided context does not specify which frameworks (such as TensorFlow or PyTorch) are used to build the deep learning models. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "         \n",
      "==============\n",
      "\n",
      "%INSTRUCTIONS: \n",
      "Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. \n",
      "Use provided concepts to understand which named entities to extract from competency answers. \n",
      "If there are no entities found for a specific concept, please write \"Not mentioned\" instead of leaving empty parentheses.\n",
      "Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         \n",
      "Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)\n",
      "Below are the competency questions and answers: \n",
      "Q: What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?\n",
      "A: Based on the provided context, the deep learning pipeline primarily uses the following data formats: - **Audio**: High sample rate audio data, such as 16-bit stereo audio at 22,000 samples per second (22 kHz), is used. Audio data is often represented as 2D spectrograms for analysis and visualization. - **Image**: High-resolution color images, for example, 2048-by-1536 8-bit 4:2:2 color images, are collected and processed. - **Other Environmental Sensor Data**: Data such as temperature or air quality, which typically have much lower data rates, are also mentioned. There is no explicit mention of video or CSV formats in the context provided. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? \n",
      "A: The context does not specify the use of standard datasets like MNIST, CIFAR, or ImageNet in the deep learning pipeline for biodiversity monitoring. Instead, it describes the use of large amounts of labeled data produced by analysts using their own user interface, including audio spectrogram and image data collected from sensor networks for classifying species signals and event types. These datasets are specific to the biodiversity monitoring projects and are not standard public datasets like MNIST, CIFAR, or ImageNet. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?\n",
      "A: The provided context does not give a detailed, step-by-step list of specific preprocessing steps such as normalization, scaling, or cleaning before training a deep learning model. However, it does mention that analysts use tools for data exploration to sort and filter data according to various attributes (e. g. , date ranges, time of day, site location, frequency ranges for audio, object size and color for images). This suggests that some level of data organization and selection is performed before model training. Additionally, labeled datasets are created to train and refine deep learning models, which implies that data labeling and possibly cleaning (to ensure correct labels) are part of the process. In summary, while the context implies that data exploration, filtering, and labeling are important preprocessing steps, it does not explicitly mention standard preprocessing techniques like normalization or scaling. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the data augmentation techniques applied in the deep learning pipeline (e.g., Flipping, Rotating, Scaling)?\n",
      "A: The provided context does not specify which data augmentation techniques (such as flipping, rotating, or scaling) are applied in the deep learning pipeline. It describes the use of deep learning models and data exploration tools but does not mention specific data augmentation methods. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?\n",
      "A: The pipeline employs deep convolutional neural networks (CNNs) and deep feed forward neural networks (DNNs) for processing audio spectrogram and image data. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?\n",
      "A: The provided context mentions that deep learning models are \"completely trainable from data, usually in a supervised fashion, such that the learning algorithm (usually stochastic gradient descent), is told what the desired output is for each input data sample. \" Therefore, stochastic gradient descent (SGD) is specifically referenced as an optimization technique applied in the deep learning pipeline. Other optimization techniques such as Adam are not mentioned in the provided context. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?\n",
      "A: The provided context does not specify the exact hyperparameters (such as learning rate or optimizer) used in the deep learning models for biodiversity monitoring. It describes the use of deep convolutional neural networks (CNNs) and deep feed forward neural networks (DNNs), but does not detail the specific hyperparameter settings. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?\n",
      "A: The provided context does not specify which metrics (such as accuracy, precision, or recall) are used to evaluate the performance of the deep learning model. It describes the use of deep learning for classification and detection, and mentions reductions in manual data review, but does not detail specific evaluation metrics. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?\n",
      "A: The provided context does not specify which regularization methods (such as dropout or L2 regularization) are used to prevent overfitting in the described deep learning pipeline. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: Which frameworks are used to build the deep learning model (e.g., TensorFlow, PyTorch)?\n",
      "A: The provided context does not specify which frameworks (such as TensorFlow or PyTorch) are used to build the deep learning models. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?\n",
      "A: The provided context does not specify which hardware resources (e. g. , GPUs, TPUs) are used for training the deep learning model. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "         \n",
      "==============\n",
      "\n",
      "%INSTRUCTIONS: \n",
      "Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. \n",
      "Use provided concepts to understand which named entities to extract from competency answers. \n",
      "If there are no entities found for a specific concept, please write \"Not mentioned\" instead of leaving empty parentheses.\n",
      "Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         \n",
      "Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)\n",
      "Below are the competency questions and answers: \n",
      "Q: What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?\n",
      "A: Based on the provided context, the deep learning pipeline primarily uses the following data formats: - **Audio**: High sample rate audio data, such as 16-bit stereo audio at 22,000 samples per second (22 kHz), is used. Audio data is often represented as 2D spectrograms for analysis and visualization. - **Image**: High-resolution color images, for example, 2048-by-1536 8-bit 4:2:2 color images, are collected and processed. - **Other Environmental Sensor Data**: Data such as temperature or air quality, which typically have much lower data rates, are also mentioned. There is no explicit mention of video or CSV formats in the context provided. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? \n",
      "A: The context does not specify the use of standard datasets like MNIST, CIFAR, or ImageNet in the deep learning pipeline for biodiversity monitoring. Instead, it describes the use of large amounts of labeled data produced by analysts using their own user interface, including audio spectrogram and image data collected from sensor networks for classifying species signals and event types. These datasets are specific to the biodiversity monitoring projects and are not standard public datasets like MNIST, CIFAR, or ImageNet. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?\n",
      "A: The provided context does not give a detailed, step-by-step list of specific preprocessing steps such as normalization, scaling, or cleaning before training a deep learning model. However, it does mention that analysts use tools for data exploration to sort and filter data according to various attributes (e. g. , date ranges, time of day, site location, frequency ranges for audio, object size and color for images). This suggests that some level of data organization and selection is performed before model training. Additionally, labeled datasets are created to train and refine deep learning models, which implies that data labeling and possibly cleaning (to ensure correct labels) are part of the process. In summary, while the context implies that data exploration, filtering, and labeling are important preprocessing steps, it does not explicitly mention standard preprocessing techniques like normalization or scaling. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the data augmentation techniques applied in the deep learning pipeline (e.g., Flipping, Rotating, Scaling)?\n",
      "A: The provided context does not specify which data augmentation techniques (such as flipping, rotating, or scaling) are applied in the deep learning pipeline. It describes the use of deep learning models and data exploration tools but does not mention specific data augmentation methods. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?\n",
      "A: The pipeline employs deep convolutional neural networks (CNNs) and deep feed forward neural networks (DNNs) for processing audio spectrogram and image data. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?\n",
      "A: The provided context mentions that deep learning models are \"completely trainable from data, usually in a supervised fashion, such that the learning algorithm (usually stochastic gradient descent), is told what the desired output is for each input data sample. \" Therefore, stochastic gradient descent (SGD) is specifically referenced as an optimization technique applied in the deep learning pipeline. Other optimization techniques such as Adam are not mentioned in the provided context. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?\n",
      "A: The provided context does not specify the exact hyperparameters (such as learning rate or optimizer) used in the deep learning models for biodiversity monitoring. It describes the use of deep convolutional neural networks (CNNs) and deep feed forward neural networks (DNNs), but does not detail the specific hyperparameter settings. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?\n",
      "A: The provided context does not specify which metrics (such as accuracy, precision, or recall) are used to evaluate the performance of the deep learning model. It describes the use of deep learning for classification and detection, and mentions reductions in manual data review, but does not detail specific evaluation metrics. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?\n",
      "A: The provided context does not specify which regularization methods (such as dropout or L2 regularization) are used to prevent overfitting in the described deep learning pipeline. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: Which frameworks are used to build the deep learning model (e.g., TensorFlow, PyTorch)?\n",
      "A: The provided context does not specify which frameworks (such as TensorFlow or PyTorch) are used to build the deep learning models. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?\n",
      "A: The provided context does not specify which hardware resources (e. g. , GPUs, TPUs) are used for training the deep learning model. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: Where is the code repository of the deep learning pipeline available (e.g., GitHub, GitLab, BitBucket)?\n",
      "A: The provided context does not mention the location or availability of a code repository (e. g. , GitHub, GitLab, BitBucket) for the deep learning pipeline. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "         \n",
      "==============\n",
      "\n",
      "%INSTRUCTIONS: \n",
      "Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. \n",
      "Use provided concepts to understand which named entities to extract from competency answers. \n",
      "If there are no entities found for a specific concept, please write \"Not mentioned\" instead of leaving empty parentheses.\n",
      "Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         \n",
      "Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)\n",
      "Below are the competency questions and answers: \n",
      "Q: What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?\n",
      "A: Based on the provided context, the deep learning pipeline primarily uses the following data formats: - **Audio**: High sample rate audio data, such as 16-bit stereo audio at 22,000 samples per second (22 kHz), is used. Audio data is often represented as 2D spectrograms for analysis and visualization. - **Image**: High-resolution color images, for example, 2048-by-1536 8-bit 4:2:2 color images, are collected and processed. - **Other Environmental Sensor Data**: Data such as temperature or air quality, which typically have much lower data rates, are also mentioned. There is no explicit mention of video or CSV formats in the context provided. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? \n",
      "A: The context does not specify the use of standard datasets like MNIST, CIFAR, or ImageNet in the deep learning pipeline for biodiversity monitoring. Instead, it describes the use of large amounts of labeled data produced by analysts using their own user interface, including audio spectrogram and image data collected from sensor networks for classifying species signals and event types. These datasets are specific to the biodiversity monitoring projects and are not standard public datasets like MNIST, CIFAR, or ImageNet. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?\n",
      "A: The provided context does not give a detailed, step-by-step list of specific preprocessing steps such as normalization, scaling, or cleaning before training a deep learning model. However, it does mention that analysts use tools for data exploration to sort and filter data according to various attributes (e. g. , date ranges, time of day, site location, frequency ranges for audio, object size and color for images). This suggests that some level of data organization and selection is performed before model training. Additionally, labeled datasets are created to train and refine deep learning models, which implies that data labeling and possibly cleaning (to ensure correct labels) are part of the process. In summary, while the context implies that data exploration, filtering, and labeling are important preprocessing steps, it does not explicitly mention standard preprocessing techniques like normalization or scaling. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the data augmentation techniques applied in the deep learning pipeline (e.g., Flipping, Rotating, Scaling)?\n",
      "A: The provided context does not specify which data augmentation techniques (such as flipping, rotating, or scaling) are applied in the deep learning pipeline. It describes the use of deep learning models and data exploration tools but does not mention specific data augmentation methods. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?\n",
      "A: The pipeline employs deep convolutional neural networks (CNNs) and deep feed forward neural networks (DNNs) for processing audio spectrogram and image data. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?\n",
      "A: The provided context mentions that deep learning models are \"completely trainable from data, usually in a supervised fashion, such that the learning algorithm (usually stochastic gradient descent), is told what the desired output is for each input data sample. \" Therefore, stochastic gradient descent (SGD) is specifically referenced as an optimization technique applied in the deep learning pipeline. Other optimization techniques such as Adam are not mentioned in the provided context. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?\n",
      "A: The provided context does not specify the exact hyperparameters (such as learning rate or optimizer) used in the deep learning models for biodiversity monitoring. It describes the use of deep convolutional neural networks (CNNs) and deep feed forward neural networks (DNNs), but does not detail the specific hyperparameter settings. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?\n",
      "A: The provided context does not specify which metrics (such as accuracy, precision, or recall) are used to evaluate the performance of the deep learning model. It describes the use of deep learning for classification and detection, and mentions reductions in manual data review, but does not detail specific evaluation metrics. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?\n",
      "A: The provided context does not specify which regularization methods (such as dropout or L2 regularization) are used to prevent overfitting in the described deep learning pipeline. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: Which frameworks are used to build the deep learning model (e.g., TensorFlow, PyTorch)?\n",
      "A: The provided context does not specify which frameworks (such as TensorFlow or PyTorch) are used to build the deep learning models. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?\n",
      "A: The provided context does not specify which hardware resources (e. g. , GPUs, TPUs) are used for training the deep learning model. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: Where is the code repository of the deep learning pipeline available (e.g., GitHub, GitLab, BitBucket)?\n",
      "A: The provided context does not mention the location or availability of a code repository (e. g. , GitHub, GitLab, BitBucket) for the deep learning pipeline. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What is the purpose of the deep learning model (e.g., classification, segmentation, detection)?\n",
      "A: The purpose of the deep learning model described in the context is primarily classification and detection. The models are used to classify the presence or absence and activity rates of different endangered species, as well as to detect specific events of interest (such as the sounds of birds colliding with energy infrastructure) in audio spectrogram and image data. The models output the relative probability that an event is from a specific data class, usually corresponding to a species or event. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "         \n",
      "==============\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:51<00:51, 51.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%INSTRUCTIONS: \n",
      "Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. \n",
      "Use provided concepts to understand which named entities to extract from competency answers. \n",
      "If there are no entities found for a specific concept, please write \"Not mentioned\" instead of leaving empty parentheses.\n",
      "Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         \n",
      "Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)\n",
      "Below are the competency questions and answers: \n",
      "Q: What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?\n",
      "A: Based on the provided context, the deep learning pipeline uses the following data formats: - **Audio**: Original recordings are in audio format (WAV files, 32-bit, single-channel, 44. 1 kHz sampling rate). - **Image**: Audio files are converted into spectrogram images, which are saved as Portable Network Graphics (PNG) files. There is no mention of video or CSV formats being used in the pipeline. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "         \n",
      "==============\n",
      "\n",
      "%INSTRUCTIONS: \n",
      "Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. \n",
      "Use provided concepts to understand which named entities to extract from competency answers. \n",
      "If there are no entities found for a specific concept, please write \"Not mentioned\" instead of leaving empty parentheses.\n",
      "Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         \n",
      "Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)\n",
      "Below are the competency questions and answers: \n",
      "Q: What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?\n",
      "A: Based on the provided context, the deep learning pipeline uses the following data formats: - **Audio**: Original recordings are in audio format (WAV files, 32-bit, single-channel, 44. 1 kHz sampling rate). - **Image**: Audio files are converted into spectrogram images, which are saved as Portable Network Graphics (PNG) files. There is no mention of video or CSV formats being used in the pipeline. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? \n",
      "A: The datasets used in the deep learning pipeline described in the context are spectrogram images generated from audio recordings of frog calls, specifically from the genus Platymantis. The recordings were sourced from the Cornell Lab of Ornithology Macaulay Library and recent field surveys. The spectrograms were created using R packages warbleR and Seewave, and each spectrogram represents a single call note from a Platymantis species. There is no mention of standard image datasets like MNIST, CIFAR, or ImageNet being used for the classification tasks; instead, the pipeline uses custom datasets of spectrograms derived from bioacoustic recordings. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "         \n",
      "==============\n",
      "\n",
      "%INSTRUCTIONS: \n",
      "Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. \n",
      "Use provided concepts to understand which named entities to extract from competency answers. \n",
      "If there are no entities found for a specific concept, please write \"Not mentioned\" instead of leaving empty parentheses.\n",
      "Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         \n",
      "Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)\n",
      "Below are the competency questions and answers: \n",
      "Q: What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?\n",
      "A: Based on the provided context, the deep learning pipeline uses the following data formats: - **Audio**: Original recordings are in audio format (WAV files, 32-bit, single-channel, 44. 1 kHz sampling rate). - **Image**: Audio files are converted into spectrogram images, which are saved as Portable Network Graphics (PNG) files. There is no mention of video or CSV formats being used in the pipeline. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? \n",
      "A: The datasets used in the deep learning pipeline described in the context are spectrogram images generated from audio recordings of frog calls, specifically from the genus Platymantis. The recordings were sourced from the Cornell Lab of Ornithology Macaulay Library and recent field surveys. The spectrograms were created using R packages warbleR and Seewave, and each spectrogram represents a single call note from a Platymantis species. There is no mention of standard image datasets like MNIST, CIFAR, or ImageNet being used for the classification tasks; instead, the pipeline uses custom datasets of spectrograms derived from bioacoustic recordings. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?\n",
      "A: Based on the provided context, the preprocessing steps involved before training the deep learning model included: 1. **Audio Clipping and Standardization**: High-quality single notes were clipped from recordings for each species, ensuring each sample had a duration of one second. Silence was added at the beginning and end of each note to standardize the temporal scale across samples. 2. **File Formatting**: Each audio sample was saved as a 32-bit, single-channel WAV file with a 44. 1 kHz sampling rate. 3. **Spectrogram Generation**: Using R packages (warbleR and Seewave), spectrograms were generated for each audio sample across a standardized frequency range (1. 0–5. 5 kHz), which covers all known Platymantis calls. 4. **Oscillogram Parameters**: A fast-Fourier transformation (FFT) of 512 points with 90% overlap between successive windows was used to generate the spectrograms. 5. **Image Saving**: All spectrograms were saved as Portable Network Graphics (PNG) files for use as input images to the CNN. These steps ensured that all input data were consistent in duration, frequency range, and format before being used to train the deep learning model. The context does not mention explicit normalization or scaling of pixel values, nor additional cleaning steps beyond the standardization and formatting described above. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "         \n",
      "==============\n",
      "\n",
      "%INSTRUCTIONS: \n",
      "Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. \n",
      "Use provided concepts to understand which named entities to extract from competency answers. \n",
      "If there are no entities found for a specific concept, please write \"Not mentioned\" instead of leaving empty parentheses.\n",
      "Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         \n",
      "Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)\n",
      "Below are the competency questions and answers: \n",
      "Q: What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?\n",
      "A: Based on the provided context, the deep learning pipeline uses the following data formats: - **Audio**: Original recordings are in audio format (WAV files, 32-bit, single-channel, 44. 1 kHz sampling rate). - **Image**: Audio files are converted into spectrogram images, which are saved as Portable Network Graphics (PNG) files. There is no mention of video or CSV formats being used in the pipeline. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? \n",
      "A: The datasets used in the deep learning pipeline described in the context are spectrogram images generated from audio recordings of frog calls, specifically from the genus Platymantis. The recordings were sourced from the Cornell Lab of Ornithology Macaulay Library and recent field surveys. The spectrograms were created using R packages warbleR and Seewave, and each spectrogram represents a single call note from a Platymantis species. There is no mention of standard image datasets like MNIST, CIFAR, or ImageNet being used for the classification tasks; instead, the pipeline uses custom datasets of spectrograms derived from bioacoustic recordings. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?\n",
      "A: Based on the provided context, the preprocessing steps involved before training the deep learning model included: 1. **Audio Clipping and Standardization**: High-quality single notes were clipped from recordings for each species, ensuring each sample had a duration of one second. Silence was added at the beginning and end of each note to standardize the temporal scale across samples. 2. **File Formatting**: Each audio sample was saved as a 32-bit, single-channel WAV file with a 44. 1 kHz sampling rate. 3. **Spectrogram Generation**: Using R packages (warbleR and Seewave), spectrograms were generated for each audio sample across a standardized frequency range (1. 0–5. 5 kHz), which covers all known Platymantis calls. 4. **Oscillogram Parameters**: A fast-Fourier transformation (FFT) of 512 points with 90% overlap between successive windows was used to generate the spectrograms. 5. **Image Saving**: All spectrograms were saved as Portable Network Graphics (PNG) files for use as input images to the CNN. These steps ensured that all input data were consistent in duration, frequency range, and format before being used to train the deep learning model. The context does not mention explicit normalization or scaling of pixel values, nor additional cleaning steps beyond the standardization and formatting described above. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the data augmentation techniques applied in the deep learning pipeline (e.g., Flipping, Rotating, Scaling)?\n",
      "A: The provided context does not mention any specific data augmentation techniques such as flipping, rotating, or scaling being applied in the deep learning pipeline. The process described involves generating spectrograms from audio recordings and standardizing their duration and frequency range, but there is no explicit reference to the use of data augmentation methods. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "         \n",
      "==============\n",
      "\n",
      "%INSTRUCTIONS: \n",
      "Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. \n",
      "Use provided concepts to understand which named entities to extract from competency answers. \n",
      "If there are no entities found for a specific concept, please write \"Not mentioned\" instead of leaving empty parentheses.\n",
      "Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         \n",
      "Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)\n",
      "Below are the competency questions and answers: \n",
      "Q: What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?\n",
      "A: Based on the provided context, the deep learning pipeline uses the following data formats: - **Audio**: Original recordings are in audio format (WAV files, 32-bit, single-channel, 44. 1 kHz sampling rate). - **Image**: Audio files are converted into spectrogram images, which are saved as Portable Network Graphics (PNG) files. There is no mention of video or CSV formats being used in the pipeline. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? \n",
      "A: The datasets used in the deep learning pipeline described in the context are spectrogram images generated from audio recordings of frog calls, specifically from the genus Platymantis. The recordings were sourced from the Cornell Lab of Ornithology Macaulay Library and recent field surveys. The spectrograms were created using R packages warbleR and Seewave, and each spectrogram represents a single call note from a Platymantis species. There is no mention of standard image datasets like MNIST, CIFAR, or ImageNet being used for the classification tasks; instead, the pipeline uses custom datasets of spectrograms derived from bioacoustic recordings. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?\n",
      "A: Based on the provided context, the preprocessing steps involved before training the deep learning model included: 1. **Audio Clipping and Standardization**: High-quality single notes were clipped from recordings for each species, ensuring each sample had a duration of one second. Silence was added at the beginning and end of each note to standardize the temporal scale across samples. 2. **File Formatting**: Each audio sample was saved as a 32-bit, single-channel WAV file with a 44. 1 kHz sampling rate. 3. **Spectrogram Generation**: Using R packages (warbleR and Seewave), spectrograms were generated for each audio sample across a standardized frequency range (1. 0–5. 5 kHz), which covers all known Platymantis calls. 4. **Oscillogram Parameters**: A fast-Fourier transformation (FFT) of 512 points with 90% overlap between successive windows was used to generate the spectrograms. 5. **Image Saving**: All spectrograms were saved as Portable Network Graphics (PNG) files for use as input images to the CNN. These steps ensured that all input data were consistent in duration, frequency range, and format before being used to train the deep learning model. The context does not mention explicit normalization or scaling of pixel values, nor additional cleaning steps beyond the standardization and formatting described above. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the data augmentation techniques applied in the deep learning pipeline (e.g., Flipping, Rotating, Scaling)?\n",
      "A: The provided context does not mention any specific data augmentation techniques such as flipping, rotating, or scaling being applied in the deep learning pipeline. The process described involves generating spectrograms from audio recordings and standardizing their duration and frequency range, but there is no explicit reference to the use of data augmentation methods. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?\n",
      "A: The type of deep learning model used in the pipeline is a Convolutional Neural Network (CNN), specifically the Inception v3 architecture. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "         \n",
      "==============\n",
      "\n",
      "%INSTRUCTIONS: \n",
      "Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. \n",
      "Use provided concepts to understand which named entities to extract from competency answers. \n",
      "If there are no entities found for a specific concept, please write \"Not mentioned\" instead of leaving empty parentheses.\n",
      "Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         \n",
      "Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)\n",
      "Below are the competency questions and answers: \n",
      "Q: What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?\n",
      "A: Based on the provided context, the deep learning pipeline uses the following data formats: - **Audio**: Original recordings are in audio format (WAV files, 32-bit, single-channel, 44. 1 kHz sampling rate). - **Image**: Audio files are converted into spectrogram images, which are saved as Portable Network Graphics (PNG) files. There is no mention of video or CSV formats being used in the pipeline. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? \n",
      "A: The datasets used in the deep learning pipeline described in the context are spectrogram images generated from audio recordings of frog calls, specifically from the genus Platymantis. The recordings were sourced from the Cornell Lab of Ornithology Macaulay Library and recent field surveys. The spectrograms were created using R packages warbleR and Seewave, and each spectrogram represents a single call note from a Platymantis species. There is no mention of standard image datasets like MNIST, CIFAR, or ImageNet being used for the classification tasks; instead, the pipeline uses custom datasets of spectrograms derived from bioacoustic recordings. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?\n",
      "A: Based on the provided context, the preprocessing steps involved before training the deep learning model included: 1. **Audio Clipping and Standardization**: High-quality single notes were clipped from recordings for each species, ensuring each sample had a duration of one second. Silence was added at the beginning and end of each note to standardize the temporal scale across samples. 2. **File Formatting**: Each audio sample was saved as a 32-bit, single-channel WAV file with a 44. 1 kHz sampling rate. 3. **Spectrogram Generation**: Using R packages (warbleR and Seewave), spectrograms were generated for each audio sample across a standardized frequency range (1. 0–5. 5 kHz), which covers all known Platymantis calls. 4. **Oscillogram Parameters**: A fast-Fourier transformation (FFT) of 512 points with 90% overlap between successive windows was used to generate the spectrograms. 5. **Image Saving**: All spectrograms were saved as Portable Network Graphics (PNG) files for use as input images to the CNN. These steps ensured that all input data were consistent in duration, frequency range, and format before being used to train the deep learning model. The context does not mention explicit normalization or scaling of pixel values, nor additional cleaning steps beyond the standardization and formatting described above. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the data augmentation techniques applied in the deep learning pipeline (e.g., Flipping, Rotating, Scaling)?\n",
      "A: The provided context does not mention any specific data augmentation techniques such as flipping, rotating, or scaling being applied in the deep learning pipeline. The process described involves generating spectrograms from audio recordings and standardizing their duration and frequency range, but there is no explicit reference to the use of data augmentation methods. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?\n",
      "A: The type of deep learning model used in the pipeline is a Convolutional Neural Network (CNN), specifically the Inception v3 architecture. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?\n",
      "A: The provided context does not specify which optimization techniques (such as SGD or Adam) were applied in the deep learning pipeline. It describes the use of transfer learning with the Inception v3 model and modifications to training steps and validation percentage, but does not mention the specific optimizer used. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "         \n",
      "==============\n",
      "\n",
      "%INSTRUCTIONS: \n",
      "Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. \n",
      "Use provided concepts to understand which named entities to extract from competency answers. \n",
      "If there are no entities found for a specific concept, please write \"Not mentioned\" instead of leaving empty parentheses.\n",
      "Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         \n",
      "Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)\n",
      "Below are the competency questions and answers: \n",
      "Q: What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?\n",
      "A: Based on the provided context, the deep learning pipeline uses the following data formats: - **Audio**: Original recordings are in audio format (WAV files, 32-bit, single-channel, 44. 1 kHz sampling rate). - **Image**: Audio files are converted into spectrogram images, which are saved as Portable Network Graphics (PNG) files. There is no mention of video or CSV formats being used in the pipeline. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? \n",
      "A: The datasets used in the deep learning pipeline described in the context are spectrogram images generated from audio recordings of frog calls, specifically from the genus Platymantis. The recordings were sourced from the Cornell Lab of Ornithology Macaulay Library and recent field surveys. The spectrograms were created using R packages warbleR and Seewave, and each spectrogram represents a single call note from a Platymantis species. There is no mention of standard image datasets like MNIST, CIFAR, or ImageNet being used for the classification tasks; instead, the pipeline uses custom datasets of spectrograms derived from bioacoustic recordings. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?\n",
      "A: Based on the provided context, the preprocessing steps involved before training the deep learning model included: 1. **Audio Clipping and Standardization**: High-quality single notes were clipped from recordings for each species, ensuring each sample had a duration of one second. Silence was added at the beginning and end of each note to standardize the temporal scale across samples. 2. **File Formatting**: Each audio sample was saved as a 32-bit, single-channel WAV file with a 44. 1 kHz sampling rate. 3. **Spectrogram Generation**: Using R packages (warbleR and Seewave), spectrograms were generated for each audio sample across a standardized frequency range (1. 0–5. 5 kHz), which covers all known Platymantis calls. 4. **Oscillogram Parameters**: A fast-Fourier transformation (FFT) of 512 points with 90% overlap between successive windows was used to generate the spectrograms. 5. **Image Saving**: All spectrograms were saved as Portable Network Graphics (PNG) files for use as input images to the CNN. These steps ensured that all input data were consistent in duration, frequency range, and format before being used to train the deep learning model. The context does not mention explicit normalization or scaling of pixel values, nor additional cleaning steps beyond the standardization and formatting described above. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the data augmentation techniques applied in the deep learning pipeline (e.g., Flipping, Rotating, Scaling)?\n",
      "A: The provided context does not mention any specific data augmentation techniques such as flipping, rotating, or scaling being applied in the deep learning pipeline. The process described involves generating spectrograms from audio recordings and standardizing their duration and frequency range, but there is no explicit reference to the use of data augmentation methods. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?\n",
      "A: The type of deep learning model used in the pipeline is a Convolutional Neural Network (CNN), specifically the Inception v3 architecture. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?\n",
      "A: The provided context does not specify which optimization techniques (such as SGD or Adam) were applied in the deep learning pipeline. It describes the use of transfer learning with the Inception v3 model and modifications to training steps and validation percentage, but does not mention the specific optimizer used. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?\n",
      "A: The provided context does not specify the exact hyperparameters used in the deep learning model, such as the learning rate or optimizer. It mentions that two parameters were modified during training: the number of training steps and the validation percentage (set to 20% due to limited data). However, details about the learning rate, optimizer type, or other specific hyperparameters are not given in the provided text. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "         \n",
      "==============\n",
      "\n",
      "%INSTRUCTIONS: \n",
      "Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. \n",
      "Use provided concepts to understand which named entities to extract from competency answers. \n",
      "If there are no entities found for a specific concept, please write \"Not mentioned\" instead of leaving empty parentheses.\n",
      "Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         \n",
      "Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)\n",
      "Below are the competency questions and answers: \n",
      "Q: What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?\n",
      "A: Based on the provided context, the deep learning pipeline uses the following data formats: - **Audio**: Original recordings are in audio format (WAV files, 32-bit, single-channel, 44. 1 kHz sampling rate). - **Image**: Audio files are converted into spectrogram images, which are saved as Portable Network Graphics (PNG) files. There is no mention of video or CSV formats being used in the pipeline. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? \n",
      "A: The datasets used in the deep learning pipeline described in the context are spectrogram images generated from audio recordings of frog calls, specifically from the genus Platymantis. The recordings were sourced from the Cornell Lab of Ornithology Macaulay Library and recent field surveys. The spectrograms were created using R packages warbleR and Seewave, and each spectrogram represents a single call note from a Platymantis species. There is no mention of standard image datasets like MNIST, CIFAR, or ImageNet being used for the classification tasks; instead, the pipeline uses custom datasets of spectrograms derived from bioacoustic recordings. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?\n",
      "A: Based on the provided context, the preprocessing steps involved before training the deep learning model included: 1. **Audio Clipping and Standardization**: High-quality single notes were clipped from recordings for each species, ensuring each sample had a duration of one second. Silence was added at the beginning and end of each note to standardize the temporal scale across samples. 2. **File Formatting**: Each audio sample was saved as a 32-bit, single-channel WAV file with a 44. 1 kHz sampling rate. 3. **Spectrogram Generation**: Using R packages (warbleR and Seewave), spectrograms were generated for each audio sample across a standardized frequency range (1. 0–5. 5 kHz), which covers all known Platymantis calls. 4. **Oscillogram Parameters**: A fast-Fourier transformation (FFT) of 512 points with 90% overlap between successive windows was used to generate the spectrograms. 5. **Image Saving**: All spectrograms were saved as Portable Network Graphics (PNG) files for use as input images to the CNN. These steps ensured that all input data were consistent in duration, frequency range, and format before being used to train the deep learning model. The context does not mention explicit normalization or scaling of pixel values, nor additional cleaning steps beyond the standardization and formatting described above. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the data augmentation techniques applied in the deep learning pipeline (e.g., Flipping, Rotating, Scaling)?\n",
      "A: The provided context does not mention any specific data augmentation techniques such as flipping, rotating, or scaling being applied in the deep learning pipeline. The process described involves generating spectrograms from audio recordings and standardizing their duration and frequency range, but there is no explicit reference to the use of data augmentation methods. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?\n",
      "A: The type of deep learning model used in the pipeline is a Convolutional Neural Network (CNN), specifically the Inception v3 architecture. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?\n",
      "A: The provided context does not specify which optimization techniques (such as SGD or Adam) were applied in the deep learning pipeline. It describes the use of transfer learning with the Inception v3 model and modifications to training steps and validation percentage, but does not mention the specific optimizer used. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?\n",
      "A: The provided context does not specify the exact hyperparameters used in the deep learning model, such as the learning rate or optimizer. It mentions that two parameters were modified during training: the number of training steps and the validation percentage (set to 20% due to limited data). However, details about the learning rate, optimizer type, or other specific hyperparameters are not given in the provided text. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?\n",
      "A: Based on the provided context, the metrics used to evaluate the performance of the deep learning model (TensorFlow/Inception v3) include: 1. **Correct identification rate (accuracy)**: This is the primary metric reported, representing the percentage of images correctly identified by the model for each species and overall. 2. **Average certainty rate**: This refers to the average confidence (certainty) score assigned by the model to its predictions for each species. There is no mention in the provided context of using precision, recall, or F1-score as evaluation metrics. The focus is on correct identification rate (accuracy) and average certainty rate. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "         \n",
      "==============\n",
      "\n",
      "%INSTRUCTIONS: \n",
      "Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. \n",
      "Use provided concepts to understand which named entities to extract from competency answers. \n",
      "If there are no entities found for a specific concept, please write \"Not mentioned\" instead of leaving empty parentheses.\n",
      "Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         \n",
      "Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)\n",
      "Below are the competency questions and answers: \n",
      "Q: What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?\n",
      "A: Based on the provided context, the deep learning pipeline uses the following data formats: - **Audio**: Original recordings are in audio format (WAV files, 32-bit, single-channel, 44. 1 kHz sampling rate). - **Image**: Audio files are converted into spectrogram images, which are saved as Portable Network Graphics (PNG) files. There is no mention of video or CSV formats being used in the pipeline. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? \n",
      "A: The datasets used in the deep learning pipeline described in the context are spectrogram images generated from audio recordings of frog calls, specifically from the genus Platymantis. The recordings were sourced from the Cornell Lab of Ornithology Macaulay Library and recent field surveys. The spectrograms were created using R packages warbleR and Seewave, and each spectrogram represents a single call note from a Platymantis species. There is no mention of standard image datasets like MNIST, CIFAR, or ImageNet being used for the classification tasks; instead, the pipeline uses custom datasets of spectrograms derived from bioacoustic recordings. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?\n",
      "A: Based on the provided context, the preprocessing steps involved before training the deep learning model included: 1. **Audio Clipping and Standardization**: High-quality single notes were clipped from recordings for each species, ensuring each sample had a duration of one second. Silence was added at the beginning and end of each note to standardize the temporal scale across samples. 2. **File Formatting**: Each audio sample was saved as a 32-bit, single-channel WAV file with a 44. 1 kHz sampling rate. 3. **Spectrogram Generation**: Using R packages (warbleR and Seewave), spectrograms were generated for each audio sample across a standardized frequency range (1. 0–5. 5 kHz), which covers all known Platymantis calls. 4. **Oscillogram Parameters**: A fast-Fourier transformation (FFT) of 512 points with 90% overlap between successive windows was used to generate the spectrograms. 5. **Image Saving**: All spectrograms were saved as Portable Network Graphics (PNG) files for use as input images to the CNN. These steps ensured that all input data were consistent in duration, frequency range, and format before being used to train the deep learning model. The context does not mention explicit normalization or scaling of pixel values, nor additional cleaning steps beyond the standardization and formatting described above. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the data augmentation techniques applied in the deep learning pipeline (e.g., Flipping, Rotating, Scaling)?\n",
      "A: The provided context does not mention any specific data augmentation techniques such as flipping, rotating, or scaling being applied in the deep learning pipeline. The process described involves generating spectrograms from audio recordings and standardizing their duration and frequency range, but there is no explicit reference to the use of data augmentation methods. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?\n",
      "A: The type of deep learning model used in the pipeline is a Convolutional Neural Network (CNN), specifically the Inception v3 architecture. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?\n",
      "A: The provided context does not specify which optimization techniques (such as SGD or Adam) were applied in the deep learning pipeline. It describes the use of transfer learning with the Inception v3 model and modifications to training steps and validation percentage, but does not mention the specific optimizer used. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?\n",
      "A: The provided context does not specify the exact hyperparameters used in the deep learning model, such as the learning rate or optimizer. It mentions that two parameters were modified during training: the number of training steps and the validation percentage (set to 20% due to limited data). However, details about the learning rate, optimizer type, or other specific hyperparameters are not given in the provided text. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?\n",
      "A: Based on the provided context, the metrics used to evaluate the performance of the deep learning model (TensorFlow/Inception v3) include: 1. **Correct identification rate (accuracy)**: This is the primary metric reported, representing the percentage of images correctly identified by the model for each species and overall. 2. **Average certainty rate**: This refers to the average confidence (certainty) score assigned by the model to its predictions for each species. There is no mention in the provided context of using precision, recall, or F1-score as evaluation metrics. The focus is on correct identification rate (accuracy) and average certainty rate. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?\n",
      "A: The provided context does not specify any particular regularization methods (such as dropout or L2 regularization) used to prevent overfitting in the deep learning pipeline described. The text discusses the use of transfer learning, cross-validation techniques (leave-one-out and k-fold), and the architecture of CNNs, but does not mention explicit regularization methods. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "         \n",
      "==============\n",
      "\n",
      "%INSTRUCTIONS: \n",
      "Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. \n",
      "Use provided concepts to understand which named entities to extract from competency answers. \n",
      "If there are no entities found for a specific concept, please write \"Not mentioned\" instead of leaving empty parentheses.\n",
      "Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         \n",
      "Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)\n",
      "Below are the competency questions and answers: \n",
      "Q: What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?\n",
      "A: Based on the provided context, the deep learning pipeline uses the following data formats: - **Audio**: Original recordings are in audio format (WAV files, 32-bit, single-channel, 44. 1 kHz sampling rate). - **Image**: Audio files are converted into spectrogram images, which are saved as Portable Network Graphics (PNG) files. There is no mention of video or CSV formats being used in the pipeline. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? \n",
      "A: The datasets used in the deep learning pipeline described in the context are spectrogram images generated from audio recordings of frog calls, specifically from the genus Platymantis. The recordings were sourced from the Cornell Lab of Ornithology Macaulay Library and recent field surveys. The spectrograms were created using R packages warbleR and Seewave, and each spectrogram represents a single call note from a Platymantis species. There is no mention of standard image datasets like MNIST, CIFAR, or ImageNet being used for the classification tasks; instead, the pipeline uses custom datasets of spectrograms derived from bioacoustic recordings. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?\n",
      "A: Based on the provided context, the preprocessing steps involved before training the deep learning model included: 1. **Audio Clipping and Standardization**: High-quality single notes were clipped from recordings for each species, ensuring each sample had a duration of one second. Silence was added at the beginning and end of each note to standardize the temporal scale across samples. 2. **File Formatting**: Each audio sample was saved as a 32-bit, single-channel WAV file with a 44. 1 kHz sampling rate. 3. **Spectrogram Generation**: Using R packages (warbleR and Seewave), spectrograms were generated for each audio sample across a standardized frequency range (1. 0–5. 5 kHz), which covers all known Platymantis calls. 4. **Oscillogram Parameters**: A fast-Fourier transformation (FFT) of 512 points with 90% overlap between successive windows was used to generate the spectrograms. 5. **Image Saving**: All spectrograms were saved as Portable Network Graphics (PNG) files for use as input images to the CNN. These steps ensured that all input data were consistent in duration, frequency range, and format before being used to train the deep learning model. The context does not mention explicit normalization or scaling of pixel values, nor additional cleaning steps beyond the standardization and formatting described above. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the data augmentation techniques applied in the deep learning pipeline (e.g., Flipping, Rotating, Scaling)?\n",
      "A: The provided context does not mention any specific data augmentation techniques such as flipping, rotating, or scaling being applied in the deep learning pipeline. The process described involves generating spectrograms from audio recordings and standardizing their duration and frequency range, but there is no explicit reference to the use of data augmentation methods. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?\n",
      "A: The type of deep learning model used in the pipeline is a Convolutional Neural Network (CNN), specifically the Inception v3 architecture. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?\n",
      "A: The provided context does not specify which optimization techniques (such as SGD or Adam) were applied in the deep learning pipeline. It describes the use of transfer learning with the Inception v3 model and modifications to training steps and validation percentage, but does not mention the specific optimizer used. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?\n",
      "A: The provided context does not specify the exact hyperparameters used in the deep learning model, such as the learning rate or optimizer. It mentions that two parameters were modified during training: the number of training steps and the validation percentage (set to 20% due to limited data). However, details about the learning rate, optimizer type, or other specific hyperparameters are not given in the provided text. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?\n",
      "A: Based on the provided context, the metrics used to evaluate the performance of the deep learning model (TensorFlow/Inception v3) include: 1. **Correct identification rate (accuracy)**: This is the primary metric reported, representing the percentage of images correctly identified by the model for each species and overall. 2. **Average certainty rate**: This refers to the average confidence (certainty) score assigned by the model to its predictions for each species. There is no mention in the provided context of using precision, recall, or F1-score as evaluation metrics. The focus is on correct identification rate (accuracy) and average certainty rate. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?\n",
      "A: The provided context does not specify any particular regularization methods (such as dropout or L2 regularization) used to prevent overfitting in the deep learning pipeline described. The text discusses the use of transfer learning, cross-validation techniques (leave-one-out and k-fold), and the architecture of CNNs, but does not mention explicit regularization methods. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: Which frameworks are used to build the deep learning model (e.g., TensorFlow, PyTorch)?\n",
      "A: The deep learning model was built using TensorFlow. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "         \n",
      "==============\n",
      "\n",
      "%INSTRUCTIONS: \n",
      "Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. \n",
      "Use provided concepts to understand which named entities to extract from competency answers. \n",
      "If there are no entities found for a specific concept, please write \"Not mentioned\" instead of leaving empty parentheses.\n",
      "Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         \n",
      "Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)\n",
      "Below are the competency questions and answers: \n",
      "Q: What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?\n",
      "A: Based on the provided context, the deep learning pipeline uses the following data formats: - **Audio**: Original recordings are in audio format (WAV files, 32-bit, single-channel, 44. 1 kHz sampling rate). - **Image**: Audio files are converted into spectrogram images, which are saved as Portable Network Graphics (PNG) files. There is no mention of video or CSV formats being used in the pipeline. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? \n",
      "A: The datasets used in the deep learning pipeline described in the context are spectrogram images generated from audio recordings of frog calls, specifically from the genus Platymantis. The recordings were sourced from the Cornell Lab of Ornithology Macaulay Library and recent field surveys. The spectrograms were created using R packages warbleR and Seewave, and each spectrogram represents a single call note from a Platymantis species. There is no mention of standard image datasets like MNIST, CIFAR, or ImageNet being used for the classification tasks; instead, the pipeline uses custom datasets of spectrograms derived from bioacoustic recordings. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?\n",
      "A: Based on the provided context, the preprocessing steps involved before training the deep learning model included: 1. **Audio Clipping and Standardization**: High-quality single notes were clipped from recordings for each species, ensuring each sample had a duration of one second. Silence was added at the beginning and end of each note to standardize the temporal scale across samples. 2. **File Formatting**: Each audio sample was saved as a 32-bit, single-channel WAV file with a 44. 1 kHz sampling rate. 3. **Spectrogram Generation**: Using R packages (warbleR and Seewave), spectrograms were generated for each audio sample across a standardized frequency range (1. 0–5. 5 kHz), which covers all known Platymantis calls. 4. **Oscillogram Parameters**: A fast-Fourier transformation (FFT) of 512 points with 90% overlap between successive windows was used to generate the spectrograms. 5. **Image Saving**: All spectrograms were saved as Portable Network Graphics (PNG) files for use as input images to the CNN. These steps ensured that all input data were consistent in duration, frequency range, and format before being used to train the deep learning model. The context does not mention explicit normalization or scaling of pixel values, nor additional cleaning steps beyond the standardization and formatting described above. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the data augmentation techniques applied in the deep learning pipeline (e.g., Flipping, Rotating, Scaling)?\n",
      "A: The provided context does not mention any specific data augmentation techniques such as flipping, rotating, or scaling being applied in the deep learning pipeline. The process described involves generating spectrograms from audio recordings and standardizing their duration and frequency range, but there is no explicit reference to the use of data augmentation methods. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?\n",
      "A: The type of deep learning model used in the pipeline is a Convolutional Neural Network (CNN), specifically the Inception v3 architecture. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?\n",
      "A: The provided context does not specify which optimization techniques (such as SGD or Adam) were applied in the deep learning pipeline. It describes the use of transfer learning with the Inception v3 model and modifications to training steps and validation percentage, but does not mention the specific optimizer used. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?\n",
      "A: The provided context does not specify the exact hyperparameters used in the deep learning model, such as the learning rate or optimizer. It mentions that two parameters were modified during training: the number of training steps and the validation percentage (set to 20% due to limited data). However, details about the learning rate, optimizer type, or other specific hyperparameters are not given in the provided text. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?\n",
      "A: Based on the provided context, the metrics used to evaluate the performance of the deep learning model (TensorFlow/Inception v3) include: 1. **Correct identification rate (accuracy)**: This is the primary metric reported, representing the percentage of images correctly identified by the model for each species and overall. 2. **Average certainty rate**: This refers to the average confidence (certainty) score assigned by the model to its predictions for each species. There is no mention in the provided context of using precision, recall, or F1-score as evaluation metrics. The focus is on correct identification rate (accuracy) and average certainty rate. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?\n",
      "A: The provided context does not specify any particular regularization methods (such as dropout or L2 regularization) used to prevent overfitting in the deep learning pipeline described. The text discusses the use of transfer learning, cross-validation techniques (leave-one-out and k-fold), and the architecture of CNNs, but does not mention explicit regularization methods. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: Which frameworks are used to build the deep learning model (e.g., TensorFlow, PyTorch)?\n",
      "A: The deep learning model was built using TensorFlow. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?\n",
      "A: The provided context does not specify which hardware resources (e. g. , GPUs, TPUs) were used for training the deep learning model. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "         \n",
      "==============\n",
      "\n",
      "%INSTRUCTIONS: \n",
      "Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. \n",
      "Use provided concepts to understand which named entities to extract from competency answers. \n",
      "If there are no entities found for a specific concept, please write \"Not mentioned\" instead of leaving empty parentheses.\n",
      "Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         \n",
      "Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)\n",
      "Below are the competency questions and answers: \n",
      "Q: What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?\n",
      "A: Based on the provided context, the deep learning pipeline uses the following data formats: - **Audio**: Original recordings are in audio format (WAV files, 32-bit, single-channel, 44. 1 kHz sampling rate). - **Image**: Audio files are converted into spectrogram images, which are saved as Portable Network Graphics (PNG) files. There is no mention of video or CSV formats being used in the pipeline. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? \n",
      "A: The datasets used in the deep learning pipeline described in the context are spectrogram images generated from audio recordings of frog calls, specifically from the genus Platymantis. The recordings were sourced from the Cornell Lab of Ornithology Macaulay Library and recent field surveys. The spectrograms were created using R packages warbleR and Seewave, and each spectrogram represents a single call note from a Platymantis species. There is no mention of standard image datasets like MNIST, CIFAR, or ImageNet being used for the classification tasks; instead, the pipeline uses custom datasets of spectrograms derived from bioacoustic recordings. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?\n",
      "A: Based on the provided context, the preprocessing steps involved before training the deep learning model included: 1. **Audio Clipping and Standardization**: High-quality single notes were clipped from recordings for each species, ensuring each sample had a duration of one second. Silence was added at the beginning and end of each note to standardize the temporal scale across samples. 2. **File Formatting**: Each audio sample was saved as a 32-bit, single-channel WAV file with a 44. 1 kHz sampling rate. 3. **Spectrogram Generation**: Using R packages (warbleR and Seewave), spectrograms were generated for each audio sample across a standardized frequency range (1. 0–5. 5 kHz), which covers all known Platymantis calls. 4. **Oscillogram Parameters**: A fast-Fourier transformation (FFT) of 512 points with 90% overlap between successive windows was used to generate the spectrograms. 5. **Image Saving**: All spectrograms were saved as Portable Network Graphics (PNG) files for use as input images to the CNN. These steps ensured that all input data were consistent in duration, frequency range, and format before being used to train the deep learning model. The context does not mention explicit normalization or scaling of pixel values, nor additional cleaning steps beyond the standardization and formatting described above. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the data augmentation techniques applied in the deep learning pipeline (e.g., Flipping, Rotating, Scaling)?\n",
      "A: The provided context does not mention any specific data augmentation techniques such as flipping, rotating, or scaling being applied in the deep learning pipeline. The process described involves generating spectrograms from audio recordings and standardizing their duration and frequency range, but there is no explicit reference to the use of data augmentation methods. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?\n",
      "A: The type of deep learning model used in the pipeline is a Convolutional Neural Network (CNN), specifically the Inception v3 architecture. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?\n",
      "A: The provided context does not specify which optimization techniques (such as SGD or Adam) were applied in the deep learning pipeline. It describes the use of transfer learning with the Inception v3 model and modifications to training steps and validation percentage, but does not mention the specific optimizer used. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?\n",
      "A: The provided context does not specify the exact hyperparameters used in the deep learning model, such as the learning rate or optimizer. It mentions that two parameters were modified during training: the number of training steps and the validation percentage (set to 20% due to limited data). However, details about the learning rate, optimizer type, or other specific hyperparameters are not given in the provided text. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?\n",
      "A: Based on the provided context, the metrics used to evaluate the performance of the deep learning model (TensorFlow/Inception v3) include: 1. **Correct identification rate (accuracy)**: This is the primary metric reported, representing the percentage of images correctly identified by the model for each species and overall. 2. **Average certainty rate**: This refers to the average confidence (certainty) score assigned by the model to its predictions for each species. There is no mention in the provided context of using precision, recall, or F1-score as evaluation metrics. The focus is on correct identification rate (accuracy) and average certainty rate. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?\n",
      "A: The provided context does not specify any particular regularization methods (such as dropout or L2 regularization) used to prevent overfitting in the deep learning pipeline described. The text discusses the use of transfer learning, cross-validation techniques (leave-one-out and k-fold), and the architecture of CNNs, but does not mention explicit regularization methods. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: Which frameworks are used to build the deep learning model (e.g., TensorFlow, PyTorch)?\n",
      "A: The deep learning model was built using TensorFlow. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?\n",
      "A: The provided context does not specify which hardware resources (e. g. , GPUs, TPUs) were used for training the deep learning model. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: Where is the code repository of the deep learning pipeline available (e.g., GitHub, GitLab, BitBucket)?\n",
      "A: I don't know. The provided context does not mention where the code repository of the deep learning pipeline is available. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "         \n",
      "==============\n",
      "\n",
      "%INSTRUCTIONS: \n",
      "Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. \n",
      "Use provided concepts to understand which named entities to extract from competency answers. \n",
      "If there are no entities found for a specific concept, please write \"Not mentioned\" instead of leaving empty parentheses.\n",
      "Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         \n",
      "Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)\n",
      "Below are the competency questions and answers: \n",
      "Q: What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?\n",
      "A: Based on the provided context, the deep learning pipeline uses the following data formats: - **Audio**: Original recordings are in audio format (WAV files, 32-bit, single-channel, 44. 1 kHz sampling rate). - **Image**: Audio files are converted into spectrogram images, which are saved as Portable Network Graphics (PNG) files. There is no mention of video or CSV formats being used in the pipeline. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? \n",
      "A: The datasets used in the deep learning pipeline described in the context are spectrogram images generated from audio recordings of frog calls, specifically from the genus Platymantis. The recordings were sourced from the Cornell Lab of Ornithology Macaulay Library and recent field surveys. The spectrograms were created using R packages warbleR and Seewave, and each spectrogram represents a single call note from a Platymantis species. There is no mention of standard image datasets like MNIST, CIFAR, or ImageNet being used for the classification tasks; instead, the pipeline uses custom datasets of spectrograms derived from bioacoustic recordings. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?\n",
      "A: Based on the provided context, the preprocessing steps involved before training the deep learning model included: 1. **Audio Clipping and Standardization**: High-quality single notes were clipped from recordings for each species, ensuring each sample had a duration of one second. Silence was added at the beginning and end of each note to standardize the temporal scale across samples. 2. **File Formatting**: Each audio sample was saved as a 32-bit, single-channel WAV file with a 44. 1 kHz sampling rate. 3. **Spectrogram Generation**: Using R packages (warbleR and Seewave), spectrograms were generated for each audio sample across a standardized frequency range (1. 0–5. 5 kHz), which covers all known Platymantis calls. 4. **Oscillogram Parameters**: A fast-Fourier transformation (FFT) of 512 points with 90% overlap between successive windows was used to generate the spectrograms. 5. **Image Saving**: All spectrograms were saved as Portable Network Graphics (PNG) files for use as input images to the CNN. These steps ensured that all input data were consistent in duration, frequency range, and format before being used to train the deep learning model. The context does not mention explicit normalization or scaling of pixel values, nor additional cleaning steps beyond the standardization and formatting described above. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the data augmentation techniques applied in the deep learning pipeline (e.g., Flipping, Rotating, Scaling)?\n",
      "A: The provided context does not mention any specific data augmentation techniques such as flipping, rotating, or scaling being applied in the deep learning pipeline. The process described involves generating spectrograms from audio recordings and standardizing their duration and frequency range, but there is no explicit reference to the use of data augmentation methods. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?\n",
      "A: The type of deep learning model used in the pipeline is a Convolutional Neural Network (CNN), specifically the Inception v3 architecture. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?\n",
      "A: The provided context does not specify which optimization techniques (such as SGD or Adam) were applied in the deep learning pipeline. It describes the use of transfer learning with the Inception v3 model and modifications to training steps and validation percentage, but does not mention the specific optimizer used. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?\n",
      "A: The provided context does not specify the exact hyperparameters used in the deep learning model, such as the learning rate or optimizer. It mentions that two parameters were modified during training: the number of training steps and the validation percentage (set to 20% due to limited data). However, details about the learning rate, optimizer type, or other specific hyperparameters are not given in the provided text. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?\n",
      "A: Based on the provided context, the metrics used to evaluate the performance of the deep learning model (TensorFlow/Inception v3) include: 1. **Correct identification rate (accuracy)**: This is the primary metric reported, representing the percentage of images correctly identified by the model for each species and overall. 2. **Average certainty rate**: This refers to the average confidence (certainty) score assigned by the model to its predictions for each species. There is no mention in the provided context of using precision, recall, or F1-score as evaluation metrics. The focus is on correct identification rate (accuracy) and average certainty rate. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?\n",
      "A: The provided context does not specify any particular regularization methods (such as dropout or L2 regularization) used to prevent overfitting in the deep learning pipeline described. The text discusses the use of transfer learning, cross-validation techniques (leave-one-out and k-fold), and the architecture of CNNs, but does not mention explicit regularization methods. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: Which frameworks are used to build the deep learning model (e.g., TensorFlow, PyTorch)?\n",
      "A: The deep learning model was built using TensorFlow. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?\n",
      "A: The provided context does not specify which hardware resources (e. g. , GPUs, TPUs) were used for training the deep learning model. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: Where is the code repository of the deep learning pipeline available (e.g., GitHub, GitLab, BitBucket)?\n",
      "A: I don't know. The provided context does not mention where the code repository of the deep learning pipeline is available. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "        Q: What is the purpose of the deep learning model (e.g., classification, segmentation, detection)?\n",
      "A: The purpose of the deep learning model described in the context is classification. Specifically, it is used to identify Platymantis frog species based on spectrogram images of their call notes. \n",
      "\n",
      "        Answer:::\n",
      "        Provide your answer as follows:\n",
      "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
      "        Answer:::\n",
      "         \n",
      "==============\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:01<00:00, 60.92s/it]\n"
     ]
    }
   ],
   "source": [
    "# from helper_functions import load_cqs, read_txt\n",
    "import numpy as np\n",
    "# from LLM_loader import llm\n",
    "\n",
    "def NER():\n",
    "    # for pub in [1,3,5,7,8,9,10,12,13,14,16,18,19,20,24,25,27,28,33,34,37,38,39,41,42,43,44,45,46,47]:\n",
    "    for pub in tqdm([1,3]):\n",
    "        # CQs = load_cqs(config.get('Paths', 'CQs_path'))\n",
    "        CQs = load_cqs('../CQs/CQs.txt')\n",
    "        # combined_prompt = read_txt(config.get('Paths', 'NER_prompt'))\n",
    "        combined_prompt = read_txt('../NER_prompt/NER_Mixtral_prompt.txt')\n",
    "        Answer_format  = '''\n",
    "        Answer:::\n",
    "        Provide your answer as follows:\n",
    "        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...\n",
    "        Answer:::\n",
    "        '''\n",
    "        # 13개의 CQ만 선택함\n",
    "        for i in tqdm([2,5,6,4,12,15,13,22,17,19,20,8,25]):\n",
    "            # Answer = read_txt(f\"{config.get('Paths', 'Ans_to_cq_output_folder')}Publication{pub}_CQ{i}.txt\")\n",
    "            Answer = read_txt(f'RAG_CQ_ans/V1_processed/Publication{pub}_CQ{i}.txt')\n",
    "            prompt = f\"Q: {CQs[i-1]}\\nA: {Answer}\\n\"\n",
    "            combined_prompt += prompt\n",
    "            combined_prompt +=Answer_format\n",
    "            # print(combined_prompt, '\\n==============\\n')\n",
    "            output = llm.invoke(combined_prompt).content\n",
    "            with open(f\"NER/Publication{pub}_concepts.txt\", 'w') as f:\n",
    "                f.write(output)\n",
    "                \n",
    "NER()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255d3ae1",
   "metadata": {},
   "source": [
    "### KG 생성(create_kg.py)\n",
    "\n",
    "* 해당 Concept이 없는 경우 빈괄호()만 생성되는데, 코드 상에서는 Not mentioned가 있어야 패스되는 것 같음. NER_Mixtral_prompt.txt 프롬프트를 일부 수정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26211b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import rdflib\n",
    "from rdflib import Graph, Namespace, Literal, RDF\n",
    "from urllib.parse import quote\n",
    "\n",
    "# Define the folders and files paths\n",
    "# ner_folder = 'NER/'\n",
    "# kg_folder = 'KG/'\n",
    "\n",
    "\n",
    "# Define the namespace\n",
    "DLPROV = Namespace(\"https://w3id.org/dlprov#\")\n",
    "\n",
    "# Function to read named entities from a publication file\n",
    "def read_named_entities(file_path):\n",
    "    named_entities = {}\n",
    "    exclude_phrases = [\"not mentioned\", \"Not mentioned\", \"Not Provided\",\n",
    "                       \"not explicitly mentioned\", \"not provided\", \"Not explicitly mentioned\"]\n",
    "    f = open(file_path, \"r\")\n",
    "    lines = f.readlines()\n",
    "\n",
    "    #lines = str(lines).strip().split('\\n')\n",
    "\n",
    "\n",
    "    for line in lines[1:]:  # Skip the first line\n",
    "        # Using regular expression to correctly split at the first parenthesis\n",
    "        #line = line.strip().split('\\n')\n",
    "        if 'named entities' in line.lower() or 'for each provided concept' in line.lower():\n",
    "            continue\n",
    "        if line.strip():\n",
    "            if ':' in line:\n",
    "                concept, entity_str = line.split(':', 1)\n",
    "                concept = concept.strip()\n",
    "                entity_str = entity_str.strip()\n",
    "                entities = [entity.strip() for entity in entity_str.split(',')]\n",
    "                filtered_entities = [entity for entity in entities if entity not in exclude_phrases]\n",
    "                if filtered_entities:\n",
    "                    named_entities[concept] = filtered_entities\n",
    "\n",
    "            else:\n",
    "                concept, entities_str = line.split('(', 1)\n",
    "                entities_str = entities_str.rstrip('),')\n",
    "                entities = [entity.strip() for entity in entities_str.split(',')]\n",
    "                filtered_entities = [entity for entity in entities if entity not in exclude_phrases]\n",
    "                if filtered_entities:\n",
    "                    named_entities[concept.strip()] = filtered_entities\n",
    "\n",
    "    return named_entities\n",
    "\n",
    "# Clean and encode entity names\n",
    "def clean_and_encode(entity_name):\n",
    "    # Remove unwanted characters and encode\n",
    "    entity_name = entity_name.replace('\\\\', '').replace(')', '').strip()\n",
    "    return quote(entity_name.replace(' ', '_'))\n",
    "\n",
    "# Convert named entities to KG\n",
    "def create_kg(named_entities):\n",
    "\n",
    "    g = Graph()\n",
    "    g.bind(\"dlprov\", DLPROV)\n",
    "\n",
    "    for concept, entities in named_entities.items():\n",
    "        # print(f'concept: {concept}')\n",
    "        # print(f'entities: {entities[0]}')\n",
    "        if (\"not mentioned\" in entities[0].lower()) or (\"not provided\" in entities[0].lower()):            \n",
    "            continue\n",
    "\n",
    "        concept_uri = DLPROV[clean_and_encode(concept.strip())]\n",
    "\n",
    "\n",
    "        for entity in entities:\n",
    "            entity_name, *abbr = entity.split('(')\n",
    "\n",
    "            if entity_name:\n",
    "                entity_name = entity_name.strip()\n",
    "                abbr = abbr[0].strip(')') if abbr else None\n",
    "                entity_uri = DLPROV[clean_and_encode(entity_name)]\n",
    "                g.add((entity_uri, RDF.type, concept_uri))\n",
    "                if abbr:\n",
    "                    abbr_uri = DLPROV[clean_and_encode(abbr)]\n",
    "                    g.add((abbr_uri, RDF.type, concept_uri))\n",
    "\n",
    "    return g\n",
    "\n",
    "\n",
    "# Function to write the KG to a file\n",
    "def write_kg_to_file(kg, output_file):\n",
    "    kg.serialize(destination=output_file, format='turtle')\n",
    "\n",
    "## Main script\n",
    "# if __name__ == \"__main__\":\n",
    "#     models = ['GPT4', 'GPT3.5', 'Gemini', 'Mixtral_8_22b']\n",
    "#     for model_name in models:\n",
    "#         llm_model_path = os.path.join(ner_folder, model_name)\n",
    "#         kg_model_path = os.path.join(kg_folder, model_name)\n",
    "#         # Iterate through the publications\n",
    "#         for publication_file in os.listdir(llm_model_path):\n",
    "#             if publication_file.endswith('.txt'):\n",
    "#                 publication_path = os.path.join(llm_model_path, publication_file)\n",
    "#                 print('Publication Path: {}'.format(publication_path))\n",
    "#                 named_entities = read_named_entities(publication_path)\n",
    "#                 print('named_entities: {}'.format(named_entities))\n",
    "\n",
    "\n",
    "#                 # Create KG\n",
    "#                 kg = create_kg(named_entities)\n",
    "\n",
    "#                 # Output KG file\n",
    "#                 output_file = os.path.join(kg_model_path, publication_file.replace('.txt', '_KG.ttl'))\n",
    "#                 write_kg_to_file(kg, output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6581fff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publication Path: NER/Publication3_concepts.txt\n",
      "named_entities: {'Method': ['Not mentioned)', ''], 'RawData': ['Audio recordings of frog calls', 'specifically from the genus Platymantis)', ''], 'DataFormat': ['WAV files', 'PNG files)', ''], 'DataAnnotationTechnique': ['Not mentioned)', ''], 'DataAugmentationTechnique': ['Not mentioned)', ''], 'Dataset': ['Custom datasets of spectrograms derived from bioacoustic recordings', 'spectrogram images generated from Platymantis audio recordings', 'recordings sourced from Cornell Lab of Ornithology Macaulay Library and recent field surveys)', ''], 'PreprocessingStep': ['Audio clipping and standardization', 'Silence addition', 'File formatting', 'Spectrogram generation using warbleR and Seewave', 'Oscillogram parameters with FFT of 512 points and 90% overlap', 'Image saving as PNG)', ''], 'DataSplitCriteria': ['Validation percentage set to 20%)', ''], 'CodeRepository': ['Not mentioned)', ''], 'DataRepository': ['Cornell Lab of Ornithology Macaulay Library)', ''], 'CodeRepositoryLink': ['Not mentioned)', ''], 'DataRepositoryLink': ['Not mentioned)', ''], 'DeepLearningModel': ['Convolutional Neural Network (CNN)', 'Inception v3)', ''], 'Hyperparameter': ['Number of training steps', 'Validation percentage)', ''], 'HyperparameterOptimization': ['Not mentioned)', ''], 'OptimizationTechnique': ['Not mentioned)', ''], 'TrainingCompletionCriteria': ['Not mentioned)', ''], 'RegularizationMethod': ['Not mentioned)', ''], 'ModelPerformanceMonitoringStrategy': ['Not mentioned)', ''], 'Framework': ['TensorFlow)', ''], 'HardwareResource': ['Not mentioned)', ''], 'PostprocessingStep': ['Not mentioned)', ''], 'PerformanceMetric': ['Correct identification rate (accuracy)', 'Average certainty rate)', ''], 'GeneralizabilityMeasure(Cross-validation techniques': ['leave-one-out and k-fold)', ''], 'RandomnessStrategy': ['Not mentioned)', ''], 'ModelPurpose': ['Classification of Platymantis frog species based on spectrogram images)', ''], 'DataBiasTechnique': ['Not mentioned)', ''], 'ModelDeploymentProcess': ['Not mentioned)', ''], 'DeploymentPlatform': ['Not mentioned).']}\n",
      "Publication Path: NER/Publication1_concepts.txt\n",
      "named_entities: {'Method': ['Not mentioned)', ''], 'RawData': ['Not mentioned)', ''], 'DataFormat': ['Audio', 'Image', 'Environmental Sensor Data)', ''], 'DataAnnotationTechnique': ['Not mentioned)', ''], 'DataAugmentationTechnique': ['Not mentioned)', ''], 'Dataset': ['Labeled audio spectrogram and image data collected from sensor networks for biodiversity monitoring projects)', ''], 'PreprocessingStep': ['Data exploration', 'Sorting', 'Filtering', 'Data labeling)', ''], 'DataSplitCriteria': ['Not mentioned)', ''], 'CodeRepository': ['Not mentioned)', ''], 'DataRepository': ['Not mentioned)', ''], 'CodeRepositoryLink': ['Not mentioned)', ''], 'DataRepositoryLink': ['Not mentioned)', ''], 'DeepLearningModel': ['CNN', 'DNN)', ''], 'Hyperparameter': ['Not mentioned)', ''], 'HyperparameterOptimization': ['Not mentioned)', ''], 'OptimizationTechnique': ['Stochastic Gradient Descent (SGD))', ''], 'TrainingCompletionCriteria': ['Not mentioned)', ''], 'RegularizationMethod': ['Not mentioned)', ''], 'ModelPerformanceMonitoringStrategy': ['Not mentioned)', ''], 'Framework': ['Not mentioned)', ''], 'HardwareResource': ['Not mentioned)', ''], 'PostprocessingStep': ['Not mentioned)', ''], 'PerformanceMetric': ['Not mentioned)', ''], 'GeneralizabilityMeasure': ['Not mentioned)', ''], 'RandomnessStrategy': ['Not mentioned)', ''], 'ModelPurpose': ['Classification', 'Detection)', ''], 'DataBiasTechnique': ['Not mentioned)', ''], 'ModelDeploymentProcess': ['Not mentioned)', ''], 'DeploymentPlatform': ['Not mentioned).']}\n"
     ]
    }
   ],
   "source": [
    "# llm_model_path = os.path.join(ner_folder, model_name)\n",
    "llm_model_path = 'NER/'\n",
    "# kg_model_path = os.path.join(kg_folder, model_name)\n",
    "kg_model_path = 'KG/'\n",
    "# Iterate through the publications\n",
    "for publication_file in os.listdir(llm_model_path):\n",
    "    if publication_file.endswith('.txt'):\n",
    "        publication_path = os.path.join(llm_model_path, publication_file)\n",
    "        print('Publication Path: {}'.format(publication_path))\n",
    "        named_entities = read_named_entities(publication_path)\n",
    "        print('named_entities: {}'.format(named_entities))\n",
    "\n",
    "        # Create KG\n",
    "        kg = create_kg(named_entities)\n",
    "        \n",
    "        # Output KG file\n",
    "        output_file = os.path.join(kg_model_path, publication_file.replace('.txt', '_KG.ttl'))\n",
    "        write_kg_to_file(kg, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3185bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29c407a0",
   "metadata": {},
   "source": [
    "## (6) Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2ce3f9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
